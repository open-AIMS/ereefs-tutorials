[
  {
    "objectID": "tutorial_template.html",
    "href": "tutorial_template.html",
    "title": "",
    "section": "",
    "text": "&lt; Tutorial blurb here - can be the same as the blurb on the ereefs tutorials homepage &gt; &gt; E.g. Learn how to … in / python.\n&lt; Breif description of tutorial concepts and outcomes &gt; &gt; E.g. In this tutorial we will look at how to get eReefs data from the AIMS server corresponding to the logged locations of tagged marine animals. Keep in mind, however, that the same methodology can be applied in any situation where we wish to extract eReefs data for a range of points with different dates of interest for each point.\nCollapsable code output\nhtml tags. Its a bit of a hacky solution, but is often worth the trouble. For example, say we want to show some large output, instead of\nlarge_output &lt;- rep(\"a\", 10000)\nprint(large_output) # This shouldn't print here as eval: false is set in the yaml header of this document\nWe can do\nlarge_output &lt;- rep(\"a\", 10000) # shows the code to produce the output but does not print\nMath equations\nMath equations are written in latex. Inline equations are latex code in between single dollar signs, e.g. \\(x=1\\), and block/whole-line equations are between double dollar signs, e.g.\n\\[\n\\begin{align}\n\\text{When } x &\\in [0, \\infty), \\; y_d = \\sqrt{x} \\\\\n\\therefore \\; y_d^2 &=\n  \\begin{cases}\n    & x \\hspace{1cm}   & x \\in [0, \\infty) \\\\\n    & \\text{undefined} & x &lt; 0\n  \\end{cases}\n\\end{align}\n\\]"
  },
  {
    "objectID": "tutorial_template.html#r-packages-python-modules",
    "href": "tutorial_template.html#r-packages-python-modules",
    "title": "",
    "section": "R packages / Python modules",
    "text": "R packages / Python modules\n\nlibrary(&lt;R package name&gt;) # comment describing what specifically the package is used for in this tutorial\n# E.g. \nlibrary(readr) # to efficiently read in data\nlibrary(janitor) # to create consistent, 'clean' variable names\nlibrary(tidyverse) # for data manipulation and plotting with ggplot2\nlibrary(lubridate) # for working with date and time variables \nlibrary(leaflet) # to create an interactive map of the tracking locations\nlibrary(knitr); library(kableExtra) # for better table printing"
  },
  {
    "objectID": "tutorial_template.html#motivating-problem",
    "href": "tutorial_template.html#motivating-problem",
    "title": "",
    "section": "Motivating problem",
    "text": "Motivating problem\n&lt; Brief introduction to the mock problem being solved within this tutorial - including a little bit on the background/context of the problem generally, and a little bit on the specific instance of the problem in this tutorial &gt;\n\nE.g. The tracking of marine animals is commonly used by researchers to gain insights into the distribution, biology, behaviour and ecology of different species. However, knowing where an animal was at a certain point in time is only one piece of the puzzle. To start to understand why an animal was where it was, we usually require information on things like: What type of habitat is present at the location? What were the environmental conditions like at the time? What other lifeforms were present at the tracked location (e.g. for food or mating)?  In this tutorial we will pretend that we have tracking data for Loggerhead Sea Turtles and wish to get eReefs data corresponding to the tracked points (in time and space) to understand more about the likely environmental conditions experienced by our turtles.\n\n\n\n\n\n\n\nRead more: &lt; about what &gt;\n\n\n\n\n\n&lt; Extra information/background about the problem - hidden from view initially to not clutter the tutorial with text which is not pertinent to the tutorial, but which may be valuable to users who like to learn about tools in the context of real-world (or at least mock real-world) problems, or which may give greater motivation to some users in general &gt;"
  },
  {
    "objectID": "tutorial_template.html#data-section---e.g.-the-data-or-more-specific-example-tracking-data-loggerhead-turtle-tracking-data-etc",
    "href": "tutorial_template.html#data-section---e.g.-the-data-or-more-specific-example-tracking-data-loggerhead-turtle-tracking-data-etc",
    "title": "",
    "section": "< Data section - e.g. ‘The data’ or more specific ‘Example tracking data’ ‘Loggerhead turtle tracking data’ etc >",
    "text": "&lt; Data section - e.g. ‘The data’ or more specific ‘Example tracking data’ ‘Loggerhead turtle tracking data’ etc &gt;\n&lt; Description of the dataset - including citations where appropriate &gt;\n\n# Read in &lt;data name&gt;\n&lt; data name &gt; &lt;- read_csv(...) |&gt; clean_names()"
  },
  {
    "objectID": "tutorial_template.html#section",
    "href": "tutorial_template.html#section",
    "title": "",
    "section": "…",
    "text": "…\n\n&lt; references here &gt;\n\n&lt; END OF TUTORIAL - GENERAL NOTES BELOW &gt;\nUsing ‘callouts’ in your tutorial\n\nCallouts are very useful in drawing attention to text, or including text which doesn’t quite fit in with the body text of the tutorial. However, using too many callouts makes things look cluttered and can detract from the flow of the tutorial - use sparingly.\nTypes of callouts:\n\n\n\n\n\n\n\nUse the ‘note’ callout blocks for general notable information.\n\n\n\n\n\n\n\n\n\nUse the ‘tip’ callout when giving a tip (duh).\n\n\n\n\n\n\n\n\n\nUse the ‘caution’ callout when giving information which deals with potential oversights, things which could go wrong, things which the user must be mindful of, etc.\n\n\n\n\n\n\n\n\n\nUse the ‘important’ callout to display information which mustn’t be missed by the reader.\n\n\n\n\n\n\n\n\n\nThis serves much the same purpose as the caution callout, I think best to avoid using this callout for that reason.\n\n\n\n\nCustomising callouts: appearance=\"simple\" removes the heading from the callout, you can include a heading with"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "eReefs tutorials",
    "section": "",
    "text": "An introduction to eReefs An introduction to the eReefs information platform, the core models, their outputs, how to access these outputs, and their potential applications and limitations.\n\n\n Start"
  },
  {
    "objectID": "index.html#overview-of-ereefs",
    "href": "index.html#overview-of-ereefs",
    "title": "eReefs tutorials",
    "section": "",
    "text": "An introduction to eReefs An introduction to the eReefs information platform, the core models, their outputs, how to access these outputs, and their potential applications and limitations.\n\n\n Start"
  },
  {
    "objectID": "index.html#setup-your-development-environment",
    "href": "index.html#setup-your-development-environment",
    "title": "eReefs tutorials",
    "section": "Setup your development environment",
    "text": "Setup your development environment\n\n\n\n\n\n\nSetup RStudio Setup R and RStudio on your development environment, to run the R tutorials found in this website.\n\n\n Start\n\n\n\n\n\n\n\n\nSetup Jupyter Notebook Setup Python and Jupyter Notebook on your development environment, to run the Python tutorials found in this website.\n\n\n Start"
  },
  {
    "objectID": "index.html#access-ereefs-data",
    "href": "index.html#access-ereefs-data",
    "title": "eReefs tutorials",
    "section": "Access eReefs data",
    "text": "Access eReefs data\n\n\n\n\n\n\nBasic access to the AIMS eReefs Server Learn the basics of extracting aggregated eReefs data from the AIMS server using the efficient OPeNDAP data access protocol.\n\n\n R  Python\n\n\n\n\n\n\n\n\nProgrammatic access to the AIMS eReefs Server Extends the methods introduced in the basic access tutorial to programmatically extract aggregated eReefs data from the AIMS server for multiple dates and points.\n\n\n R  Python"
  },
  {
    "objectID": "index.html#plot-ereefs-data",
    "href": "index.html#plot-ereefs-data",
    "title": "eReefs tutorials",
    "section": "Plot eReefs data",
    "text": "Plot eReefs data\n\n\n\n\n\n\nTime series plots Learn how to create time series plots of eReefs data. [Note: the python tutorial has not yet been updated to cover the more advanced topics included in the R tutorial]\n\n\n R  Python"
  },
  {
    "objectID": "index.html#process-ereefs-data",
    "href": "index.html#process-ereefs-data",
    "title": "eReefs tutorials",
    "section": "Process eReefs data",
    "text": "Process eReefs data\n\n\n\n\n\n\nCorrelation: wind and current Learn how to extract and process eReefs data to answer the question: How strong does the wind need to be to set the direction of the surface ocean currents?\n\n\n R  Python"
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html",
    "href": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html",
    "title": "Access eReefs data",
    "section": "",
    "text": "Learn how to extract eReefs data from the AIMS server for multiple dates and points with OPeNDAP in .\nIn this tutorial we will look at how to get eReefs data from the AIMS server corresponding to the logged locations of tagged marine animals. Keep in mind, however, that the same methodology can be applied in any situation where we wish to extract eReefs data for a range of points with different dates of interest for each point."
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html#r-packages",
    "href": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html#r-packages",
    "title": "Access eReefs data",
    "section": "R packages",
    "text": "R packages\n\nlibrary(RNetCDF) # to access server data files via OPeNDAP\nlibrary(readr) # to efficiently read in data\nlibrary(janitor) # to create consistent, 'clean' variable names\nlibrary(tidyverse) # for data manipulation and plotting with ggplot2\nlibrary(lubridate) # for working with date and time variables\nlibrary(leaflet) # to create an interactive map of the tracking locations\nlibrary(knitr); library(kableExtra) # for better table printing"
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html#motivating-problem",
    "href": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html#motivating-problem",
    "title": "Access eReefs data",
    "section": "Motivating problem",
    "text": "Motivating problem\nThe tracking of marine animals is commonly used by researchers to gain insights into the distribution, biology, behaviour and ecology of different species. However, knowing where an animal was at a certain point in time is only one piece of the puzzle. To start to understand why an animal was where it was, we usually require information on things like: What type of habitat is present at the location? What were the environmental conditions like at the time? What other lifeforms were present at the tracked location (e.g. for food or mating)?\nIn this tutorial we will pretend that we have tracking data for Loggerhead Sea Turtles and wish to get eReefs data corresponding to the tracked points (in time and space) to understand more about the likely environmental conditions experienced by our turtles.\n\n\n\n\n\n\nRead more: Tracking marine animals\n\n\n\n\n\nMarine animals are typically tracked using either acoustic or satellite tags. These tags are attached to the animals and transmit signals back to receivers, logging the animal’s location at different points in time. In some cases other data such as depth, temperature, and animal movement profiles are recorded and the data transmitted to the receivers whenever possible.\nAcoustic tracking requires a network of receivers to be placed in the ocean in order to pick up the tags’ transmitted signals when they come within range (typically around 500 m). Acoustic tracking has the advantage of being able transmit and receive signals underwater, however is limited by the coverage of the receiver network. In some instances, researchers do without the receiver network and follow the animals around in a boat to receive the data. The suitability of acoustic tracking depends on the study species and research question.\nSatellite tracking, on the other hand, is able to track animals over virtually the entire ocean as the tags transmit signals to a network of satellites orbiting the earth. However, unlike acoustic tags, the signals cannot be transmitted through water and the tagged animals must breach the ocean surface in order to have their location logged and any other recorded data be received. The accuracy of the logged location depends on the quality of the transmitted signal. For high-quality signals, the location uncertainty can be in the hundreds of metres, however for bad quality signals this can blow out to over 10 km."
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html#example-tracking-data",
    "href": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html#example-tracking-data",
    "title": "Access eReefs data",
    "section": "Example tracking data",
    "text": "Example tracking data\nWe will use satellite tracking data for Loggerhead Sea Turtles (Caretta caretta) provided in Strydom (2022). This data contains tracking detections which span the length of the Great Barrier Reef off the east coast of Queensland Australia from December 2021 to April 2022 (shown in Figure 1).\nDownload the satellite tracking data file into your data folder.\n\n\n\n\n\n\nThis dataset is a summarised representation of the tracking locations per 1-degree cell. This implies a coordinate uncertainty of roughly 110 km. This level of uncertainty renders the data virtually useless for most practical applications, though it will suffice for the purposes of this tutorial. Records which are landbased as a result of the uncertainty have been removed and from here on in we will just pretend that the coordinates are accurate.\n\n\n\n\n# Read in data\nloggerhead_data &lt;- read_csv(\"data/Strydom_2022_DOI10-15468-k4s6ap.csv\") |&gt;\n  clean_names() |&gt; # clean up variable names\n  rename( # rename variables for easier use\n    record_id = gbif_id,\n    latitude = decimal_latitude,\n    longitude = decimal_longitude,\n    date_time = event_date\n  )\n\n# Remove land based records (as a result of coordinate uncertainty)\nland_based_records &lt;- c(4022992331, 4022992326, 4022992312, 4022992315, 4022992322, 4022992306)\nloggerhead_data &lt;- loggerhead_data |&gt;\n  dplyr::filter(!(record_id %in% land_based_records))\n\n# Select the variables relevant to this tutorial\nloggerhead_data &lt;- loggerhead_data |&gt;\n  select(longitude, latitude, date_time, record_id, species)\n\n# View the tracking locations on an interactive map\nloggerhead_data |&gt;\n  leaflet() |&gt;\n  addTiles() |&gt;\n  addMarkers(label = ~date_time)\n\n\n\n\n\n\n\nFigure 1: Loggerhead Sea Turtle satellite tracking records (December 2021 - April 2022)"
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html#extracting-data-from-the-server",
    "href": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html#extracting-data-from-the-server",
    "title": "Access eReefs data",
    "section": "Extracting data from the server",
    "text": "Extracting data from the server\nWe will extend the basic methods introduced in the preceding tutorial Accessing eReefs data from the AIMS server to extract data for a set of points and dates.\nWe will extract the eReefs daily mean temperature (temp), salinity (salt), and east- and northward current velocities (u and v) corresponding to the coordinates and dates for the tracking detections shown in Table 1.\n\n\nShow code to produce table\n# Print table of tracking detections (Strydom, 2022)\nloggerhead_data |&gt;\n  arrange(date_time) |&gt;\n  mutate(date = format(date_time, \"%Y-%m-%d\"), time = format(date_time, \"%H:%M\")) |&gt;\n  select(date, time, longitude, latitude) |&gt;\n  kable() |&gt; kable_styling() |&gt; scroll_box(height = \"300px\", fixed_thead = TRUE)\n\n\n\n\nTable 1: Loggerhead Sea Turtle detections (Strydom, 2022)\n\n\n\n\n\n\n\ndate\ntime\nlongitude\nlatitude\n\n\n\n\n2021-12-21\n17:57\n152.5\n-24.5\n\n\n2022-01-02\n21:49\n153.5\n-25.5\n\n\n2022-01-05\n07:33\n152.5\n-23.5\n\n\n2022-01-06\n05:03\n151.5\n-23.5\n\n\n2022-01-09\n20:25\n151.5\n-22.5\n\n\n2022-01-13\n06:28\n151.5\n-21.5\n\n\n2022-01-14\n18:26\n150.5\n-21.5\n\n\n2022-01-17\n17:06\n150.5\n-20.5\n\n\n2022-01-19\n17:44\n149.5\n-20.5\n\n\n2022-01-21\n07:22\n149.5\n-19.5\n\n\n2022-01-23\n07:02\n148.5\n-19.5\n\n\n2022-01-27\n17:00\n147.5\n-18.5\n\n\n2022-01-30\n17:02\n146.5\n-18.5\n\n\n2022-02-02\n09:14\n146.5\n-17.5\n\n\n2022-02-03\n21:37\n153.5\n-24.5\n\n\n2022-02-06\n18:25\n146.5\n-16.5\n\n\n2022-02-07\n07:15\n145.5\n-16.5\n\n\n2022-02-09\n18:33\n145.5\n-15.5\n\n\n2022-02-12\n08:59\n153.5\n-26.5\n\n\n2022-02-12\n10:34\n145.5\n-14.5\n\n\n2022-03-25\n07:10\n144.5\n-13.5\n\n\n2022-04-01\n18:41\n143.5\n-12.5\n\n\n2022-04-09\n22:00\n143.5\n-11.5\n\n\n2022-04-14\n06:31\n143.5\n-10.5\n\n\n2022-04-21\n10:30\n143.5\n-9.5\n\n\n\n\n\n\n\n\n\n\n\nWe will take advantage of the consistent file naming on the server to extract the data of interest programmatically. We will first need to copy the OPeNDAP data link for one of the files within the correct model and aggregation folders and then replace the date.\nSelecting a random date within the daily aggregated data with one data file per day (daily-daily) for the 1km hydro model (gbr1_2.0), we see the files have the naming format:\nhttps://thredds.ereefs.aims.gov.au/thredds/dodsC/ereefs/gbr1_2.0/daily-daily/EREEFS_AIMS-CSIRO_gbr1_2.0_hydro_daily-daily-YYYY-MM-DD.nc\nWe will now write a script which extracts the data for the dates and coordinates in Table 1. For each unique date we will open the corresponding file on the server and extract the daily mean temperature, salinity, northward and southward current velocities for each set of coordinates corresponding to the date.\n\n# GET DATA FOR EACH DATE AND COORDINATE (LAT LON) PAIR\nt_start = Sys.time() # to track run time of extraction\n\n## 1. Setup variables for data extraction\n# Server file name = &lt;file_prefix&gt;&lt;date (yyyy-mm-dd)&gt;&lt;file_suffix&gt;\nfile_prefix &lt;- \"https://thredds.ereefs.aims.gov.au/thredds/dodsC/ereefs/gbr1_2.0/daily-daily/EREEFS_AIMS-CSIRO_gbr1_2.0_hydro_daily-daily-\"\nfile_suffix &lt;- \".nc\"\n\n# Table of dates and coordinates for which to extract data (dates as character string)\ndetections &lt;- loggerhead_data |&gt;\n  mutate(date = as.character(as_date(date_time))) |&gt;\n  select(date, longitude, latitude) |&gt;\n  distinct()\n\nextracted_data &lt;- data.frame() # to save the extracted data\ndates &lt;- unique(detections$date) # unique dates for which to open server files\n\n## 2. For each date of interest, open a connection to the corresponding data file on the server\nfor (i in 1:length(dates)) {\n  date_i &lt;- dates[i]\n\n  # Open file\n  file_name_i &lt;- paste0(file_prefix, date_i, file_suffix)\n  server_file_i &lt;- open.nc(file_name_i)\n\n  # Coordinates for which to extract data for the current date\n  coordinates_i &lt;- detections |&gt; dplyr::filter(date == date_i)\n\n  # Get all coordinates in the open file (each representing the center-point of the corresponding grid cell)\n  server_lons_i &lt;- var.get.nc(server_file_i, \"longitude\")\n  server_lats_i &lt;- var.get.nc(server_file_i, \"latitude\")\n\n  ## 3. For each coordinate (lon, lat) for the current date, get the data for the closest grid cell (1km^2) from the open server file\n  for (j in 1:nrow(coordinates_i)) {\n\n    # Current coordinate of interest\n    lon_j &lt;- coordinates_i[j,]$longitude\n    lat_j &lt;- coordinates_i[j,]$latitude\n\n    # Find the index of the grid cell containing our coordinate of interest (i.e. the center-point closest to our point of interest)\n    lon_index &lt;- which.min(abs(server_lons_i - lon_j))\n    lat_index &lt;- which.min(abs(server_lats_i - lat_j))\n\n    # Setup start vector arguments for RNetCDF::var.get.nc (same for temp, salt, currents u & v)\n    ###################################\n    # Recall the order of the dimensions (longitude, latitude, k, time) from the previous tutorial. Therefore we want [lon_index, lat_index, k = 16 corresponding to a depth of 0.5m, time = 1 (as we're using the daily files this is the only option)]. If you are still confused go back to the previous tutorial or have a look at the structure of one of the server files by uncommenting the following 5 lines of code:\n    # not_yet_run = TRUE  # used so the following lines are only run once\n    # if (not_yet_run) {\n    #   print.nc(server_file_i)\n    #   not_yet_run = FALSE\n    # }\n    ##################################\n    start_j &lt;- c(lon_index, lat_index, 16, 1) # k = 16 corresponds to depth = 0.5m\n    count_j &lt;- c(1, 1, 1, 1) # only extracting a single value for each variable\n\n    # Get the data for the grid cell containing our point of interest\n    temp_j &lt;- var.get.nc(server_file_i, \"temp\", start_j, count_j)\n    salt_j &lt;- var.get.nc(server_file_i, \"salt\", start_j, count_j)\n    u_j &lt;- var.get.nc(server_file_i, \"u\", start_j, count_j)\n    v_j &lt;- var.get.nc(server_file_i, \"v\", start_j, count_j)\n    extracted_data_j &lt;- data.frame(date_i, lon_j, lat_j, temp_j, salt_j, u_j, v_j)\n\n    ## 4. Save data in memory and repeat for next date-coordinate pair\n    extracted_data &lt;- rbind(extracted_data, extracted_data_j)\n  }\n  # Close connection to open server file and move to the next date\n  close.nc(server_file_i)\n}\n\n# Calculate the run time of the extraction\nt_stop &lt;- Sys.time()\nextract_time &lt;- t_stop - t_start\n\n# Rename the extracted data columns\ncolnames(extracted_data) &lt;- c(\"date\", \"lon\", \"lat\", \"temp\", \"salt\", \"u\", \"v\")\n\n\n\n\n\n\n\nIn the code above we match the closest eReefs model grid cell to each point in our list of coordinates (i.e. for each tracking detection). This will therefore match grid cells to all the coordinates, even if they are not within the eReefs model boundary. This behaviour may be useful when we have points right along the coastline as the eReefs models have small gaps at many points along the coast (see image below). However, in other cases this behaviour may not be desirable. For example, if we had points down near Sydney they would be matched to the closest eReefs grid cells (somewhere up near Brisbane) and the extracted data would be erroneous.\n\n\n\n\nOur extracted data is shown below in Table 2. To get this data we opened 24 files on the server (corresponding to unique dates in Table 1) and extracted data for 25 unique date-coordinate pairs. On our machine this took 1.2 mins to run.\n\n# Print the extracted data\nextracted_data |&gt; kable() |&gt; kable_styling() |&gt; scroll_box(height = \"300px\", fixed_thead = TRUE)\n\n\n\nTable 2: Extracted daily mean temperature, salinity, and east- and northward current velocities (u, v respectively) for Loggerhead Sea Turtle detections (Strydom, 2022)\n\n\n\n\n\n\n\ndate\nlon\nlat\ntemp\nsalt\nu\nv\n\n\n\n\n2022-01-17\n150.5\n-20.5\n29.39174\n35.33709\n-0.0659649\n-0.1778789\n\n\n2022-01-02\n153.5\n-25.5\n26.00014\n35.29583\n-0.0353987\n0.0375804\n\n\n2021-12-21\n152.5\n-24.5\n28.09089\n35.25638\n0.0713334\n-0.0199901\n\n\n2022-01-27\n147.5\n-18.5\n29.46284\n33.98060\n0.2652802\n0.0044857\n\n\n2022-02-12\n145.5\n-14.5\n29.89639\n34.71634\n-0.0812021\n0.0344770\n\n\n2022-02-12\n153.5\n-26.5\n26.79947\n35.38062\n-0.1235716\n0.0057869\n\n\n2022-01-23\n148.5\n-19.5\n28.98567\n35.26142\n-0.1370672\n-0.0219267\n\n\n2022-04-14\n143.5\n-10.5\n29.47597\n34.41768\n-0.0409690\n0.0776469\n\n\n2022-01-19\n149.5\n-20.5\n29.91821\n35.47683\n0.0181383\n-0.1044010\n\n\n2022-01-09\n151.5\n-22.5\n29.24492\n35.33996\n0.0007144\n-0.1032533\n\n\n2022-01-14\n150.5\n-21.5\n28.98728\n35.40287\n-0.0638922\n-0.0470999\n\n\n2022-04-09\n143.5\n-11.5\n29.93064\n34.55591\n-0.0967079\n0.1442562\n\n\n2022-01-21\n149.5\n-19.5\n29.57756\n35.11963\n-0.1611882\n-0.0347497\n\n\n2022-01-30\n146.5\n-18.5\n30.14936\n34.59603\n-0.1835063\n0.1426468\n\n\n2022-02-03\n153.5\n-24.5\n27.06327\n35.32142\n0.5498420\n-0.7972959\n\n\n2022-02-09\n145.5\n-15.5\n29.54386\n34.74210\n-0.1198241\n0.0636992\n\n\n2022-02-07\n145.5\n-16.5\n30.37080\n34.06056\n-0.1037723\n0.2000399\n\n\n2022-03-25\n144.5\n-13.5\n29.04090\n34.75312\n-0.4569216\n0.3603792\n\n\n2022-01-13\n151.5\n-21.5\n28.41784\n35.25340\n-0.0951453\n0.0155211\n\n\n2022-04-01\n143.5\n-12.5\n30.14290\n34.44403\n0.0250692\n-0.0175750\n\n\n2022-04-21\n143.5\n-9.5\n29.56355\n34.19133\n0.0587259\n0.0340937\n\n\n2022-01-05\n152.5\n-23.5\n25.64322\n35.22873\n0.0303043\n0.0149218\n\n\n2022-02-06\n146.5\n-16.5\n29.13773\n34.69858\n-0.1224007\n-0.1002772\n\n\n2022-01-06\n151.5\n-23.5\n28.12745\n35.40966\n-0.0329330\n0.0152895\n\n\n2022-02-02\n146.5\n-17.5\n30.58525\n34.71669\n0.1280997\n-0.0543357"
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html#matching-extracted-data-to-tracking-data",
    "href": "tutorials/r/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_r.html#matching-extracted-data-to-tracking-data",
    "title": "Access eReefs data",
    "section": "Matching extracted data to tracking data",
    "text": "Matching extracted data to tracking data\nWe will match up the eReefs data with our tracking detections by combining the two datasets based on common date, longitude and latitude values.\n\n# Ensure common variables date, lon and lat between the two datasets\nextracted_data &lt;- extracted_data |&gt;\n  rename(longitude = lon, latitude = lat)\nloggerhead_data &lt;- loggerhead_data |&gt;\n  mutate(date = as_date(date_time))\n\n# Merge the two datasets based on common date, lon and lat values\ncombined_data &lt;- merge(\n  loggerhead_data, extracted_data,\n  by = c(\"date\", \"longitude\", \"latitude\")\n) |&gt; select(-date)\n\n# Print the combined data\ncombined_data |&gt; kable() |&gt; kable_styling() |&gt;  scroll_box(height = \"300px\", fixed_thead = TRUE)\n\n\n\nTable 3: Loggerhead Sea Turtle tracking detections (Strydom, 2022) and corresponding eReefs daily mean temperature, salinity, east- and northward current velocities (u, v respectively).\n\n\n\n\n\n\n\nlongitude\nlatitude\ndate_time\nrecord_id\nspecies\ntemp\nsalt\nu\nv\n\n\n\n\n152.5\n-24.5\n2021-12-21 17:57:22\n4022992328\nCaretta caretta\n28.09089\n35.25638\n0.0713334\n-0.0199901\n\n\n153.5\n-25.5\n2022-01-02 21:49:55\n4022992329\nCaretta caretta\n26.00014\n35.29583\n-0.0353987\n0.0375804\n\n\n152.5\n-23.5\n2022-01-05 07:33:53\n4022992304\nCaretta caretta\n25.64322\n35.22873\n0.0303043\n0.0149218\n\n\n151.5\n-23.5\n2022-01-06 05:03:23\n4022992302\nCaretta caretta\n28.12745\n35.40966\n-0.0329330\n0.0152895\n\n\n151.5\n-22.5\n2022-01-09 20:25:01\n4022992319\nCaretta caretta\n29.24492\n35.33996\n0.0007144\n-0.1032533\n\n\n151.5\n-21.5\n2022-01-13 06:28:09\n4022992308\nCaretta caretta\n28.41784\n35.25340\n-0.0951453\n0.0155211\n\n\n150.5\n-21.5\n2022-01-14 18:26:17\n4022992318\nCaretta caretta\n28.98728\n35.40287\n-0.0638922\n-0.0470999\n\n\n150.5\n-20.5\n2022-01-17 17:06:32\n4022992330\nCaretta caretta\n29.39174\n35.33709\n-0.0659649\n-0.1778789\n\n\n149.5\n-20.5\n2022-01-19 17:44:38\n4022992320\nCaretta caretta\n29.91821\n35.47683\n0.0181383\n-0.1044010\n\n\n149.5\n-19.5\n2022-01-21 07:22:08\n4022992316\nCaretta caretta\n29.57756\n35.11963\n-0.1611882\n-0.0347497\n\n\n148.5\n-19.5\n2022-01-23 07:02:18\n4022992323\nCaretta caretta\n28.98567\n35.26142\n-0.1370672\n-0.0219267\n\n\n147.5\n-18.5\n2022-01-27 17:00:04\n4022992327\nCaretta caretta\n29.46284\n33.98060\n0.2652802\n0.0044857\n\n\n146.5\n-18.5\n2022-01-30 17:02:42\n4022992314\nCaretta caretta\n30.14936\n34.59603\n-0.1835063\n0.1426468\n\n\n146.5\n-17.5\n2022-02-02 09:14:56\n4022992301\nCaretta caretta\n30.58525\n34.71669\n0.1280997\n-0.0543357\n\n\n153.5\n-24.5\n2022-02-03 21:37:56\n4022992313\nCaretta caretta\n27.06327\n35.32142\n0.5498420\n-0.7972959\n\n\n146.5\n-16.5\n2022-02-06 18:25:38\n4022992303\nCaretta caretta\n29.13773\n34.69858\n-0.1224007\n-0.1002772\n\n\n145.5\n-16.5\n2022-02-07 07:15:30\n4022992310\nCaretta caretta\n30.37080\n34.06056\n-0.1037723\n0.2000399\n\n\n145.5\n-15.5\n2022-02-09 18:33:03\n4022992311\nCaretta caretta\n29.54386\n34.74210\n-0.1198241\n0.0636992\n\n\n145.5\n-14.5\n2022-02-12 10:34:12\n4022992325\nCaretta caretta\n29.89639\n34.71634\n-0.0812021\n0.0344770\n\n\n153.5\n-26.5\n2022-02-12 08:59:01\n4022992324\nCaretta caretta\n26.79947\n35.38062\n-0.1235716\n0.0057869\n\n\n144.5\n-13.5\n2022-03-25 07:10:49\n4022992309\nCaretta caretta\n29.04090\n34.75312\n-0.4569216\n0.3603792\n\n\n143.5\n-12.5\n2022-04-01 18:41:42\n4022992307\nCaretta caretta\n30.14290\n34.44403\n0.0250692\n-0.0175750\n\n\n143.5\n-11.5\n2022-04-09 22:00:54\n4022992317\nCaretta caretta\n29.93064\n34.55591\n-0.0967079\n0.1442562\n\n\n143.5\n-10.5\n2022-04-14 06:31:48\n4022992321\nCaretta caretta\n29.47597\n34.41768\n-0.0409690\n0.0776469\n\n\n143.5\n-9.5\n2022-04-21 10:30:09\n4022992305\nCaretta caretta\n29.56355\n34.19133\n0.0587259\n0.0340937\n\n\n\n\n\n\n\n\n\n\nHooray! We now have our combined dataset of the Loggerhead Sea Turtle tracking detections and the corresponding eReefs daily aggregated data (Table 3).\n Strydom A. 2022. Wreck Rock Turtle Care - satellite tracking. Data downloaded from OBIS-SEAMAP; originated from Satellite Tracking and Analysis Tool (STAT). DOI: 10.15468/k4s6ap accessed via GBIF.org on 2023-02-17."
  },
  {
    "objectID": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html",
    "href": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html",
    "title": "Processing eReefs data ",
    "section": "",
    "text": "Learn how to use eReefs data to answer the question ‘How strong does the wind need to be to set the direction of the surface ocean currents?’ in ."
  },
  {
    "objectID": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#motivating-problem",
    "href": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#motivating-problem",
    "title": "Processing eReefs data ",
    "section": "Motivating problem",
    "text": "Motivating problem\nThe East Australian Current (EAC) usually is a strong southward current near Myrmidon and Davies Reefs. During winter months the wind moves in a north eastern direction in the near opposite direction to the EAC. When the wind is low the surface currents are dominated by the EAC. As the wind picks up, at some speed the wind overpowers the EAC and the surface current moves in the direction of the wind.\nWe will use the AIMS eReefs extraction tool to extract data for two locations of interest:\n\nMyrmidon Reef which is on the far outer edge of the GBR and is almost right in the middle of the southern Eastern Australian Current; and\nDavies Reef which is further in the reef matrix, but in a similar sector of the GBR.\n\nThe locations of these two reefs are shown in Figure 1.\nWe then process the data and investigate the relationship between the strength of the wind and the direction of the surface currents for the two locations.\n\n\nShow code to create map\n# Plot site coordinates on an interactive map with reef boundaries\nlibrary(leaflet)\nsite_coords &lt;- read.csv(\"resources/site_coordinates.csv\")\nsite_map &lt;- site_coords |&gt;\n    leaflet( # create a blank leaflet map\n      options = leafletOptions(attributionControl=FALSE) # remove the 'leaflet' watermark\n    ) |&gt;\n    addTiles() |&gt; # add a basemap (OpenStreetMap by default)\n    addMarkers() |&gt; # add a marker at the given coordinates\n    addScaleBar()\n\n# Add the GBR reef features (WMS layer) to the map\nsite_map &lt;- site_map |&gt;\n    addWMSTiles(\n      baseUrl = \"https://maps.eatlas.org.au/maps/wms?\", # Link to WMS server\n      layers = c(\"ea:GBR_GBRMPA_GBR-features\"), # Names of layers (located in the WMS server) to display\n      options = WMSTileOptions(format = \"image/png\", transparent = TRUE)\n    )\n\n# Display the map centred at our site\nsite_map |&gt;\n    setView(lng = mean(site_coords$lon), lat = mean(site_coords$lat), zoom = 8)\n\n\n\n\n\n\n\n\nFigure 1: Myrmidon Reef and Davies Reef locations."
  },
  {
    "objectID": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#analysis-method",
    "href": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#analysis-method",
    "title": "Processing eReefs data ",
    "section": "Analysis method",
    "text": "Analysis method\nTo determine the relation between the wind and the surface currents we will use the AIMS eReefs extraction tool to pull out hourly time series wind and current data for our two locations of interest. We will then look at the correlation between the wind and current vectors, where a correlation of 1 indicates they are pointing in the same direction, and -1 indicated they are in opposite directions."
  },
  {
    "objectID": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#setting-up-to-get-the-data",
    "href": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#setting-up-to-get-the-data",
    "title": "Processing eReefs data ",
    "section": "Setting up to get the data",
    "text": "Setting up to get the data\nTo extract the time series data using the extraction tool we need to create a CSV file containing the sites of interest. This file needs to contain the coordinates and names of the sites. To create this I first added my points manually in Google Earth Pro. This was done to simply get the location of Myrmidon and Davies Reefs. Using Google Earth to create your CSV file for the extraction tool is only useful if you don’t already know the coordinates of your sites.\nScreenshot of Google Earth Pro with Myromidon and Davies reef sites\nThe points can be added using the Add placemark tool (looks like a pin). The locations can be seen by displaying the placemark properties. The resulting KML file can be found here: extraction-tool-locations.kml.\nThe location of the two sites were copied to create the CSV file for the data extraction tool."
  },
  {
    "objectID": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#extracting-the-data",
    "href": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#extracting-the-data",
    "title": "Processing eReefs data ",
    "section": "Extracting the data",
    "text": "Extracting the data\nThe CSV file was uploaded to the AIMS extraction tool and the extraction was performed with the following settings:\n\nData collection: GBR1 Hydro (Version 2)\nVariables:\n\nEastward wind speed (wspeed_u)\nNorthward wind speed (wspeed_v)\nNorthward current (v)\nEastward current (u)\n\nDate range: 1 January 2019 - 31 December 2019\nTime step: hourly\nDepths: -2.35 m\n\nOnce the extraction request was submitted, the dataset was created after an hour of data processing. Then, the data was available for download from Extraction request: Example dataset: Wind-vs-Current at Davies and Myrmidon Reefs (2019)."
  },
  {
    "objectID": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#downloading-the-data",
    "href": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#downloading-the-data",
    "title": "Processing eReefs data ",
    "section": "Downloading the data",
    "text": "Downloading the data\nIn this notebook, we will download the data using scripting. There is no need to re-run the extraction request as each extraction performed by the extraction tool has a permanent public page created for it that can be used to facilitate sharing of the data.\nLet’s first create a temporary folder to contain the downloaded data.\n\nif (!file.exists(\"temp\")) dir.create(\"temp\")\n\nNow let’s download the data. The file to download is 12.9 MB and so this download might take a little while. To allow us to re-run this script without having to wait for the download each time we first check that the download has not already been done.\n\nextractionfile &lt;- file.path(\"temp\", \"2009.c451dc3-collected.csv\")\n\nif (!file.exists(extractionfile)) {\n  message(\"Downloading extraction data ...\")\n  url &lt;- \"https://api.ereefs.aims.gov.au/data-extraction/request/2009.c451dc3/files/2009.c451dc3-collected.csv\"\n  download.file(url, destfile = extractionfile)\n  message(\"Download complete\")\n} else {\n  message(\"Skipping redownloading extraction data\")\n}"
  },
  {
    "objectID": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#reading-and-cleaning-the-data",
    "href": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#reading-and-cleaning-the-data",
    "title": "Processing eReefs data ",
    "section": "Reading and cleaning the data",
    "text": "Reading and cleaning the data\nRead the resulting CSV file into a data frame.\n\nlibrary(janitor)\nlibrary(tidyverse)\n# Read in data with 'clean' variable names\ndata &lt;- read.csv(extractionfile) |&gt; clean_names() |&gt; glimpse()\n\nRows: 70,080\nColumns: 12\n$ aggregated_date_time &lt;chr&gt; \"2019-01-01T00:00\", \"2019-01-01T00:00\", \"2019-01-…\n$ variable             &lt;chr&gt; \"wspeed_u\", \"wspeed_u\", \"wspeed_v\", \"wspeed_v\", \"…\n$ depth                &lt;dbl&gt; 99999.90, 99999.90, 99999.90, 99999.90, -2.35, -2…\n$ site_name            &lt;chr&gt; \"Myrmidon Reef\", \"Davies Reef\", \"Myrmidon Reef\", …\n$ latitude             &lt;dbl&gt; -18.26560, -18.82284, -18.26560, -18.82284, -18.2…\n$ longitude            &lt;dbl&gt; 147.3890, 147.6452, 147.3890, 147.6452, 147.3890,…\n$ mean                 &lt;dbl&gt; -9.56848758, -8.88017520, 3.26042985, 2.75675041,…\n$ median               &lt;dbl&gt; -9.56848758, -8.88017520, 3.26042985, 2.75675041,…\n$ p5                   &lt;dbl&gt; -9.56848758, -8.88017520, 3.26042985, 2.75675041,…\n$ p95                  &lt;dbl&gt; -9.56848758, -8.88017520, 3.26042985, 2.75675041,…\n$ lowest               &lt;dbl&gt; -9.56848758, -8.88017520, 3.26042985, 2.75675041,…\n$ highest              &lt;dbl&gt; -9.56848758, -8.88017520, 3.26042985, 2.75675041,…\n\n\nThe first thing we might notice about the data is that, somewhat confusingly, we have a bunch of aggregation statistics (mean, median, p5, p95, lowest, highest) which all take the same value. This is because we have extracted hourly “aggregated” data, but the time step for the eReefs model is also hourly. Therefore each row represents an aggregation over a single data point. Don’t worry if you are confused by this, it’s not important. Just think of this as a quirk of the eReefs data extraction tool. We’ll clean this up now by replacing the aggregation statistics with a single column called value and rename aggregated_date_time to date_time, to avoid any further confusion.\n\ndata2 &lt;- data |&gt;\n  # Create 'value' column\n  mutate(value = mean) |&gt;\n  # Remove aggregation statistic columns\n  select(-c(mean, median, p5, p95, lowest, highest)) |&gt;\n  # Rename aggregated_date_time\n  rename(date_time = aggregated_date_time) |&gt;\n  glimpse()\n\nRows: 70,080\nColumns: 7\n$ date_time &lt;chr&gt; \"2019-01-01T00:00\", \"2019-01-01T00:00\", \"2019-01-01T00:00\", …\n$ variable  &lt;chr&gt; \"wspeed_u\", \"wspeed_u\", \"wspeed_v\", \"wspeed_v\", \"v\", \"v\", \"u…\n$ depth     &lt;dbl&gt; 99999.90, 99999.90, 99999.90, 99999.90, -2.35, -2.35, -2.35,…\n$ site_name &lt;chr&gt; \"Myrmidon Reef\", \"Davies Reef\", \"Myrmidon Reef\", \"Davies Ree…\n$ latitude  &lt;dbl&gt; -18.26560, -18.82284, -18.26560, -18.82284, -18.26560, -18.8…\n$ longitude &lt;dbl&gt; 147.3890, 147.6452, 147.3890, 147.6452, 147.3890, 147.6452, …\n$ value     &lt;dbl&gt; -9.56848758, -8.88017520, 3.26042985, 2.75675041, 0.04731101…\n\n\nMuch better! Now it is clear to see that the data is in long format. That is, a single row for each value and a separate column describing the meaning of the value — in this case the column variable describes what the column value means. Converting the data into tidy format will help with subsequent analyses (and is probably also the format you are most comfortable working with). When data is in tidy format, each variable has its own column, each observation has its own row, and each value has its own cell.\nIf we think of the wind and current velocities as the things being “measured”, then the observations in the dataset are the values of the measurements for each time point at each site. Therefore we would like a single row for each unique combination of date_time and site_name (or latitude and longitude pair). Let’s see if this is what we get we try to “widen” the data into tidy format.\n\n# Try widening to tidy format\ndata2 |&gt; pivot_wider(\n  names_from = \"variable\",\n  values_from = \"value\"\n) |&gt; head() |&gt; knitr::kable() |&gt; kableExtra::kable_styling()\n\n\n\n\ndate_time\ndepth\nsite_name\nlatitude\nlongitude\nwspeed_u\nwspeed_v\nv\nu\n\n\n\n\n2019-01-01T00:00\n99999.90\nMyrmidon Reef\n-18.26560\n147.3890\n-9.568488\n3.260430\nNA\nNA\n\n\n2019-01-01T00:00\n99999.90\nDavies Reef\n-18.82284\n147.6452\n-8.880175\n2.756750\nNA\nNA\n\n\n2019-01-01T00:00\n-2.35\nMyrmidon Reef\n-18.26560\n147.3890\nNA\nNA\n0.0473110\n0.0124983\n\n\n2019-01-01T00:00\n-2.35\nDavies Reef\n-18.82284\n147.6452\nNA\nNA\n0.1003906\n-0.0488348\n\n\n2019-01-01T01:00\n99999.90\nMyrmidon Reef\n-18.26560\n147.3890\n-9.420339\n3.528335\nNA\nNA\n\n\n2019-01-01T01:00\n99999.90\nDavies Reef\n-18.82284\n147.6452\n-8.749271\n2.753487\nNA\nNA\n\n\n\n\n\n\n\nWith all the NAs in the wind and current speed columns, this doesn’t look tidy at all! And it’s because we forgot to account for depth. However, we can notice that depth is actually a redundant variable in this dataset. That’s because wind speed implies a NA depth value (coded as 10000m in the extracted data) and current implies a depth of -2.35m (as this is the only depth we chose to extract). Therefore we can just remove depth entirely from our dataset without losing any information, that is, as long as we remember that the current relates to a depth of -2.35m. For the forgetful among us, we could rename the current variables to v_2.35m and u_2.35m. In fact moving the depth into the variable names would be a good solution to create a tidy dataset if we had selected multiple depths.\n\n# Widen to tidy format\ndata_tidy &lt;- data2 |&gt;\n  # Drop the depth column\n  select(-depth) |&gt;\n  # Widen into tidy format\n  pivot_wider(\n    names_from = \"variable\",\n    values_from = \"value\"\n  )\n\ndata_tidy |&gt; head() |&gt; knitr::kable() |&gt; kableExtra::kable_styling()\n\n\n\n\ndate_time\nsite_name\nlatitude\nlongitude\nwspeed_u\nwspeed_v\nv\nu\n\n\n\n\n2019-01-01T00:00\nMyrmidon Reef\n-18.26560\n147.3890\n-9.568488\n3.260430\n0.0473110\n0.0124983\n\n\n2019-01-01T00:00\nDavies Reef\n-18.82284\n147.6452\n-8.880175\n2.756750\n0.1003906\n-0.0488348\n\n\n2019-01-01T01:00\nMyrmidon Reef\n-18.26560\n147.3890\n-9.420339\n3.528335\n-0.0132966\n-0.0292120\n\n\n2019-01-01T01:00\nDavies Reef\n-18.82284\n147.6452\n-8.749271\n2.753487\n-0.0361294\n-0.0908067\n\n\n2019-01-01T02:00\nMyrmidon Reef\n-18.26560\n147.3890\n-9.333500\n3.765564\n-0.0665416\n-0.0605202\n\n\n2019-01-01T02:00\nDavies Reef\n-18.82284\n147.6452\n-8.463824\n2.587230\n-0.1804798\n-0.1182885\n\n\n\n\n\n\n\nNow each row gives the wind and current speeds for different sites at different points in time."
  },
  {
    "objectID": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#correlation",
    "href": "tutorials/r/process-ereefs-data/correlation/correlation_wind_and_current_r.html#correlation",
    "title": "Processing eReefs data ",
    "section": "Correlation",
    "text": "Correlation\nOur aim is to create an index that estimates the correlation of the current and the wind vectors.\nThe correlation of the current and wind vectors can be estimated based using the dot product. An overview of the relationship between correlation and using the dot product is described in Geometric Interpretation of the Correlation between Two Variables. The correlation between the two vectors is given by:\n\\[\nr = \\cos(\\theta) = \\frac{a \\cdot b}{||a||\\cdot||b||}\n\\]\nwhere \\(a \\cdot b\\) is the dot product between the two vectors and \\(||a||\\) and \\(||b||\\) are the magnitudes of the vectors. The dot product can be calculated as\n\\[\na \\cdot b = a_x \\times b_x + a_y \\times b_y\n\\]\nand the magnitude of the vectors as\n\\[\n||a|| = \\sqrt{a^2_x + a^2_y} \\;\\;, \\; \\;\\;\\;\\;\n||b|| = \\sqrt{b^2_x + b^2_y}\n\\]\n\n# Calculate current and wind magnitudes and correlation\ndata_corr &lt;- data_tidy |&gt;\n  mutate(\n    curr_mag = sqrt(u^2 + v^2),\n    wind_mag = sqrt(wspeed_u^2 + wspeed_v^2),\n    wind_curr_corr = (u * wspeed_u  +  v * wspeed_v) / (curr_mag * wind_mag)\n  ) |&gt; glimpse()\n\nRows: 17,520\nColumns: 11\n$ date_time      &lt;chr&gt; \"2019-01-01T00:00\", \"2019-01-01T00:00\", \"2019-01-01T01:…\n$ site_name      &lt;chr&gt; \"Myrmidon Reef\", \"Davies Reef\", \"Myrmidon Reef\", \"Davie…\n$ latitude       &lt;dbl&gt; -18.26560, -18.82284, -18.26560, -18.82284, -18.26560, …\n$ longitude      &lt;dbl&gt; 147.3890, 147.6452, 147.3890, 147.6452, 147.3890, 147.6…\n$ wspeed_u       &lt;dbl&gt; -9.568488, -8.880175, -9.420339, -8.749271, -9.333500, …\n$ wspeed_v       &lt;dbl&gt; 3.260430, 2.756750, 3.528335, 2.753487, 3.765564, 2.587…\n$ v              &lt;dbl&gt; 0.04731101, 0.10039063, -0.01329665, -0.03612937, -0.06…\n$ u              &lt;dbl&gt; 0.012498257, -0.048834795, -0.029212014, -0.090806716, …\n$ curr_mag       &lt;dbl&gt; 0.04893402, 0.11163832, 0.03209583, 0.09773019, 0.08994…\n$ wind_mag       &lt;dbl&gt; 10.108727, 9.298236, 10.059420, 9.172319, 10.064477, 8.…\n$ wind_curr_corr &lt;dbl&gt; 0.070077977, 0.684380009, 0.707019119, 0.775324802, 0.3…\n\n\nLet’s look at the relationship between the wind and current as a function of the wind speed. Here we are considering each hourly sample as an independent estimate of the relationship. In reality this is not the case as the longer the wind blows the more effect it will have on the current. As this is just a coding example and not an in-depth analysis we don’t need to worry about this limitation of the analysis.\nLet’s create a scatter plot to see if there is a relationship between the wind and currents.\n\ndata_corr |&gt;\n  ggplot(aes(\n    x = wind_mag,\n    y = wind_curr_corr,\n    color = site_name\n  )) +\n  geom_point(size = 1, alpha = 0.7) +\n  labs(\n    title = \"Correlation between wind and surface current (hourly data, 2019)\",\n    x = \"Wind speed (m/s)\",\n    y = \"Wind-current correlation\",\n    color = \"Site\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis scatter plot shows that the relationship between wind and current is weak. This is not surprising given that we are considering just the hourly samples, with no consideration for how long the wind has been blowing. At low wind conditions the current has an even chance of being aligned with the wind (correlation \\(r= 1\\)) as in the opposite direction (correlation \\(r= -1\\)), however in high wind we can see that there is much more chance that the currents are aligned with the wind.\nTo understand this relationship better we want to understand how much the wind nudges the current in its direction. If we bin the wind speeds then collect all the correlation samples in each bin then we can see if they average to zero (indicating that there is no relationship between the wind and current) or there is average alignment.\n\n# Bin the wind magnitude for each site and get the mean correlation in each bin, i.e. divide the range of the wind mag into 20 equal segments (bins), assign each observation to a bin, get the mean of the correlations for all observations in each bin\nn_bins &lt;- 20\n\n# Get data for each site\ndav &lt;- data_corr |&gt; dplyr::filter(site_name == \"Davies Reef\")\nmyr &lt;- data_corr |&gt; dplyr::filter(site_name == \"Myrmidon Reef\")\n\n# Get the edges for the bins\nedges_dav &lt;- seq(min(dav$wind_mag), max(dav$wind_mag), length.out = n_bins + 1)\nedges_myr &lt;- seq(min(myr$wind_mag), max(myr$wind_mag), length.out = n_bins + 1)\n\n# Bin the wind magnitude, calculate the mean correlation for each bin, and get the create columns for the bin endpoints\nbinned_dav &lt;- dav |&gt;\n  group_by(\n    site_name,\n    bin = cut(wind_mag, breaks = edges_dav, include.lowest = TRUE)\n  ) |&gt;\n  summarise(mean_corr = mean(wind_curr_corr)) |&gt;\n  cbind(\n    xmin = edges_dav[1:n_bins],\n    xmax = edges_dav[2:(n_bins+1)]\n  )\nbinned_myr &lt;- myr |&gt;\n  group_by(\n    site_name,\n    bin = cut(wind_mag, breaks = edges_myr, include.lowest = TRUE)\n  ) |&gt;\n  summarise(mean_corr = mean(wind_curr_corr)) |&gt;\n  cbind(\n    xmin = edges_myr[1:n_bins],\n    xmax = edges_myr[2:(n_bins+1)]\n  )\n\n# Plot the binned data\nrbind(binned_dav, binned_myr) |&gt;\n  ggplot(aes(color = site_name)) +\n  geom_segment(\n    mapping = aes(y = mean_corr, yend = mean_corr, x = xmin, xend = xmax),\n    linewidth = 2, alpha = 0.6\n  ) +\n  labs(\n    x = \"Wind speed (m/s)\",\n    y = \"Wind-current correlation\",\n    title = \"Mean correlation between wind and surface current (hourly data, 2019)\",\n    color = \"Site\") +\n  theme_bw(base_size = 12) +\n  scale_color_manual(values = c(\"darkgreen\",\"red\"))\n\n\n\n\n\n\n\n\nFrom this we can see that for wind speeds below about 8 m/s the surface current direction is unrelated to the wind. Above this wind speed the surface current is increasingly determined by the direction of the wind. By the time the wind is 16 m/s the direction of the surface current is dominated by the wind direction.\n\n\n\n\n\n\nIt should be remembered that this analysis is based on the eReefs Hydrodynamic model and as such is not based on real data. The eReefs model has however been tuned to accurately capture the flow dynamics of the GBR and so we would expect the estimates from this analysis to be approximately correct."
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/map-plot/map_plot_r.html",
    "href": "tutorials/r/plot-ereefs-data/map-plot/map_plot_r.html",
    "title": "Plotting eReefs data",
    "section": "",
    "text": "MATERIAL FROM BASIC SERVER ACCESS DUMPED BELOW IDEAS:\n\nlink between the two tutorials (e.g. from basic server access provide a link in the footer saying ‘learn how to plot the extracted data on a map’)\nexplain CRS\ncontinue on to more advanced examples (e.g. ggplot, leaflet)\nfollow on tutorial ‘producing map animations’ (e.g. with gganimate)"
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html",
    "href": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html",
    "title": "Access eReefs data",
    "section": "",
    "text": "Learn how to extract eReefs data from the AIMS server for multiple dates and points with OPeNDAP in  python.\nIn this tutorial we will look at how to get eReefs data from the AIMS server corresponding to the logged locations of tagged marine animals. Keep in mind, however, that the same methodology can be applied in any situation where we wish to extract eReefs data for a range of points with different dates of interest for each point."
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html#python-modules",
    "href": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html#python-modules",
    "title": "Access eReefs data",
    "section": "Python modules",
    "text": "Python modules\n\nimport pandas as pd  # for data analysis\nimport numpy as np  # for quik maths\nfrom janitor import clean_names  # to create consitent, 'clean' variable names\nimport folium  # a python API to interactive leaflet maps\nimport datetime as dt # to time how long the data export takes\nfrom IPython.display import display # to render, i.e. 'display', html tables\nfrom netCDF4 import Dataset"
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html#motivating-problem",
    "href": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html#motivating-problem",
    "title": "Access eReefs data",
    "section": "Motivating problem",
    "text": "Motivating problem\nThe tracking of marine animals is commonly used by researchers to gain insights into the distribution, biology, behaviour and ecology of different species. However, knowing where an animal was at a certain point in time is only one piece of the puzzle. To start to understand why an animal was where it was, we usually require information on things like: What type of habitat is present at the location? What were the environmental conditions like at the time? What other lifeforms were present at the tracked location (e.g. for food or mating)?\nIn this tutorial we will pretend that we have tracking data for Loggerhead Sea Turtles and wish to get eReefs data corresponding to the tracked points (in time and space) to understand more about the likely environmental conditions experienced by our turtles.\n\n\n\n\n\n\nRead more: Tracking marine animals\n\n\n\n\n\nMarine animals are typically tracked using either acoustic or satellite tags. These tags are attached to the animals and transmit signals back to recievers, logging the animal’s location at different points in time. In some cases other data such as depth, temperature, and animal movement profiles are recorded and the data transmitted to the recievers whenever possible.\nAcoustic tracking requires a network of recievers to be placed in the ocean in order to pick up the tags’ transmitted signals when they come within range (typically around 500 m). Acoustic tracking has the advantage of being able transmit and recieve signals underwater, however is limited by the coverage of the reciever network. In some instances, researchers do without the reciever network and follow the animals around in a boat to receive the data. The suitability of acoustic tracking depends on the study species and research question.\nSatellite tracking, on the other hand, is able to track animals over virtually the entire ocean as the tags transmit signals to a network of satellites orbiting the earth. However, unlike acoustic tags, the signals cannot be transmitted through water and the tagged animals must breach the ocean surface in order to have their location logged and any other recorded data be received. The accuracy of the logged location depends on the quality of the transmitted signal. For high-quality signals, the location uncertainty can be in the hundreds of metres, however for bad quality signals this can blow out to over 10 km."
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html#example-tracking-data",
    "href": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html#example-tracking-data",
    "title": "Access eReefs data",
    "section": "Example tracking data",
    "text": "Example tracking data\nWe will use satellite tracking data for Loggerhead Sea Turtles (Caretta caretta) provided in Strydom (2022). This data contains tracking detections which span the length of the Great Barrier Reef off the east coast of Queensland Australia from December 2021 to April 2022 (shown in Figure 1).\n\n\n\n\n\n\nThis dataset is a summarised representation of the tracking locations per 1-degree cell. This implies a coordinate uncertainty of roughly 110 km. This level of uncertainty renders the data virtually useless for most practical applications, though it will suffice for the purposes of this tutorial. Records which are landbased as a result of the uncertainty have been removed and from here on in we will just pretend that the coordinates are accurate.\n\n\n\n\n# Read in data\ndata = pd.read_csv(\"data/Loggerhead_Sea_Turtle_satellite_tracking_detections__Strydom_2022_DOI10-15468-k4s6ap.csv\")\n\n# Convert columns names from camelCase to snake_case\ndata = data.clean_names(case_type = \"snake\")\n\n# Rename some variables for easier use\ndata = data.rename(columns = {\n  \"gbif_id\": \"record_id\",\n  \"decimal_latitude\": \"latitude\",\n  \"decimal_longitude\": \"longitude\",\n  \"event_date\": \"date_time\"\n})\n\n# Ensure date_time is in the datetime data format\ndata['date_time'] = pd.to_datetime(data['date_time'])\n\n# Seperate date_time into date and time variables\ndata = data.assign(\n  date = data['date_time'].dt.strftime(\"%Y-%m-%d\"),\n  time = data['date_time'].dt.strftime(\"%H:%M\")\n)\n\n# Remove land based records (as a result of coordinate uncertainty)\nland_based_records = [4022992331, 4022992326, 4022992312, 4022992315, 4022992322, 4022992306]\ndata = data.query(\"record_id not in @land_based_records\")\n\n# Select the variables relevant to this tutorial\nselect_vars = [\"longitude\", \"latitude\", \"date\", \"time\", \"date_time\",\"record_id\", \"species\"]\ndata = data[select_vars]\n\n# View the tracking locations on an interactive map:\n# Create map centred on the mean coordinates of the tracking locations\ncentre_point = [data['latitude'].mean(), data['longitude'].mean()]\ntrack_map = folium.Map(location = centre_point, zoom_start = 4)\n\n# Add markers to map at each tracking location\nfor row in data.itertuples():\n  coords_i = [row.latitude, row.longitude]\n  marker_i = folium.Marker(\n    location = coords_i,\n    popup = row.date_time\n  ).add_to(track_map)\n\ntrack_map\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 1: Loggerhead Sea Turtle satellite tracking records (December 2021 - April 2022)"
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html#extract-data-from-server",
    "href": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html#extract-data-from-server",
    "title": "Access eReefs data",
    "section": "Extract data from server",
    "text": "Extract data from server\nWe will extend the basic methods introduced in the preceeding tutorial Accessing eReefs data from the AIMS server to extract data for a set of points and dates.\nWe will extract the eReefs 1km hydrodynamic model daily mean temperature (temp), salinity (salt), and east- and northward current velocities (u and v) corresponding to the coordinates and dates for the tracking detections shown in Table 1.\n\n# Create table of tracking detections (sort by date-time; select relevant variables)\ntbl_detections = data.\\\n  sort_values('date_time')\\\n  [['date', 'time', 'longitude', 'latitude']]\n\n# Output table in html format (hide row indices; format coordinates to their precision of 1 decimal place)\ntbl_detections = tbl_detections.style.\\\n  hide(axis = 'index').\\\n  format(precision=1)\ndisplay(tbl_detections)\n\n\n\n\n\nTable 1: Loggerhead Sea Turtle detections (Strydom, 2022)\n\n\n\n\n\n\n\n\ndate\ntime\nlongitude\nlatitude\n\n\n\n\n2021-12-21\n17:57\n152.5\n-24.5\n\n\n2022-01-02\n21:49\n153.5\n-25.5\n\n\n2022-01-05\n07:33\n152.5\n-23.5\n\n\n2022-01-06\n05:03\n151.5\n-23.5\n\n\n2022-01-09\n20:25\n151.5\n-22.5\n\n\n2022-01-13\n06:28\n151.5\n-21.5\n\n\n2022-01-14\n18:26\n150.5\n-21.5\n\n\n2022-01-17\n17:06\n150.5\n-20.5\n\n\n2022-01-19\n17:44\n149.5\n-20.5\n\n\n2022-01-21\n07:22\n149.5\n-19.5\n\n\n2022-01-23\n07:02\n148.5\n-19.5\n\n\n2022-01-27\n17:00\n147.5\n-18.5\n\n\n2022-01-30\n17:02\n146.5\n-18.5\n\n\n2022-02-02\n09:14\n146.5\n-17.5\n\n\n2022-02-03\n21:37\n153.5\n-24.5\n\n\n2022-02-06\n18:25\n146.5\n-16.5\n\n\n2022-02-07\n07:15\n145.5\n-16.5\n\n\n2022-02-09\n18:33\n145.5\n-15.5\n\n\n2022-02-12\n08:59\n153.5\n-26.5\n\n\n2022-02-12\n10:34\n145.5\n-14.5\n\n\n2022-03-25\n07:10\n144.5\n-13.5\n\n\n2022-04-01\n18:41\n143.5\n-12.5\n\n\n2022-04-09\n22:00\n143.5\n-11.5\n\n\n2022-04-14\n06:31\n143.5\n-10.5\n\n\n2022-04-21\n10:30\n143.5\n-9.5\n\n\n\n\n\n\n\n\n\nWe will take advantage of the consistent file naming on the server to extract the data of interest programatically. We will first need to copy the OPeNDAP data link for one of the files within the correct model and aggregation folders and then replace the date.\nSelecting a random date within the daily aggregated data (daily-daily; one data file per day) for the 1km hydro model (gbr1_2.0), we see the files have the naming format:\nhttps://thredds.ereefs.aims.gov.au/thredds/dodsC/ereefs/gbr1_2.0/daily-daily/EREEFS_AIMS-CSIRO_gbr1_2.0_hydro_daily-daily-YYYY-MM-DD.nc\nWe will now write a script which extracts the data for the dates and coordinates in Table 1. For each unique date we will open the corresponding file on the server and extract the daily mean temperature, salinity, northward and southward current velocities for each set of coordinates corresponding to the date.\n\n# GET DATA FOR EACH DATE AND COORDINATE (LAT LON) PAIR\nt_start = dt.datetime.now() # to track run time of extraction\n\n## 1. Setup variables for data extraction\n# Server file name = &lt;file_prefix&gt;&lt;date (yyyy-mm-dd)&gt;&lt;file_suffix&gt;\nfile_prefix = \"https://thredds.ereefs.aims.gov.au/thredds/dodsC/ereefs/gbr1_2.0/daily-daily/EREEFS_AIMS-CSIRO_gbr1_2.0_hydro_daily-daily-\"\nfile_suffix = \".nc\"\n\n# Table of dates and coordinates for which to extract data (dates as character string)\ndetections = data[['date', 'longitude', 'latitude']].drop_duplicates()\n\nextracted_data = pd.DataFrame() # to save the extracted data\ndates = detections['date'].unique() # unique dates for which to open server files\n\n## 2. For each date of interest, open a connection to the corresponding data file on the server\nfor i in range(len(dates)):\n  date_i = dates[i]\n\n  # Open file\n  file_name_i = file_prefix + dates[i] + file_suffix\n  server_file_i = Dataset(file_name_i)\n\n  # Coordinates for which to extract data for the current date\n  coordinates_i = detections.query(\"date == @date_i\")\n\n  # Get all coordinates in the open file (each representing the center-point of the corresponding grid cell)\n  server_lons_i = server_file_i.variables['longitude'][:]\n  server_lats_i = server_file_i.variables['latitude'][:]\n\n  ## 3. For each coordinate (lon, lat) for the current date, get the data for the closest grid cell (1km^2) from the open server file\n  for row_j in coordinates_i.itertuples():\n\n    # Current coordinate of interest\n    lon_j = row_j.longitude\n    lat_j = row_j.latitude\n\n    # Find the index of the grid cell containing our coordinate of interest (i.e. the center-point closest to our point of interest)\n    lon_index_j = np.argmin(np.abs(server_lons_i - lon_j))\n    lat_index_j = np.argmin(np.abs(server_lats_i - lat_j))\n    # Note: This will return the closest grid cell, even for coordinates outside of the eReefs model boundary\n\n    # Setup the dimension indices for which to extract data (needs to be a tuple; recall that python starts counting at 0)\n    dim_ind = tuple([0, 15, lat_index_j, lon_index_j])\n    ########################################\n    # Recall the order of the dimensions (time, k, latitude, longitude) from the previous tutorial. Therefore we want [time = 1 (as we're using the daily files this is the only option), k = 15 corresponding to a depth of 0.5m, lat_index_j, lon_index_j]. If you are still confused, go back to the previous tutorial or have a look at the structure of one of the server files by uncommenting the following 5 lines of code:\n    # not_yet_run = True  # used so the following lines are only run once\n    # if not_yet_run:\n    #   print(server_file_i.dimensions)\n    #   print(server_file_i.variables)\n    #   not_yet_run = False\n    ########################################\n\n    # Get the data for the grid cell containing our point of interest\n    temp_j = server_file_i.variables['temp'][dim_ind]\n    salt_j = server_file_i.variables['salt'][dim_ind]\n    u_j = server_file_i.variables['u'][dim_ind]\n    v_j = server_file_i.variables['v'][dim_ind]\n\n    extracted_data_j = pd.DataFrame({\n      'date': [date_i],\n      'lon': [lon_j],\n      'lat': [lat_j],\n      'temp': [temp_j],\n      'salt': [salt_j],\n      'u': [u_j],\n      'v': [v_j]\n    })\n\n    ## 4. Save data in memory and repeat for next date-coordinate pair\n    extracted_data = pd.concat([extracted_data, extracted_data_j], ignore_index = True)\n\n  # Close connection to open server file and move to the next date\n  server_file_i.close()\n\n# Calculate the run time of the extraction\nt_stop = dt.datetime.now()\nextract_time = t_stop - t_start\nextract_mins = int(extract_time.total_seconds()/60)\nextract_secs = int(extract_time.total_seconds() % 60)\nprint(\"Data extracted for\", len(detections), \"points from\", len(dates), \"files. \\nExtraction time:\", extract_mins, \"min\", extract_secs, \"sec.\")\n\nData extracted for 25 points from 24 files. \nExtraction time: 1 min 9 sec.\n\n\nOur extracted data is shown below in Table 2.\n\n\n\n\n\n\nIn the code above we match the closest eReefs model grid cell to each point in our list of coordinates (i.e. for each tracking detection). This will therefore match grid cells to all the coordinates, even if they are not within the eReefs model boundary. This behaviour may be useful when we have points right along the coastline as the eReefs models have small gaps at many points along the coast (see image below). However, in other cases this behaviour may not be desirable. For example, if we had points down near Sydney they would be matched to the closest eReefs grid cells (somewhere up near Brisbane) and the extracted data would be erroneous.\n\n\n\n\n\n# Output table in html format (sort by date; hide row indices; format coordinates to their precision of 1 decimal place, temp & salt to 2 dp, u & v to 3 dp)\ntbl_extracted = extracted_data.sort_values('date').style.\\\n  hide(axis = 'index').\\\n  format({\n    **dict.fromkeys(['lon', 'lat'], '{:.1f}'),\n    **dict.fromkeys(['temp', 'salt'], '{:.2f}'),\n    **dict.fromkeys(['u', 'v'], '{:.3f}')\n  })\ndisplay(tbl_extracted)\n\n\n\n\n\nTable 2: Extracted daily mean temperature, salinity, and east- and northward current velocities (u, v respectively) for Loggerhead Sea Turtle detections (Strydom, 2022)\n\n\n\n\n\n\n\n\ndate\nlon\nlat\ntemp\nsalt\nu\nv\n\n\n\n\n2021-12-21\n152.5\n-24.5\n28.09\n35.26\n0.071\n-0.020\n\n\n2022-01-02\n153.5\n-25.5\n26.00\n35.30\n-0.035\n0.038\n\n\n2022-01-05\n152.5\n-23.5\n25.64\n35.23\n0.030\n0.015\n\n\n2022-01-06\n151.5\n-23.5\n28.13\n35.41\n-0.033\n0.015\n\n\n2022-01-09\n151.5\n-22.5\n29.24\n35.34\n0.001\n-0.103\n\n\n2022-01-13\n151.5\n-21.5\n28.42\n35.25\n-0.095\n0.016\n\n\n2022-01-14\n150.5\n-21.5\n28.99\n35.40\n-0.064\n-0.047\n\n\n2022-01-17\n150.5\n-20.5\n29.39\n35.34\n-0.066\n-0.178\n\n\n2022-01-19\n149.5\n-20.5\n29.92\n35.48\n0.018\n-0.104\n\n\n2022-01-21\n149.5\n-19.5\n29.58\n35.12\n-0.161\n-0.035\n\n\n2022-01-23\n148.5\n-19.5\n28.99\n35.26\n-0.137\n-0.022\n\n\n2022-01-27\n147.5\n-18.5\n29.46\n33.98\n0.265\n0.004\n\n\n2022-01-30\n146.5\n-18.5\n30.15\n34.60\n-0.184\n0.143\n\n\n2022-02-02\n146.5\n-17.5\n30.59\n34.72\n0.128\n-0.054\n\n\n2022-02-03\n153.5\n-24.5\n27.06\n35.32\n0.550\n-0.797\n\n\n2022-02-06\n146.5\n-16.5\n29.14\n34.70\n-0.122\n-0.100\n\n\n2022-02-07\n145.5\n-16.5\n30.37\n34.06\n-0.104\n0.200\n\n\n2022-02-09\n145.5\n-15.5\n29.54\n34.74\n-0.120\n0.064\n\n\n2022-02-12\n153.5\n-26.5\n26.80\n35.38\n-0.124\n0.006\n\n\n2022-02-12\n145.5\n-14.5\n29.90\n34.72\n-0.081\n0.034\n\n\n2022-03-25\n144.5\n-13.5\n29.04\n34.75\n-0.457\n0.360\n\n\n2022-04-01\n143.5\n-12.5\n30.14\n34.44\n0.025\n-0.018\n\n\n2022-04-09\n143.5\n-11.5\n29.93\n34.56\n-0.097\n0.144\n\n\n2022-04-14\n143.5\n-10.5\n29.48\n34.42\n-0.041\n0.078\n\n\n2022-04-21\n143.5\n-9.5\n29.56\n34.19\n0.059\n0.034"
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html#match-extracted-data-to-tracking-data",
    "href": "tutorials/python/access-ereefs-data-server/programmatic-server-access/programmatic_server_access_python.html#match-extracted-data-to-tracking-data",
    "title": "Access eReefs data",
    "section": "Match extracted data to tracking data",
    "text": "Match extracted data to tracking data\nWe will match up the eReefs data with our tracking detections by combining the two datasets based on common date, longitude and latitude values.\n\n# Rename lon and lat columns of extracted_data to longitude, latitude (to match those of data)\nextracted_data = extracted_data.rename(columns = {\n  'lon': 'longitude',\n  'lat': 'latitude'\n})\n\n# Merge the two datasets based on common date, lon and lat values\ncombined_data = pd.merge(\n  data, extracted_data,\n  on = ['date', 'longitude', 'latitude']\n)\n\n# Print the combined data (reorder columns; sort by date and time; format numeric columns' decimal places)\ntbl_combined = combined_data.\\\n  reindex(columns = ['date', 'time', 'longitude', 'latitude', 'record_id', 'temp', 'salt', 'u', 'v']).\\\n  sort_values(by = ['date', 'time']).\\\n  style.\\\n  hide(axis = 'index').\\\n  format({\n    **dict.fromkeys(['longitude', 'latitude'], '{:.1f}'),\n    **dict.fromkeys(['temp', 'salt'], '{:.2f}'),\n    **dict.fromkeys(['u', 'v'], '{:.3f}')\n  })\ndisplay(tbl_combined)\n\n\n\n\n\nTable 3: Loggerhead Sea Turtle (Caretta caretta) tracking detections (Strydom, 2022) and corresponding eReefs daily mean temperature, salinity, east- and northward current velocities (u, v respectively).\n\n\n\n\n\n\n\n\ndate\ntime\nlongitude\nlatitude\nrecord_id\ntemp\nsalt\nu\nv\n\n\n\n\n2021-12-21\n17:57\n152.5\n-24.5\n4022992328\n28.09\n35.26\n0.071\n-0.020\n\n\n2022-01-02\n21:49\n153.5\n-25.5\n4022992329\n26.00\n35.30\n-0.035\n0.038\n\n\n2022-01-05\n07:33\n152.5\n-23.5\n4022992304\n25.64\n35.23\n0.030\n0.015\n\n\n2022-01-06\n05:03\n151.5\n-23.5\n4022992302\n28.13\n35.41\n-0.033\n0.015\n\n\n2022-01-09\n20:25\n151.5\n-22.5\n4022992319\n29.24\n35.34\n0.001\n-0.103\n\n\n2022-01-13\n06:28\n151.5\n-21.5\n4022992308\n28.42\n35.25\n-0.095\n0.016\n\n\n2022-01-14\n18:26\n150.5\n-21.5\n4022992318\n28.99\n35.40\n-0.064\n-0.047\n\n\n2022-01-17\n17:06\n150.5\n-20.5\n4022992330\n29.39\n35.34\n-0.066\n-0.178\n\n\n2022-01-19\n17:44\n149.5\n-20.5\n4022992320\n29.92\n35.48\n0.018\n-0.104\n\n\n2022-01-21\n07:22\n149.5\n-19.5\n4022992316\n29.58\n35.12\n-0.161\n-0.035\n\n\n2022-01-23\n07:02\n148.5\n-19.5\n4022992323\n28.99\n35.26\n-0.137\n-0.022\n\n\n2022-01-27\n17:00\n147.5\n-18.5\n4022992327\n29.46\n33.98\n0.265\n0.004\n\n\n2022-01-30\n17:02\n146.5\n-18.5\n4022992314\n30.15\n34.60\n-0.184\n0.143\n\n\n2022-02-02\n09:14\n146.5\n-17.5\n4022992301\n30.59\n34.72\n0.128\n-0.054\n\n\n2022-02-03\n21:37\n153.5\n-24.5\n4022992313\n27.06\n35.32\n0.550\n-0.797\n\n\n2022-02-06\n18:25\n146.5\n-16.5\n4022992303\n29.14\n34.70\n-0.122\n-0.100\n\n\n2022-02-07\n07:15\n145.5\n-16.5\n4022992310\n30.37\n34.06\n-0.104\n0.200\n\n\n2022-02-09\n18:33\n145.5\n-15.5\n4022992311\n29.54\n34.74\n-0.120\n0.064\n\n\n2022-02-12\n08:59\n153.5\n-26.5\n4022992324\n26.80\n35.38\n-0.124\n0.006\n\n\n2022-02-12\n10:34\n145.5\n-14.5\n4022992325\n29.90\n34.72\n-0.081\n0.034\n\n\n2022-03-25\n07:10\n144.5\n-13.5\n4022992309\n29.04\n34.75\n-0.457\n0.360\n\n\n2022-04-01\n18:41\n143.5\n-12.5\n4022992307\n30.14\n34.44\n0.025\n-0.018\n\n\n2022-04-09\n22:00\n143.5\n-11.5\n4022992317\n29.93\n34.56\n-0.097\n0.144\n\n\n2022-04-14\n06:31\n143.5\n-10.5\n4022992321\n29.48\n34.42\n-0.041\n0.078\n\n\n2022-04-21\n10:30\n143.5\n-9.5\n4022992305\n29.56\n34.19\n0.059\n0.034\n\n\n\n\n\n\n\n\nHooray! We now have our combined dataset of the Loggerhead Sea Turtle tracking detections and the corresponding eReefs daily aggregated data (Table 3).\nStrydom A. 2022. Wreck Rock Turtle Care - satellite tracking. Data downloaded from OBIS-SEAMAP; originated from Satellite Tracking and Analysis Tool (STAT). DOI: 10.15468/k4s6ap accessed via GBIF.org on 2023-02-17."
  },
  {
    "objectID": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html",
    "href": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html",
    "title": "Processing eReefs data ",
    "section": "",
    "text": "Learn how to use eReefs data to answer the question ‘How strong does the wind need to be to set the direction of the surface ocean currents?’ in  python."
  },
  {
    "objectID": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#motivating-problem",
    "href": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#motivating-problem",
    "title": "Processing eReefs data ",
    "section": "Motivating problem",
    "text": "Motivating problem\nThe East Australian Current (EAC) usually is a strong southward current near Myrmidon and Davies Reefs. During winter months the wind moves in a north eastern direction in the near opposite direction to the EAC. When the wind is low the surface currents are dominated by the EAC. As the wind picks up, at some speed the wind overpowers the EAC and the surface current moves in the direction of the wind.\nWe will use the AIMS eReefs extraction tool to extract data for two locations of interest:\n\nMyrmidon Reef which is on the far outer edge of the GBR and is almost right in the middle of the southern Eastern Australian Current; and\nDavies Reef which is further in the reef matrix, but in a similar sector of the GBR.\n\n\nWe then process the data and investigate the relationship between the strength of the wind and the direction of the surface currents for the two locations."
  },
  {
    "objectID": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#analysis-method",
    "href": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#analysis-method",
    "title": "Processing eReefs data ",
    "section": "Analysis method",
    "text": "Analysis method\nTo determine the relation between the wind and the surface currents we will use the AIMS eReefs extraction tool to pull out hourly time series wind and current data for our two locations of interest. We will then look at the correlation between the wind and current vectors, where a correlation of 1 indicates they are pointing in the same direction, and -1 indicated they are in opposite directions."
  },
  {
    "objectID": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#setting-up-to-get-the-data",
    "href": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#setting-up-to-get-the-data",
    "title": "Processing eReefs data ",
    "section": "Setting up to get the data",
    "text": "Setting up to get the data\nTo extract the time series data using the extraction tool we need to create a CSV file containing the sites of interest. This file needs to contain the coordinates and names of the sites. To create this I first added my points manually in Google Earth Pro. This was done to simply get the location of Myrmidon and Davies Reefs. Using Google Earth to create your CSV file for the extraction tool is only useful if you don’t already know the coordinates of your sites.\nScreenshot of Google Earth Pro with Myromidon and Davies reef sites\nThe points can be added using the Add placemark tool (looks like a pin). The locations can be seen by displaying the placemark properties. The resulting KML file can be found here: extraction-tool-locations.kml.\nThe location of the two sites were copied to create the CSV file for the data extraction tool."
  },
  {
    "objectID": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#extracting-the-data",
    "href": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#extracting-the-data",
    "title": "Processing eReefs data ",
    "section": "Extracting the data",
    "text": "Extracting the data\nThe CSV file was uploaded to the AIMS extraction tool and the extraction was performed with the following settings:\n\nData collection: GBR1 Hydro (Version 2)\nVariables:\n\nEastward wind speed (wspeed_u)\nNorthward wind speed (wspeed_v)\nNorthward current (v)\nEastward current (u)\n\nDate range: 1 January 2019 - 31 December 2019\nTime step: hourly\nDepths: -2.35 m\n\nOnce the extraction request was submitted the dataset was created after an one hour of processing the data was available for download from Extraction request: Example dataset: Wind-vs-Current at Davies and Myrmidon Reefs (2019)."
  },
  {
    "objectID": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#downloading-the-data",
    "href": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#downloading-the-data",
    "title": "Processing eReefs data ",
    "section": "Downloading the data",
    "text": "Downloading the data\nIn this notebook we will download the data using scripting. There is no need to re-run the extraction request as each extraction performed by the extraction tool has a permanent public page created for it that can be used to facilitate sharing of the data.\nLet’s first create a temporary folder to contain the downloaded data. Note: The temp folder is excluded using the .gitignore so it is not saved to the code repository, which is why we must reproduce it.\n\nimport os\nif not os.path.exists('temp'):\n    os.makedirs('temp')\n\nNow let’s download the data. The file to download is 12.9 MB and so this download might take a little while. To allow us to re-run this script without having to wait for the download each time we first check that the download has not already been done.\n\nimport urllib.request\nextractionfile = os.path.join('temp','2009.c451dc3-collected.csv')  # Use os.path.join so the script will work cross-platform\n\nif not os.path.exists(extractionfile):\n    print(\"Downloading extraction data ...\")\n    url = 'https://api.ereefs.aims.gov.au/data-extraction/request/2009.c451dc3/files/2009.c451dc3-collected.csv'\n    req = urllib.request.urlretrieve(url, extractionfile)\n    print(req)\nelse:\n    print(\"Skipping redownloading extraction data\")\n\nSkipping redownloading extraction data"
  },
  {
    "objectID": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#reading-and-cleaning-the-data",
    "href": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#reading-and-cleaning-the-data",
    "title": "Processing eReefs data ",
    "section": "Reading and cleaning the data",
    "text": "Reading and cleaning the data\nRead the resulting CSV file into a Pandas data frame.\n\nimport pandas as pd\nfrom janitor import clean_names\ndf = pd.read_csv(extractionfile).clean_names()\ndf.head()\n\n\n\n\n\n\n\n\naggregated_date_time\nvariable\ndepth\nsite_name\nlatitude\nlongitude\nmean\nmedian\np5\np95\nlowest\nhighest\n\n\n\n\n0\n2019-01-01T00:00\nwspeed_u\n99999.90\nMyrmidon Reef\n-18.265599\n147.389028\n-9.568488\n-9.568488\n-9.568488\n-9.568488\n-9.568488\n-9.568488\n\n\n1\n2019-01-01T00:00\nwspeed_u\n99999.90\nDavies Reef\n-18.822840\n147.645209\n-8.880175\n-8.880175\n-8.880175\n-8.880175\n-8.880175\n-8.880175\n\n\n2\n2019-01-01T00:00\nwspeed_v\n99999.90\nMyrmidon Reef\n-18.265599\n147.389028\n3.260430\n3.260430\n3.260430\n3.260430\n3.260430\n3.260430\n\n\n3\n2019-01-01T00:00\nwspeed_v\n99999.90\nDavies Reef\n-18.822840\n147.645209\n2.756750\n2.756750\n2.756750\n2.756750\n2.756750\n2.756750\n\n\n4\n2019-01-01T00:00\nv\n-2.35\nMyrmidon Reef\n-18.265599\n147.389028\n0.047311\n0.047311\n0.047311\n0.047311\n0.047311\n0.047311\n\n\n\n\n\n\n\nThe first thing we might notice about the data is that, somewhat confusingly, we have a bunch of aggregation statistics (mean, median, p5, p95, lowest, highest) which all take the same value. This is because we have extracted hourly “aggregated” data, but the time step for the eReefs model is also hourly. Therefore each row represents an aggregation over a single data point. Don’t worry if you are confused by this, it’s not important. Just think of this as a quirk of the eReefs data extraction tool. We’ll clean this up now by replacing the aggregation statistics with a single column called value and rename aggregated_date_time to date_time, to avoid any further confusion.\n\n# Create 'value' column, remove aggregation statistic columns, rename aggregated_date_time\ndf2 = df.\\\n  assign(value = df['mean']).\\\n  drop(columns=['mean', 'median', 'p5', 'p95', 'lowest','highest']).\\\n  rename(columns={\"aggregated_date_time\": \"date_time\"})\ndf2.head()\n\n\n\n\n\n\n\n\ndate_time\nvariable\ndepth\nsite_name\nlatitude\nlongitude\nvalue\n\n\n\n\n0\n2019-01-01T00:00\nwspeed_u\n99999.90\nMyrmidon Reef\n-18.265599\n147.389028\n-9.568488\n\n\n1\n2019-01-01T00:00\nwspeed_u\n99999.90\nDavies Reef\n-18.822840\n147.645209\n-8.880175\n\n\n2\n2019-01-01T00:00\nwspeed_v\n99999.90\nMyrmidon Reef\n-18.265599\n147.389028\n3.260430\n\n\n3\n2019-01-01T00:00\nwspeed_v\n99999.90\nDavies Reef\n-18.822840\n147.645209\n2.756750\n\n\n4\n2019-01-01T00:00\nv\n-2.35\nMyrmidon Reef\n-18.265599\n147.389028\n0.047311\n\n\n\n\n\n\n\nMuch better! Now it is clear to see that the data is in long format. That is, a single row for each value and a separate column describing the meaning of the value — in this case the column variable describes what the column value means. Converting the data into tidy format will help with subsequent analyses (and is probably also the format you are most comfortable working with). When data is in tidy format, each variable has its own column, each observation has its own row, and each value has its own cell.\nIf we think of the wind and current velocities as the things being “measured”, then the observations in the dataset are the values of the measurements for each time point at each site. Therefore we would like a single row for each unique combination of date_time and site_name (or latitude and longitude pair). Let’s see if this is what we get we try to “widen” the data into tidy format.\n\n# Try pivoting the data into tidy format\ndf2.pivot(\n  index = [\"site_name\", \"latitude\", \"longitude\", \"date_time\", \"depth\"],\n  columns=\"variable\",\n  values=\"value\"\n).head()\n\n\n\n\n\n\n\n\n\n\n\nvariable\nu\nv\nwspeed_u\nwspeed_v\n\n\nsite_name\nlatitude\nlongitude\ndate_time\ndepth\n\n\n\n\n\n\n\n\nDavies Reef\n-18.82284\n147.645209\n2019-01-01T00:00\n-2.35\n-0.048835\n0.100391\nNaN\nNaN\n\n\n99999.90\nNaN\nNaN\n-8.880175\n2.756750\n\n\n2019-01-01T01:00\n-2.35\n-0.090807\n-0.036129\nNaN\nNaN\n\n\n99999.90\nNaN\nNaN\n-8.749271\n2.753487\n\n\n2019-01-01T02:00\n-2.35\n-0.118289\n-0.180480\nNaN\nNaN\n\n\n\n\n\n\n\nWith all the NAs in the wind and current speed columns, this doesn’t look tidy at all! And it’s because we forgot to account for depth. However, we can notice that depth is actually a redundant variable in this dataset. That’s because wind speed implies a NA depth value (coded as 10000m in the extracted data) and current implies a depth of -2.35m (as this is the only depth we chose to extract). Therefore we can just remove depth entirely from our dataset without losing any information, that is, as long as we remember that the current relates to a depth of -2.35m. For the forgetful among us, we could rename the current variables to v_2.35m and u_2.35m. In fact moving the depth into the variable names would be a good solution to create a tidy dataset if we had selected multiple depths.\n\n# Pivot into tidy format (excluding depth)\ndf_tidy = df2.pivot(\n  index = [\"site_name\", \"latitude\", \"longitude\", \"date_time\"],\n  columns=\"variable\",\n  values=\"value\"\n)\ndf_tidy.head()\n\n\n\n\n\n\n\n\n\n\nvariable\nu\nv\nwspeed_u\nwspeed_v\n\n\nsite_name\nlatitude\nlongitude\ndate_time\n\n\n\n\n\n\n\n\nDavies Reef\n-18.82284\n147.645209\n2019-01-01T00:00\n-0.048835\n0.100391\n-8.880175\n2.756750\n\n\n2019-01-01T01:00\n-0.090807\n-0.036129\n-8.749271\n2.753487\n\n\n2019-01-01T02:00\n-0.118289\n-0.180480\n-8.463824\n2.587230\n\n\n2019-01-01T03:00\n-0.110750\n-0.278911\n-8.223801\n2.112059\n\n\n2019-01-01T04:00\n-0.079472\n-0.312360\n-8.565171\n2.711215\n\n\n\n\n\n\n\nMuch better! Now each row gives the wind and current speeds for different sites at different points in time."
  },
  {
    "objectID": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#correlation",
    "href": "tutorials/python/process-ereefs-data/correlation/correlation_wind_and_current_python.html#correlation",
    "title": "Processing eReefs data ",
    "section": "Correlation",
    "text": "Correlation\nOur aim is to create an index that estimates the correlation of the current and the wind vectors.\nThe correlation of the current and wind vectors can be estimated based using the dot product. An overview of the relationship between correlation and using the dot product is described in Geometric Interpretation of the Correlation between Two Variables. The correlation between the two vectors is given by:\n\\[\nr = \\cos(\\theta) = \\frac{a \\cdot b}{||a||\\cdot||b||}\n\\]\nwhere \\(a \\cdot b\\) is the dot product between the two vectors and \\(||a||\\) and \\(||b||\\) are the magnitudes of the vectors. The dot product can be calculated as\n\\[\na \\cdot b = a_x \\times b_x + a_y \\times b_y\n\\]\nand the magnitude of the vectors as\n\\[\n||a|| = \\sqrt{a^2_x + a^2_y} \\;\\;, \\; \\;\\;\\;\\;\n||b|| = \\sqrt{b^2_x + b^2_y}\n\\]\n\nimport numpy as np\ndf_corr = df_tidy\ndf_corr['currentmag'] = np.sqrt(df_corr['u']**2 + df_corr['v']**2)\ndf_corr['windmag'] = np.sqrt(df_corr['wspeed_u']**2 + df_corr['wspeed_v']**2)\ndf_corr['windcurrentcorr'] = (df_corr['u'] * df_corr['wspeed_u']  +  df_corr['v'] * df_corr['wspeed_v']) / (df_corr['currentmag'] * df_corr['windmag'])\ndf_corr.head()\n\n\n\n\n\n\n\n\n\n\nvariable\nu\nv\nwspeed_u\nwspeed_v\ncurrentmag\nwindmag\nwindcurrentcorr\n\n\nsite_name\nlatitude\nlongitude\ndate_time\n\n\n\n\n\n\n\n\n\n\n\nDavies Reef\n-18.82284\n147.645209\n2019-01-01T00:00\n-0.048835\n0.100391\n-8.880175\n2.756750\n0.111638\n9.298236\n0.684380\n\n\n2019-01-01T01:00\n-0.090807\n-0.036129\n-8.749271\n2.753487\n0.097730\n9.172319\n0.775325\n\n\n2019-01-01T02:00\n-0.118289\n-0.180480\n-8.463824\n2.587230\n0.215790\n8.850428\n0.279727\n\n\n2019-01-01T03:00\n-0.110750\n-0.278911\n-8.223801\n2.112059\n0.300094\n8.490683\n0.126259\n\n\n2019-01-01T04:00\n-0.079472\n-0.312360\n-8.565171\n2.711215\n0.322311\n8.984033\n-0.057391\n\n\n\n\n\n\n\nLet’s look at the relationship between the wind and current as a function of the wind speed. Here we are considering each hourly sample as an independent estimate of the relationship. In reality this is not the case as the longer the wind blows the more effect it will have on the current. As this is just a coding example and not an in-depth analysis we don’t need to worry about this limitation of the analysis.\nLet’s pull out the data for Davies and Myrmidon Reefs separately so they are easy to plot.\n\ndavies = df_corr.query('site_name == \"Davies Reef\"')\nmyrmidon = df_corr.query('site_name == \"Myrmidon Reef\"')\nmyrmidon.head()\n\n\n\n\n\n\n\n\n\n\nvariable\nu\nv\nwspeed_u\nwspeed_v\ncurrentmag\nwindmag\nwindcurrentcorr\n\n\nsite_name\nlatitude\nlongitude\ndate_time\n\n\n\n\n\n\n\n\n\n\n\nMyrmidon Reef\n-18.265599\n147.389028\n2019-01-01T00:00\n0.012498\n0.047311\n-9.568488\n3.260430\n0.048934\n10.108727\n0.070078\n\n\n2019-01-01T01:00\n-0.029212\n-0.013297\n-9.420339\n3.528335\n0.032096\n10.059420\n0.707019\n\n\n2019-01-01T02:00\n-0.060520\n-0.066542\n-9.333500\n3.765564\n0.089947\n10.064477\n0.347188\n\n\n2019-01-01T03:00\n-0.078321\n-0.111699\n-9.281394\n3.784521\n0.136421\n10.023316\n0.222466\n\n\n2019-01-01T04:00\n-0.076759\n-0.132760\n-8.527690\n3.885759\n0.153353\n9.371266\n0.096513\n\n\n\n\n\n\n\nLet’s create a scatter plot to see if there is a relationship between the wind and currents.\n\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax=fig.add_axes([0,0,1,1])\nax.scatter(myrmidon[\"windmag\"],myrmidon[\"windcurrentcorr\"], color='r', s=1)\nax.scatter(davies[\"windmag\"],davies[\"windcurrentcorr\"], color='b', s=1)\nax.set_xlabel('Wind speed (m/s)')\nax.set_ylabel('Wind-current correlation')\nax.set_title('Correlation between wind and surface current (hourly data, 2019)')\n\nText(0.5, 1.0, 'Correlation between wind and surface current (hourly data, 2019)')\n\n\n\n\n\n\n\n\n\nThis scatter plot shows that the relationship between wind and current is weak. This is not surprising given that we are considering just the hourly samples, with no consideration for how long the wind has been blowing. At low wind conditions the current has an even chance of being aligned with the wind (correlation \\(r= 1\\)) as in the opposite direction (correlation \\(r= -1\\)), however in high wind we can see that there is much more chance that the currents are aligned with the wind.\nTo understand this relationship better we want to understand how much the wind nudges the current in its direction. If we bin the wind speeds then collect all the correlation samples in each bin then we can see if they average to zero (indicating that there is no relationship between the wind and current) or there is average alignment.\n\nfrom scipy import stats\nwind = davies[\"windmag\"]\ncurrent = davies[\"windcurrentcorr\"]\nbin_means, bin_edges, binnumber = stats.binned_statistic(wind, current, 'mean', bins=20)\nplt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=5,\n           label='Davies Reef')\n\nwind = myrmidon[\"windmag\"]\ncurrent = myrmidon[\"windcurrentcorr\"]\nbin_means, bin_edges, binnumber = stats.binned_statistic(wind, current, 'mean', bins=20)\nplt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='r', lw=5,\n           label='Myrmidon Reef')\n\nplt.xlabel('Wind speed (m/s)')\nplt.ylabel('Wind-current correlation')\nplt.title('Mean correlation between wind and surface current (hourly data, 2019)')\nplt.legend()\n\n\n\n\n\n\n\n\nFrom this we can see that for wind speeds below about 8 m/s the surface current direction is unrelated to the wind. Above this wind speed the surface current is increasingly determined by the direction of the wind. By the time the wind is 16 m/s the direction of the surface current is dominated by the wind direction.\n\n\n\n\n\n\nIt should be remembered that this analysis is based on the eReefs Hydrodynamic model and as such is not based on real data. The eReefs model has however been tuned to accurately capture the flow dynamics of the GBR and so we would expect the estimates from this analysis to be approximately correct."
  },
  {
    "objectID": "tutorials/general/intro_to_ereefs.html",
    "href": "tutorials/general/intro_to_ereefs.html",
    "title": "eReefs overview",
    "section": "",
    "text": "A gentle introduction to eReefs — what it is and how to use it."
  },
  {
    "objectID": "tutorials/general/intro_to_ereefs.html#what-is-ereefs",
    "href": "tutorials/general/intro_to_ereefs.html#what-is-ereefs",
    "title": "eReefs overview",
    "section": "What is eReefs?",
    "text": "What is eReefs?\nThe Great Barrier Reef (the Reef) is the largest coral reef system on earth, covering an area of about 350,000 km\\(^2\\) off the coast of Queensland, Australia. It is an ecosystem of immense complexity, with a myriad of physical, chemical and biological processes interacting through time to shape the reef which we see today and into the future.\nTracking the health of the Reef has become increasingly important as the cumulative impacts of a range of stressors intensify under a changing climate and jeopardise the future of the Reef as we know it. While the Reef’s great economic, cultural and social value does fuel a considerable amount of research and investment, its shear size means that observations of its condition are relatively few and far between. eReefs aims to solve this problem by using computer modelling to simulate some of the key processes shaping the Reef — from the catchments, to the rivers and estuaries, to the reef lagoon, reef matrix and open ocean — to better understand the ecosystem as a whole and monitor its health in (near) real-time.\nPut simply, eReefs is an information platform which takes data from a wide range of sources and passes it through a series of computer models to get predictions across a number of key attributes of the Great Barrier Reef, such as water temperature, water chemistry, and nutrient and sediment loads. These predictions are routinely generated across the entire extent of the Great Barrier Reef, in near-real-time. The end result is a collection of datasets which are, essentially, snapshots of the Reef’s condition through time. These datasets are the core output of eReefs, with a number of other data products, visualisations, and services built upon them.\nA simplified overview of the eReefs information platform is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: A simplified overview of the eReefs information platform."
  },
  {
    "objectID": "tutorials/general/intro_to_ereefs.html#the-ereefs-platform",
    "href": "tutorials/general/intro_to_ereefs.html#the-ereefs-platform",
    "title": "eReefs overview",
    "section": "The eReefs Platform",
    "text": "The eReefs Platform\neReefs is often described as an ‘information platform’ to capture the wide range of information services which can be built upon it. Some examples of these services, along with the different elements which make up the eReefs platform, are shown in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: A schematic overview of the eReefs platform.\n\n\n\n\n\nMany different Research Components are used to transform input data into a range of data products. These are mostly computer models (more specifically, numerical models), and include:\n\nOcean Colour - An optical model which uses satellite imagery to infer the optical properties, i.e. colour, of the waters in the Great Barrier Reef lagoon. The colour of the water actually contains a lot of information on water quality, including things like sediment loads and types, the location of river plumes and phytoplankton blooms, and water circulation structures.\nRegional Models - These are the core of the eReefs platform. They include a hydrodynamic model to predict the physical state of the system, a sediment transport model predicting the fate of suspended fine sediments, and a biogeochemical model for water column and benthic production, water quality and nutrient cycling. These models are run across the entire extent of the Great Barrier Reef, from the coastline to the open ocean, at either 4 km or 1 km resolution.\nRelocatable Coastal Models (RECOM) - An automated re-locatable modelling system capable of generating high resolution models of hydrodynamics, waves, sediment transport and biogeochemistry, nested within the 4 km or 1 km regional models. Essentially, RECOM implements the regional models over a smaller, user-defined area with much higher resolution.\nMarine Forecasting Models - Provide what is essentially a marine weather forecast, capable of providing short-term predictions of water temperature, flow, mixing and quality, among other things.\nCatchment Models - Predict the flow of water entering the catchments via rainfall and exiting into the ocean via rivers, as well as the introduction of sediments and pollutants via run-off.\n\nIn Figure 3 we can see the extents of the 4 km and 1 km regional models and the catchment models, along with the locations of the in-field input data streams.\n\n\n\n\n\n\n\n\nFigure 3: Map of the eReefs GBR1 (1 km resolution) and GBR4 (4 km resolution) model domains; the observational infrastructure which routinely collects in-field data for use as model inputs and model validation; and the extent of catchment models which predict water flows and run-off from the land."
  },
  {
    "objectID": "tutorials/general/intro_to_ereefs.html#the-ereefs-regional-models",
    "href": "tutorials/general/intro_to_ereefs.html#the-ereefs-regional-models",
    "title": "eReefs overview",
    "section": "The eReefs Regional Models",
    "text": "The eReefs Regional Models\nThe Regional Models are the core of the eReefs platform and include three main models, the:\n\nHydrodynamic model - predicts the physical state of the system\nSediment transport model - predicts the fate of suspended fine sediments\nBiogeochemical model - predicts many attributes of water column and benthic production, water quality and nutrient cycling\n\nThese models are supported by a wave model, an optical model, a carbon chemistry and reef processes model, catchment models and data assimilation systems.\n\nModel grids and extents\nThe models are run using a curvilinear orthogonal grid, where their total spatial extent is divided into many grid cells (a.k.a. pixels) at both 4 km and 1 km resolution; dubbed the GBR4 and GBR1 models, respectively. For each grid cell, the models predict values across a consistent set of reef attributes at a range of depths in the water column.\nBoth the GBR1 and GBR4 models extend along the Queensland coast from Papua New Guinea to the New South Wales border, and offshore to beyond the continental slope. The 4 km model encompasses some of the Western Coral Sea and the Queensland Plateau, whereas the 1 km model is limited to the shelf regions. The boundaries of both models are shown in Figure 4 along with the grid sizes.\n\n\n\n\n\n\n\n\nFigure 4: eReefs GBR1 (1 km resolution) and GBR4 (4 km resolution) model boundaries and grid sizes.\n\n\n\n\n\n\n\n\n\n\n\nRelocatable Coastal Model (RECOM)\n\n\n\nIn addition to the 4 km and 1 km resolution models, the eReefs platform also contains a Relocatable Coastal Model (RECOM) which offers higher resolution over smaller, user-defined areas. We do not yet cover the use of RECOM in these tutorials. Learn more about RECOM."
  },
  {
    "objectID": "tutorials/general/intro_to_ereefs.html#the-ereefs-outputs",
    "href": "tutorials/general/intro_to_ereefs.html#the-ereefs-outputs",
    "title": "eReefs overview",
    "section": "The eReefs Outputs",
    "text": "The eReefs Outputs\nThe core outputs from the eReefs platform are the datasets produced by the Hydrodynamic and Biogeochemical Regional Models. These datasets contain the predicted values across a range of variables for different points in time and three-dimensional space (latitude, longitude and depth in the water column and benthic layers). The datasets are produced routinely in near-real-time, and hindcast into the past. The outputs are available in both 4 km and 1 km resolution. An overview of the different raw model outputs available on the National Computer Infrastructure (NCI) eReefs server is shown in Table 1, the different variables contained in each model is presented in Model output variables.\n\nAvailable outputs\n\n\n\n\nTable 1: Overview of the eReefs Hydrodynamic and Biogeochemical model outputs (current model versions only; as of May 2023).\n\n\n\n\n\n\nModel\nVersion\nResolution\nType\nDate from\nDate to\nTime interval\n\n\n\n\nHydrodynamic GBR1\n2.0\n1 km\nNear-real-time & hindcast\nDecember 2014\nPresent\nHourly\n\n\nHydrodynamic GBR4\n2.0\n4 km\nNear-real-time & hindcast\nSeptember 2010\nPresent\nHourly\n\n\nBiogeochemical GBR1\n3.2\n1 km\nNear-real-time\nOctober 2019\nPresent\nDaily\n\n\nBiogeochemical GBR4\n3.1\n4 km\nNear-real-time\nOctober 2019\nPresent\nDaily\n\n\nBiogeochemical GBR4 Baseline\n3.1\n4 km\nHindcast\n2010-12-01\n2019-04-30\nDaily\n\n\nBiogeochemical GBR4 Pre-industrial\n3.1\n4 km\nHindcast\n2010-12-01\n2019-04-30\nDaily\n\n\nBiogeochemical GBR4 Reduced loads\n3.1\n4 km\nHindcast\n2010-12-01\n2019-04-30\nDaily\n\n\n\n\n\n\n\n\n\n\n\n\nOutput variables\n\n\n\n\n\n\nDownload the model variables list\n\n\n\nYou can download the below lists of the model variables in .xlsx format: ereefs_hydro_bgc_model_variables.xlsx.\n\n\n\n\n\n\n\n\nLists currently incomplete\n\n\n\nThe below lists of model variables are not yet complete. While, for the Hydrodynamic model, all the variables are listed, some are missing descriptions. For the Biogeochemical model, which contains over 350 variables, not all model variables are listed. We aim to complete this list in time, as well as specify which variables are included in the AIMS aggregated model outputs (see Accessing model data below).\n\n\n\nHydrodynamic modelBiogeochemical model\n\n\n\n\n\n\nTable 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccessing outputs\nThe eReefs Hydrodynamic and Biogeochemical model outputs are available in two types:\n\nRaw outputs - Unprocessed model outputs published by the Commonwealth Scientific and Industrial Research Organisation (CSIRO) on the National Computational Infrastructure (NCI) server. They contain the model outputs at hourly (Hydrodynamic) or daily (Biogeochemical) time intervals.\nAggregated outputs - Daily, monthly, and yearly aggregated model outputs processed and published by the Australian Institute of Marine Science (AIMS) on the AIMS eReefs THREDDS server. They contain aggregated model outputs whereby, for a given variable and model grid cell, the raw model output values are collected and processed to get the either the daily, monthly or yearly mean value.\n\nThe output datasets can be accessed in a range of different ways — these are listed below.\n\n\n\n\n\n\nNCI Server\n Raw outputs  NetCDF file format  OPeNDAP, HTTPS, WMS\n\n\n\n\nAIMS THREDDS Server\n Aggregated outputs  NetCDF file format  OPeNDAP, HTTPS, WMS\n\n\n\n\n\n\n\n\n\nData Explorer\n Raw outputs  Point-and-click user interface  Visualise data as you go\n\n\n\n\nAIMS Data Extraction Tool\n Raw and Aggregated outputs  Point-and-click user interface  CSV file format\n\n\n\n\n\n\n\n\n\nData Brokering API\n Raw outputs\n\n\n\n\nereefs R package\n Raw outputs  R scripting interface"
  },
  {
    "objectID": "tutorials/general/intro_to_ereefs.html#using-ereefs-data",
    "href": "tutorials/general/intro_to_ereefs.html#using-ereefs-data",
    "title": "eReefs overview",
    "section": "Using eReefs Data",
    "text": "Using eReefs Data\n\nApplications\nReef management challenges for which the eReefs modelling suite has application are numerous and include prediction of coral bleaching hotspots, impacts of ocean acidification on coral calcification, catchment impacts on reef health (sedimentation issues, eutrophication), crown of thorns starfish (COTS) infestations, hypoxia, management of dredging, shipping and port related activities, fisheries management, search and rescue, larval connectivity and prediction of impacts of floods or extreme weather events on marine systems. Since the modelling package operates routinely in near real-time, emergent events such as floods, dredge plumes, phytoplankton blooms, cyclones, vessel groundings, bleaching events etc. may be investigated and responded to in a timely manner.\n\n\nLimitations\nThe eReefs Hydrodynamic and Biogeochemical (BGC) datasets are based on spatial and temporal models, and as such only provide estimates of the environmental conditions. The data does not come from in-water measurements and thus will have a spatially varying level of error in the modelled values.\nFor further information on model accuracy and validation, see the eReefs research paper.\nA technical assessment of the skill level of the BGC version 3.1 model shows that the absolute accuracy of the BGC model varies significantly with variable and location. As a result care should be taken to ensure the model is fit-for-purpose and in general BGC results should used in combination with second sources of information when making inferences or management decisions.\nThe modelled scenarios run for version 3.1 of the BGC model were developed for the purpose of comparing catchment run off effect comparison. As such they were driven with historic weather and river flow boundary conditions, but the sediment and nutrient loads were based on the results of the 2019 Source Catchment modelling. In this catchment modelling the land use is static over the simulation run. This means that for the ‘Baseline’ scenario this uses estimated land use from 2019 applied over all years (2010 - 2019). As a result improvements in land practices are effectively back dated to start of the simulation (2010). This results in early years in the simulation having slightly lower nutrient and sediment loads then actually happened. The BGC modelling team indicated this approach is likely to introduce small additional errors in places where the land practices have improved significantly, but are likely to be smaller than the inherent errors in the model. These errors only apply if the Baseline model data is interpreted as an estimate of historic conditions, rather than the original intended purpose of the scenario comparison.\nThe wind data used for the Hydrodynamic model is originally from the BOM Access-R weather models. These models capture synoptic winds and some of the features of cyclones, however they do not represent the high speed winds near the eye of cyclones well. For this reason the maximum wind speed aggregations do not capture the peak winds of cyclones."
  },
  {
    "objectID": "tutorials/general/intro_to_ereefs.html#sec-learnMore",
    "href": "tutorials/general/intro_to_ereefs.html#sec-learnMore",
    "title": "eReefs overview",
    "section": "Learn more",
    "text": "Learn more\n\nA video presentation on the eReefs platform\nThe ‘eReefs Research’ website\nThe research paper “eReefs: An operational information system for managing the Great Barrier Reef”\nThe CSIRO Enironmental Modelling Suite (EMS) documentation"
  },
  {
    "objectID": "tutorials/general/hydrodynamic_model.html",
    "href": "tutorials/general/hydrodynamic_model.html",
    "title": "eReefs overview",
    "section": "",
    "text": "WORK IN PROGRESS An introduction to the eReefs Hydrodynamic Model and its output data.\n\n\n\n\n\n\n\nA general introduction to eReefs was given in eReefs overview: Introduction to eReefs, including an introduction to the Hydrodynamic model and its place within the larger eReefs platform.\n\n\n\nThe eReefs Hydrodynamic, Sediment Transport, and Biogeochemical Regional Models are the core of the eReefs platform; with the datasets output from the Hydrodynamic and Biogeochemical models comprising the core eReefs data products. In this tutorial we dive a bit deeper into the Hydrodynamic model and its outputs.\nThe eReefs hydrodynamic model predicts the movement of water and key environmental conditions (temperature, salinity, currents, tides). This model allows us to better understand how cyclones mix the water, the location of potentially damaging heat waves, the ocean currents that disperse larvae of corals and Crown-of-Thorns starfish, and fresh water plumes from flooded rivers that can damage inshore reefs."
  },
  {
    "objectID": "tutorials/general/biogeochemical_model.html",
    "href": "tutorials/general/biogeochemical_model.html",
    "title": "eReefs overview",
    "section": "",
    "text": "WORK IN PROGRESS An introduction to the eReefs Biogeochemical Model and its output data.\n\n\n\n\n\n\n\nA general introduction to eReefs was given in eReefs overview: Introduction to eReefs, including an introduction to the Biogeochemical model and its place within the larger eReefs platform.\n\n\n\nThe eReefs Hydrodynamic, Sediment Transport, and Biogeochemical Regional Models are the core of the eReefs platform, with the datasets output from the Hydrodynamic and Biogeochemical models comprising the core eReefs data products. In this tutorial we dive a bit deeper into the Biogeochemical model and its outputs.\n\nThe GBR4 BioGeoChemical (GBR) model builds on the GBR4 hydrodynamic model by modelling the water quality (nutrients and suspended sediment) and key ecological processes (coral, seagrass, plankton) that drive the water chemistry. This model allows us to better understand how water quality is affected by land runoff. Detailed information about the model can be found in the paper: CSIRO Environmental Modelling Suite (EMS): Scientific description of the optical and biogeochemical models (vB3p0).\nVersion 3.1 of the BGC was developed to compare the effects of land practice improvements on water quality changes in the Great Barrier Reef. It was run with three scenarios of river sediment and nutrient loads to simulate the differences between baseline conditions (based on current land use practices in 2019), pre-industrial catchment conditions, and target catchment conditions (anthropogenic loads reduced according to the percentage reductions of DIN, PN, PP and TSS specified in the Reef 2050 Water Quality Improvement Plan 2017-2022).\nA technical assessment of the skill level of the BGC version 3.1 model shows that the absolute accuracy of the BGC model varies significantly with variable and location. A older analysis on the BGC version 2.0 (Skerratt et al., 2018) provides additional background on the skill of the BGC model. As a result care should be taken to ensure the model is fit-for-purpose and in general BGC results should used in combination with second sources of information for making recommendations."
  },
  {
    "objectID": "tutorials/python/plot-ereefs-data/time-series-plot/time_series_plot_python.html",
    "href": "tutorials/python/plot-ereefs-data/time-series-plot/time_series_plot_python.html",
    "title": "Plotting eReefs data",
    "section": "",
    "text": "Learn how to create time series plots of eReefs data in  python.\n\nIn this notebook we use OpeNDAP to extract time series data at a single location of interest, then plot this data. This extraction process can also be done with the AIMS eReefs data extraction tool. If you which to perform bigger extractions then we recommend using this tool instead of this process outlined in this example.\n\nNote: This script has no error checking and so changing the date ranges or locations might result in out of bounds errors.\n\n\nLoad the required Python libraries\n\nfrom netCDF4 import Dataset, num2date\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport os\nimport datetime\nimport pandas as pd\nimport numpy as np\ncartopy.config['data_dir'] = os.getenv('CARTOPY_DIR', cartopy.config.get('data_dir'))\n\n\n\nChoose OPeNDAP end point\nThe first part of the process is to choose the OPeNDAP end point on the AIMS eReefs THREDDS server. You can view the products in the AIMS eReefs THREDDS catalogue. At this stage there is no grouped OPeNDAP service for the entire time series and so this script only works for looking at a single month of data. Hopefully this can be improved in the future.\n\n# Connect to the OpeNDAP endpoint for the specified month.\nmonth = 3\nyear = 2020\nnetCDF_datestr = str(year)+'-'+format(month, '02')\nnetCDF_datestr\n\n'2020-03'\n\n\n\n# OPeNDAP URL to file \"EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-YYYY-MM.nc\". Hydrodynamic 4km model, daily data for the month specified\ninputFile = \"https://thredds.ereefs.aims.gov.au/thredds/dodsC/ereefs/gbr4_v2/daily-monthly/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-\"+netCDF_datestr+\".nc\"\n\nnc_data = Dataset(inputFile, 'r')\nprint(nc_data.title)\n\n# To find a list of the variables uncomment the next line:\nnc_data.variables\n\neReefs AIMS-CSIRO GBR4 Hydrodynamic v2 daily aggregation\n\n\n{'mean_cur': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float32 mean_cur(time, k, latitude, longitude)\n     coordinates: time zc latitude longitude\n     substanceOrTaxon_id: http://environment.data.gov.au/def/feature/ocean_current\n     units: ms-1\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/LCEWMP01/\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://qudt.org/vocab/unit#MeterPerSecond\n     short_name: mean_cur\n     aggregation: mean_speed\n     standard_name: mean_current_speed\n     long_name: mean_current_speed\n     _ChunkSizes: [  1   1 133 491]\n unlimited dimensions: time\n current shape = (31, 17, 723, 491)\n filling off,\n 'salt': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float32 salt(time, k, latitude, longitude)\n     qudt__unit: http://qudt.org/vocab/unit/PSU\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/PSLTMP01/\n     coordinates: time zc latitude longitude\n     substanceOrTaxon_id: http://sweet.jpl.nasa.gov/2.2/matrWater.owl#SaltWater\n     scaledQuantityKind_id: http://environment.data.gov.au/def/property/practical_salinity\n     short_name: salt\n     aggregation: Daily\n     units: PSU\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://environment.data.gov.au/water/quality/def/unit/PSU\n     long_name: Salinity\n     _ChunkSizes: [  1   1 133 491]\n unlimited dimensions: time\n current shape = (31, 17, 723, 491)\n filling off,\n 'temp': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float32 temp(time, k, latitude, longitude)\n     puv__parameter: https://vocab.nerc.ac.uk/collection/P01/current/TEMPMP01/\n     coordinates: time zc latitude longitude\n     substanceOrTaxon_id: http://sweet.jpl.nasa.gov/2.2/matrWater.owl#SaltWater\n     scaledQuantityKind_id: http://environment.data.gov.au/def/property/sea_water_temperature\n     short_name: temp\n     aggregation: Daily\n     units: degrees C\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://qudt.org/vocab/unit#DegreeCelsius\n     long_name: Temperature\n     _ChunkSizes: [  1   1 133 491]\n unlimited dimensions: time\n current shape = (31, 17, 723, 491)\n filling off,\n 'u': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float32 u(time, k, latitude, longitude)\n     vector_components: u v\n     coordinates: time zc latitude longitude\n     substanceOrTaxon_id: http://environment.data.gov.au/def/feature/ocean_current\n     vector_name: Currents\n     aggregation: Daily\n     units: ms-1\n     long_name: Eastward current\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/LCEWMP01/\n     scaledQuantityKind_id: http://environment.data.gov.au/def/property/sea_water_velocity_eastward\n     short_name: u\n     standard_name: eastward_sea_water_velocity\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://qudt.org/vocab/unit#MeterPerSecond\n     _ChunkSizes: [  1   1 133 491]\n unlimited dimensions: time\n current shape = (31, 17, 723, 491)\n filling off,\n 'v': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float32 v(time, k, latitude, longitude)\n     vector_components: u v\n     coordinates: time zc latitude longitude\n     substanceOrTaxon_id: http://environment.data.gov.au/def/feature/ocean_current\n     vector_name: Currents\n     aggregation: Daily\n     units: ms-1\n     long_name: Northward current\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/LCNSMP01/\n     scaledQuantityKind_id: http://environment.data.gov.au/def/property/sea_water_velocity_northward\n     short_name: v\n     standard_name: northward_sea_water_velocity\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://qudt.org/vocab/unit#MeterPerSecond\n     _ChunkSizes: [  1   1 133 491]\n unlimited dimensions: time\n current shape = (31, 17, 723, 491)\n filling off,\n 'zc': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float64 zc(k)\n     positive: up\n     coordinate_type: Z\n     units: m\n     long_name: Z coordinate\n     axis: Z\n     _CoordinateAxisType: Height\n     _CoordinateZisPositive: up\n unlimited dimensions: \n current shape = (17,)\n filling off,\n 'time': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float64 time(time)\n     units: days since 1990-01-01 00:00:00 +10\n     long_name: Time\n     standard_name: time\n     coordinate_type: time\n     puv__uom: http://vocab.nerc.ac.uk/collection/P06/current/UTAA/\n     calendar: gregorian\n     _CoordinateAxisType: Time\n     _ChunkSizes: 1024\n unlimited dimensions: time\n current shape = (31,)\n filling off,\n 'latitude': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float64 latitude(latitude)\n     long_name: Latitude\n     standard_name: latitude\n     units: degrees_north\n     coordinate_type: latitude\n     projection: geographic\n     puv__ofProperty: http://vocab.nerc.ac.uk/collection/S06/current/S0600045/\n     puv__uom: http://vocab.nerc.ac.uk/collection/P06/current/DEGN/\n     _CoordinateAxisType: Lat\n unlimited dimensions: \n current shape = (723,)\n filling off,\n 'longitude': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float64 longitude(longitude)\n     standard_name: longitude\n     long_name: Longitude\n     units: degrees_east\n     puv__uom: http://vocab.nerc.ac.uk/collection/P06/current/DEGE/\n     coordinate_type: longitude\n     projection: geographic\n     _CoordinateAxisType: Lon\n unlimited dimensions: \n current shape = (491,)\n filling off,\n 'mean_wspeed': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float32 mean_wspeed(time, latitude, longitude)\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/ESEWMPXX/\n     coordinates: time latitude longitude\n     units: ms-1\n     short_name: mean_wspeed\n     aggregation: mean_speed\n     standard_name: mean_wind_speed\n     long_name: mean_wind_speed\n     _ChunkSizes: [  1 133 491]\n unlimited dimensions: time\n current shape = (31, 723, 491)\n filling off,\n 'eta': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float32 eta(time, latitude, longitude)\n     puv__parameter: https://vocab.nerc.ac.uk/collection/P01/current/ASLVMP01/\n     coordinates: time latitude longitude\n     substanceOrTaxon_id: http://environment.data.gov.au/def/feature/ocean_near_surface\n     scaledQuantityKind_id: http://environment.data.gov.au/def/property/sea_surface_elevation\n     short_name: eta\n     standard_name: sea_surface_height_above_sea_level\n     aggregation: Daily\n     units: metre\n     positive: up\n     medium_id: http://environment.data.gov.au/def/feature/ocean\n     unit_id: http://qudt.org/vocab/unit#Meter\n     long_name: Surface elevation\n     _ChunkSizes: [  1 133 491]\n unlimited dimensions: time\n current shape = (31, 723, 491)\n filling off,\n 'wspeed_u': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float32 wspeed_u(time, latitude, longitude)\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/ESEWMPXX/\n     coordinates: time latitude longitude\n     short_name: wspeed_u\n     aggregation: Daily\n     units: ms-1\n     long_name: eastward_wind\n     _ChunkSizes: [  1 133 491]\n unlimited dimensions: time\n current shape = (31, 723, 491)\n filling off,\n 'wspeed_v': &lt;class 'netCDF4._netCDF4.Variable'&gt;\n float32 wspeed_v(time, latitude, longitude)\n     puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/ESNSMPXX/\n     coordinates: time latitude longitude\n     short_name: wspeed_v\n     aggregation: Daily\n     units: ms-1\n     long_name: northward_wind\n     _ChunkSizes: [  1 133 491]\n unlimited dimensions: time\n current shape = (31, 723, 491)\n filling off}\n\n\n\n\nSelect the point location\nWork out the bounds of the gridded data. We can then use this to find out which grid cell best matches our location of interest.\n\nNote: This only works because the AIMS eReefs aggregate datasets are regridded onto a regularly spaced grid. The original raw model data is on a curvilinear grid and this approach would not work for that data.\n\n\nlons = nc_data.variables['longitude'][:]\nmax_lon = max(lons)\nmin_lon = min(lons)\nlats = nc_data.variables['latitude'][:]\nmax_lat = max(lats)\nmin_lat = min(lats)\ngrid_lon = lons.size\ngrid_lat = lats.size\nprint(\"Grid bounds, Lon: \"+str(min_lon)+\" - \"+str(max_lon)+\" Lat:\"+str(min_lat)+\" - \"+str(max_lat))\nprint(\"Grid size is: \"+str(grid_lon)+\" x \"+str(grid_lat))\n\nGrid bounds, Lon: 142.168788 - 156.868788 Lat:-28.696022 - -7.036022\nGrid size is: 491 x 723\n\n\nFind the closest index to the location of interest.\n\n# Davies reef\nlat = -18.82\nlon = 147.64\nselectedLatIndex = round((lat-min_lat)/(max_lat-min_lat)*grid_lat)\nselectedLonIndex = round((lon-min_lon)/(max_lon-min_lon)*grid_lon)\nprint(\"Grid position of location: \"+str(selectedLatIndex)+\", \"+str(selectedLonIndex))\n\nGrid position of location: 330, 183\n\n\n\n\nExtract values\nExtract the values over time at this location. Note that because we are access the underlying data here this results in an OpeNDAP call to get the data from the remote server. As a result this call can take a while (~10 sec).\n\nselectedDepthIndex = 15 # -1.5m\nselectedDepthIndex2 = 10 # -17.75m\n\n# Time, Depth, Lat, Lon\ndailyTemp1 = nc_data.variables['temp'][:,[selectedDepthIndex,selectedDepthIndex2], selectedLatIndex, selectedLonIndex]\nprint(dailyTemp1[0:5])\n\n[[29.586527 27.764503]\n [29.513176 27.867159]\n [29.505758 28.179167]\n [29.782936 28.216429]\n [30.008278 28.062304]]\n\n\nLet’s get the wind for the same location. The wind variable doesn’t have any depth dimension and so our indexing into the data is different. The wind is a vector measurement, with an x and y component.\n\nwspeed_v = nc_data.variables['wspeed_v'][:, selectedLatIndex, selectedLonIndex]\nwspeed_u = nc_data.variables['wspeed_v'][:, selectedLatIndex, selectedLonIndex]\n\nTo get the wind speed we need to calculate the magnitude of this vector.\n\nwspeed = np.sqrt(wspeed_v**2 + wspeed_u**2)\n\nGet the time series. Note that the time values are stored as the number of days since 1990-01-01 00:00:00 +10.\n\ntimes = nc_data.variables['time'][:]\nprint(times[0:5])\n\n[11017. 11018. 11019. 11020. 11021.]\n\n\n\n\nPlot the time series\n\n# Convert the days since the 1990 origin into Pandas dates for plotting\nt = pd.to_datetime(times,unit='D',origin=pd.Timestamp('1990-01-01'))\n\nfig, ax1 = plt.subplots()\nfig.set_size_inches(8, 7)\n\n\nax1.set_xlabel('Date')\nax1.set_ylabel('Temperature (deg C)')\nax1.plot(t, dailyTemp1[:,0], color='tab:red', label='Temp (-1.5 m)')\nax1.plot(t, dailyTemp1[:,1], color='tab:orange', label='Temp (-17.75 m)')\n#ax1.tick_params(axis='y', labelcolor=color)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ncolor = 'tab:blue'\nax2.set_ylabel('Wind speed (m/s)', color=color)  # we already handled the x-label with ax1\nax2.plot(t, wspeed, color=color, label='Wind')\nax2.tick_params(axis='y', labelcolor=color)\n\nfig.legend()\n# Set the axes formating to show the dates on an angle on the current figure (gcf)\nplt.gcf().autofmt_xdate()\n\n#fig.tight_layout()  # otherwise the right y-label is slightly clipped\n\n\n\n\n\n\n\n\nFrom this graph we can see that the surface water at Davies Reef was very warm during March 2020. There was a strong stratification of the temperature profile with cool water at -18 m. Around the 10th March the wind picked up for a few days, mixing the water, cooling the surface down rapidly."
  },
  {
    "objectID": "tutorials/python/follow-along/python_and_jupyter_setup.html",
    "href": "tutorials/python/follow-along/python_and_jupyter_setup.html",
    "title": "Setup Python and Jupyter Notebook",
    "section": "",
    "text": "Learn how to setup Python and Jupyter Notebook and follow along with the eReefs tutorials."
  },
  {
    "objectID": "tutorials/python/follow-along/python_and_jupyter_setup.html#install-python",
    "href": "tutorials/python/follow-along/python_and_jupyter_setup.html#install-python",
    "title": "Setup Python and Jupyter Notebook",
    "section": "Install Python",
    "text": "Install Python"
  },
  {
    "objectID": "tutorials/python/follow-along/python_and_jupyter_setup.html#install-dependencies",
    "href": "tutorials/python/follow-along/python_and_jupyter_setup.html#install-dependencies",
    "title": "Setup Python and Jupyter Notebook",
    "section": "Install dependencies",
    "text": "Install dependencies"
  },
  {
    "objectID": "tutorials/python/follow-along/python_and_jupyter_setup.html#install-jupyter-notebook",
    "href": "tutorials/python/follow-along/python_and_jupyter_setup.html#install-jupyter-notebook",
    "title": "Setup Python and Jupyter Notebook",
    "section": "Install Jupyter Notebook",
    "text": "Install Jupyter Notebook"
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html",
    "href": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "",
    "text": "Learn the basics of extracting eReefs data from the AIMS server with OPeNDAP in  python.\nIn this tutorial we will look at how to access eReefs data directly from the AIMS THREDDS server in python.\nThis server hosts aggregated eReefs model data in NetCDF file format and offers access to the data files via OPeNDAP, HTTP Server, and Web Map Service (WMS). While we could download the data files manually via the HTTPServer link, this approach is cumbersome when downloading multiple files, given their large size. Thankfully, OPeNDAP provides a way to access the data files over the internet and extract only the data we want.\nFor example, say we want the daily mean surface temperature at a single location for the last 30 days. If we were to download the 30 individual daily aggregated NetCDF files, with each file ~ 350 Mb, this would require us to download over 10 Gb of data just to get 300 numbers. The vast majority of this data would be irrelevant to our needs as the NetCDF files contain data for a range of variables, at a range of depths, for many, many locations. However, with OPeNDAP, we can extract the daily mean values directly from the server without downloading any unneeded data."
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#motivating-problem",
    "href": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#motivating-problem",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Motivating problem",
    "text": "Motivating problem\nWe will extract the daily mean water temperature for the 10th of December 2022 at 1.5 m depth across the entire scope of the eReefs model. We will then plot this data. This example will introduce the basics of how to connect to files on the server and extract the data we want."
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#navigating-the-ereefs-server",
    "href": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#navigating-the-ereefs-server",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Navigating the eReefs server",
    "text": "Navigating the eReefs server\nADD: Info about the folder and file naming on the AIMS THREDDS Server."
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#python-libraries",
    "href": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#python-libraries",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Python libraries",
    "text": "Python libraries\n\nfrom netCDF4 import Dataset, num2date\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport os\ncartopy.config['data_dir'] = os.getenv('CARTOPY_DIR', cartopy.config.get('data_dir'))"
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#connect-to-a-file-on-the-server",
    "href": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#connect-to-a-file-on-the-server",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Connect to a file on the server",
    "text": "Connect to a file on the server\nFirst we need to find the right NetCDF file on the server. The available eReefs data NetCDF files are listed in the AIMS THREDDS Server catalogue. We will navigate to the eReefs 4 km Hydrodynamic Model daily aggregated data for the month of December 2022 and copy the OPeNDAP data URL.\n\n\ninput_file = \"https://thredds.ereefs.aims.gov.au/thredds/dodsC/ereefs/gbr4_v2/daily-monthly/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-2022-12.nc\"\n\nWe can then open a connection to this file using the Dataset function from the netCDF4 library.\n\ndailyAggDec22_nc = Dataset(input_file)\n\n\n\n\n\n\n\nIf you wish to download NetCDF files from the server you can click the HTTPServer link instead of OPeNDAP. The file can then be loaded into python by specifying the path: Dataset(\"&lt;path to downloaded file&gt;\")."
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#examine-file-structure",
    "href": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#examine-file-structure",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Examine file structure",
    "text": "Examine file structure\nIf we wish to look at the structure of the file we have connected to, including what variables and dimensions are available, we access the various attributes below.\n\n\ndailyAggDec22_nc.title\n\n\n\n'eReefs AIMS-CSIRO GBR4 Hydrodynamic v2 daily aggregation'\n\n\n\n\n\ndailyAggDec22_nc.description\n\n\n\n'Aggregation of raw hourly input data (from eReefs AIMS-CSIRO GBR4 Hydrodynamic v2 subset) to daily means. Also calculates mean magnitude of wind and ocean current speeds. Data is regridded from curvilinear (per input data) to rectilinear via inverse weighted distance from up to 4 closest cells.'\n\n\n\n\n\ndailyAggDec22_nc.dimensions\n\n\n\n{'time': &lt;class 'netCDF4._netCDF4.Dimension'&gt; (unlimited): name = 'time', size = 31, 'k': &lt;class 'netCDF4._netCDF4.Dimension'&gt;: name = 'k', size = 17, 'latitude': &lt;class 'netCDF4._netCDF4.Dimension'&gt;: name = 'latitude', size = 723, 'longitude': &lt;class 'netCDF4._netCDF4.Dimension'&gt;: name = 'longitude', size = 491}\n\n\n\n\n\ndailyAggDec22_nc.variables\n\n\n\n{'mean_cur': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 mean_cur(time, k, latitude, longitude)\n    puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/LCEWMP01/\n    coordinates: time zc latitude longitude\n    units: ms-1\n    short_name: mean_cur\n    aggregation: mean_speed\n    standard_name: mean_current_speed\n    long_name: mean_current_speed\n    _ChunkSizes: [  1   1 133 491]\nunlimited dimensions: time\ncurrent shape = (31, 17, 723, 491)\nfilling off, 'salt': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 salt(time, k, latitude, longitude)\n    qudt__unit: http://qudt.org/vocab/unit/PSU\n    puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/PSLTMP01/\n    coordinates: time zc latitude longitude\n    short_name: salt\n    aggregation: Daily\n    units: PSU\n    long_name: Salinity\n    _ChunkSizes: [  1   1 133 491]\nunlimited dimensions: time\ncurrent shape = (31, 17, 723, 491)\nfilling off, 'temp': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 temp(time, k, latitude, longitude)\n    puv__parameter: https://vocab.nerc.ac.uk/collection/P01/current/TEMPMP01/\n    coordinates: time zc latitude longitude\n    short_name: temp\n    aggregation: Daily\n    units: degrees C\n    long_name: Temperature\n    _ChunkSizes: [  1   1 133 491]\nunlimited dimensions: time\ncurrent shape = (31, 17, 723, 491)\nfilling off, 'u': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 u(time, k, latitude, longitude)\n    vector_components: u v\n    puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/LCEWMP01/\n    coordinates: time zc latitude longitude\n    short_name: u\n    standard_name: eastward_sea_water_velocity\n    vector_name: Currents\n    aggregation: Daily\n    units: ms-1\n    long_name: Eastward current\n    _ChunkSizes: [  1   1 133 491]\nunlimited dimensions: time\ncurrent shape = (31, 17, 723, 491)\nfilling off, 'v': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 v(time, k, latitude, longitude)\n    vector_components: u v\n    puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/LCNSMP01/\n    coordinates: time zc latitude longitude\n    short_name: v\n    standard_name: northward_sea_water_velocity\n    vector_name: Currents\n    aggregation: Daily\n    units: ms-1\n    long_name: Northward current\n    _ChunkSizes: [  1   1 133 491]\nunlimited dimensions: time\ncurrent shape = (31, 17, 723, 491)\nfilling off, 'zc': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat64 zc(k)\n    units: m\n    positive: up\n    long_name: Z coordinate\n    axis: Z\n    coordinate_type: Z\n    _CoordinateAxisType: Height\n    _CoordinateZisPositive: up\nunlimited dimensions: \ncurrent shape = (17,)\nfilling off, 'time': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat64 time(time)\n    units: days since 1990-01-01 00:00:00 +10\n    long_name: Time\n    standard_name: time\n    coordinate_type: time\n    puv__uom: http://vocab.nerc.ac.uk/collection/P06/current/UTAA/\n    calendar: gregorian\n    _CoordinateAxisType: Time\n    _ChunkSizes: 1024\nunlimited dimensions: time\ncurrent shape = (31,)\nfilling off, 'latitude': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat64 latitude(latitude)\n    units: degrees_north\n    long_name: Latitude\n    standard_name: latitude\n    coordinate_type: latitude\n    projection: geographic\n    puv__ofProperty: http://vocab.nerc.ac.uk/collection/S06/current/S0600045/\n    puv__uom: http://vocab.nerc.ac.uk/collection/P06/current/DEGN/\n    _CoordinateAxisType: Lat\nunlimited dimensions: \ncurrent shape = (723,)\nfilling off, 'longitude': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat64 longitude(longitude)\n    puv__uom: http://vocab.nerc.ac.uk/collection/P06/current/DEGE/\n    units: degrees_east\n    long_name: Longitude\n    standard_name: longitude\n    coordinate_type: longitude\n    projection: geographic\n    _CoordinateAxisType: Lon\nunlimited dimensions: \ncurrent shape = (491,)\nfilling off, 'mean_wspeed': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 mean_wspeed(time, latitude, longitude)\n    puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/ESEWMPXX/\n    coordinates: time latitude longitude\n    units: ms-1\n    short_name: mean_wspeed\n    aggregation: mean_speed\n    standard_name: mean_wind_speed\n    long_name: mean_wind_speed\n    _ChunkSizes: [  1 133 491]\nunlimited dimensions: time\ncurrent shape = (31, 723, 491)\nfilling off, 'eta': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 eta(time, latitude, longitude)\n    puv__parameter: https://vocab.nerc.ac.uk/collection/P01/current/ASLVMP01/\n    coordinates: time latitude longitude\n    short_name: eta\n    standard_name: sea_surface_height_above_geoid\n    aggregation: Daily\n    units: metre\n    positive: up\n    long_name: Surface elevation\n    _ChunkSizes: [  1 133 491]\nunlimited dimensions: time\ncurrent shape = (31, 723, 491)\nfilling off, 'wspeed_u': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 wspeed_u(time, latitude, longitude)\n    puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/ESEWMPXX/\n    coordinates: time latitude longitude\n    short_name: wspeed_u\n    aggregation: Daily\n    units: ms-1\n    long_name: eastward_wind\n    _ChunkSizes: [  1 133 491]\nunlimited dimensions: time\ncurrent shape = (31, 723, 491)\nfilling off, 'wspeed_v': &lt;class 'netCDF4._netCDF4.Variable'&gt;\nfloat32 wspeed_v(time, latitude, longitude)\n    puv__parameter: http://vocab.nerc.ac.uk/collection/P01/current/ESNSMPXX/\n    coordinates: time latitude longitude\n    short_name: wspeed_v\n    aggregation: Daily\n    units: ms-1\n    long_name: northward_wind\n    _ChunkSizes: [  1 133 491]\nunlimited dimensions: time\ncurrent shape = (31, 723, 491)\nfilling off}"
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#extract-data",
    "href": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#extract-data",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Extract data",
    "text": "Extract data\nNow that we have an open connection to a file on the server we need to extract the daily mean temperature at 1.5m depth for the 10th of December.\nFrom the dailyAggDec22.variables output above we can see that the variable corresponding to temperature is: \\(\\texttt{ temp(time, k, latitude, longitude)}\\).\nThe dimensions for temperature are in brackets. This means that there is a temperature value for every combination of longitude, latitude, depth (k) and time. We can now see why these NetCDF files are so large.\nTo extract data from the file we need to access the variable of interest using the structure\n&lt;file&gt;.variables[&lt;variable name&gt;][&lt;dimension 1 indices&gt;, &lt;dim. 2 indices&gt;, ...]\nHere we access the values for the given variable at certain indexes along each dimension.\nTherefore, we need the following:\n\nfile: an eReefs NetCDF file connection; in our case dailyAggDec22.nc.\nvariable name: the name of the data variable we wish to extract; in our case \"temp\".\ndimension indices: a vector specifying for which indices of each dimension to extract the temperature values.\n\nLet’s look at how to construct the vector of dimension indices.\nTime: Since we have the daily aggregated data for December 2022, and are interested only in a single day (the 10th), time is a constant value; i.e. we have a single index for which to extract. From the dailyAggDec22.dimensions output we can see we have 31 time indexs, these correspond to the day of the month, therefore we want the time=10.\nDepth: Again we have a constant value of interest (1.5 m). The index k corresponds to different depths as shown in the table below, where we see that for the 4km models k=16 maps to a depth of 1.5 m.\n\n\nTable of eReefs depths corresponding to index k\n\n\n\n\n\n\nIndex k (R)\nIndex k (Python)\nHydrodynamic 1km model\nHydrodynamic & BioGeoChemical 4km models\n\n\n\n\n1\n0\n-140.00\n-145.00\n\n\n2\n1\n-120.00\n-120.00\n\n\n3\n2\n-103.00\n-103.00\n\n\n4\n3\n-88.00\n-88.00\n\n\n5\n4\n-73.00\n-73.00\n\n\n6\n5\n-60.00\n-60.00\n\n\n7\n6\n-49.00\n-49.00\n\n\n8\n7\n-39.50\n-39.50\n\n\n9\n8\n-31.00\n-31.00\n\n\n10\n9\n-24.00\n-23.75\n\n\n11\n10\n-18.00\n-17.75\n\n\n12\n11\n-13.00\n-12.75\n\n\n13\n12\n-9.00\n-8.80\n\n\n14\n13\n-5.25\n-5.55\n\n\n15\n14\n-2.35\n-3.00\n\n\n16\n15\n-0.50\n-1.50\n\n\n17\n16\nNA\n-0.50\n\n\n\n\n\n\n\n\nLongitude and latitude: We want temperatures for every available longitude and latitude so we can plot the data across the entire spatial range of the eReefs model. Therefore we want every index of latitude and longitude. In python this is easily specified by using the notation vector[:] which can be though of as short hand for vector[&lt;minimum index&gt;: &lt;maximum index&gt;].\n\n# EXTRACT DATA\ntemps_10Dec22_1p5m = dailyAggDec22_nc.variables['temp'][\n  10,  # time index --&gt; 10th day of month\n  16,  # depth index k --&gt; 1.5m depth\n  :,   # latitude indices --&gt; all possible\n  :,   # longitude indices --&gt; all possible\n]\n\ntemps_10Dec22_1p5m\n\nmasked_array(\n  data=[[      nan,       nan,       nan, ...,       nan,       nan,\n               nan],\n        [      nan,       nan,       nan, ..., 22.950163, 22.948303,\n         22.942863],\n        [      nan,       nan,       nan, ..., 22.945545, 22.947357,\n         22.934599],\n        ...,\n        [      nan,       nan,       nan, ...,       nan,       nan,\n               nan],\n        [      nan,       nan,       nan, ...,       nan,       nan,\n               nan],\n        [      nan,       nan,       nan, ...,       nan,       nan,\n               nan]],\n  mask=False,\n  fill_value=1e+20,\n  dtype=float32)"
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#plot-data",
    "href": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#plot-data",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Plot data",
    "text": "Plot data\nLet’s plot the data. To do this we will need to extract the longitude and latitude variables from the open server file.\n\nlons = dailyAggDec22_nc.variables[\"longitude\"][:]\nlats = dailyAggDec22_nc.variables[\"latitude\"][:]\n\n\n# Setup plot canvas\nplt.figure(figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')\nax = plt.axes(projection=ccrs.PlateCarree())\nax.set_extent([135, 165, -6.5, -29.5], ccrs.PlateCarree())\n\n# Add the data to the plot\nplt.contourf(lons, lats, temps_10Dec22_1p5m, 30, transform=ccrs.PlateCarree())\n\n&lt;cartopy.mpl.contour.GeoContourSet object at 0x764ec6e72880&gt;\n\n# Add landmass coastlines shapefile\nax.coastlines()\n\n# Show plot\nplt.show()\n\n/opt/conda/lib/python3.8/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip\n  warnings.warn(f'Downloading: {url}', DownloadWarning)\n\n\n\n\n\n\n\n\nFigure 1: Extracted eReefs daily aggregated mean temperature at 1.5m depth for 10 December 2022.\n\n\n\n\n\nHooray! We can now see in Figure 1 that our data was extracted successfully."
  },
  {
    "objectID": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#close-file-connection",
    "href": "tutorials/python/access-ereefs-data-server/basic-server-access/basic_server_access_python.html#close-file-connection",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Close file connection",
    "text": "Close file connection\nNow that we are done with the openned server file it is best practice to close it.\n\ndailyAggDec22_nc.close()"
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html",
    "href": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html",
    "title": "Plotting eReefs data",
    "section": "",
    "text": "Learn how to create time series plots of eReefs data in .\nIn this tutorial we use time series plots of eReefs data to examine the seasonal trends in northward currents for a single location off the coast of Cooktown, QLD Australia."
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#motivating-problem",
    "href": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#motivating-problem",
    "title": "Plotting eReefs data",
    "section": "Motivating problem",
    "text": "Motivating problem\nHeat waves occur in the shallow waters of the Great Barrier Reef (GBR) in much the same way that they occur on land. When the waters gets too hot for too long, corals become sick and can eventually die (as a result of coral bleaching). As the Earth continues to warm under a changing climate, these heat waves are becoming hotter, longer, and more frequent, threatening the survival of the GBR as we know it.\nDifferent coral populations along the GBR are adapted to the long-term temperature ranges which may be considered normal for the area in which they live. In general, this means that the northern populations are better able to handle warmer waters than their southern counterparts. Some of these adaptations are encoded in the their genomes and we can imagine that the spread of these warm-adapted genes to the southern GBR would confer some level of resilience to future heat wave events.\nHowever, major ocean currents along the GBR bifurcate, i.e. split, off the coast of Cairns (Figure 1), creating prevailing northward currents in the waters to the north. It has been suggested that these northward currents may act as a genetic barrier for corals, preventing coral larvae from travelling south and spreading their genes into the Southern GBR.\nWe wish to examine this phenomena on a finer scale to get a sense of how strong and persistent this genetic barrier may be.\n\n\n\n\n\n\n\n\nFigure 1: eReefs 2016 yearly mean temperatures (left) and currents (right) for the Great Barrier Reef showing north-to-south temperature gradient and current bifurcation north-east of Cairns (shown in red and orange)."
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#r-packages",
    "href": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#r-packages",
    "title": "Plotting eReefs data",
    "section": "R Packages",
    "text": "R Packages\n\nlibrary(readr) # faster data importing\nlibrary(tidyverse) # a suite of packages including dplyr, tidyr, ggplot2, stringr\nlibrary(janitor) # create better variable names\nlibrary(leaflet) # making interactive maps\nlibrary(lubridate) # easier handling of dates and times\nlibrary(plotly) # making interactive plots\nlibrary(fontawesome) # to put icons in R markdown html\nlibrary(htmltools) # for styling interactive outputs\n\n# Load our custom function to create time series plots with data grouped by year (or a given period &lt;= 12 months). We will go through this function later in the tutorial.\nsource(\"ggTS_byYear.R\")\n\nNOTE: The code for ggTS_byYear.R is provided the Advanced time series plots section below."
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#the-data",
    "href": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#the-data",
    "title": "Plotting eReefs data",
    "section": "The data",
    "text": "The data\nData was extracted from the eReefs GBR Hydrological Model (4km) v2.0 using the online data extraction tool for a single point approximately 60 km south of Lizard Island (shown in Figure 2). For this point we extracted the daily aggregated data for the northward current at a depth of 0.5 m (as coral larvae generally travel in the top of the water column) and the northward windspeed (to see if the wind may be a key driver of current speed and direction). Both of these variables have the units of meters per second (m/s), where a positive value indicates the speed of the current/wind to the north, and a negative value, to the south. We extracted all data between 1 September 2010 - 1 September 2022.\nDownload the 2303.b87e9d6-collected.csv file to the data folder and rename it to extracted_ereefs_data__time_series_plot.csv.\n\n# Import data with 'clean' variable names and then take a 'glimpse'\nereefs_data &lt;- read_csv(\"data/extracted_ereefs_data__time_series_plot.csv\") |&gt; clean_names() |&gt; glimpse()\n\nRows: 8,768\nColumns: 12\n$ aggregated_date_time &lt;dttm&gt; 2010-09-01, 2010-09-01, 2010-09-02, 2010-09-02, …\n$ variable             &lt;chr&gt; \"wspeed_v\", \"v\", \"wspeed_v\", \"v\", \"wspeed_v\", \"v\"…\n$ depth                &lt;dbl&gt; 99999.9, -0.5, 99999.9, -0.5, 99999.9, -0.5, 9999…\n$ site_name            &lt;chr&gt; \"extraction point\", \"extraction point\", \"extracti…\n$ latitude             &lt;dbl&gt; -15.25, -15.25, -15.25, -15.25, -15.25, -15.25, -…\n$ longitude            &lt;dbl&gt; 145.46, 145.46, 145.46, 145.46, 145.46, 145.46, 1…\n$ mean                 &lt;dbl&gt; 6.17655035, 0.11174639, 5.97394102, 0.11343298, 7…\n$ median               &lt;dbl&gt; 6.26913939, 0.14671343, 6.04144896, 0.11044985, 7…\n$ p5                   &lt;dbl&gt; 5.346766602, 0.000353042, 5.447607187, 0.07272162…\n$ p95                  &lt;dbl&gt; 7.231647611, 0.200932400, 6.545938471, 0.20063861…\n$ lowest               &lt;dbl&gt; 5.2204528552, 0.0000000000, 5.3579356785, 0.06825…\n$ highest              &lt;dbl&gt; 7.231647611, 0.200932400, 6.545938471, 0.20063861…\n\n\nHere we can see that we have data for the wspeed_v (northward wind velocity) and v (northward current velocity) for our single site site_a with the coordinates (145.46, -15.25). As we have downloaded the daily aggregated data, we get the mean, median, p5 (5th percentile), p95 (95th percentile), lowest and highest values for each day between 1 September 2010 through 31 August 2022.\n\n\n\n\n\n\nAbove we used the readr package’s read_csv function which is much faster at reading in large datasets compared to R’s base function read.csv. We then applied the janitor package’s clean_names function, which formats the variable names in consistent matter, making it much more convenient for use in R."
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#interactive-gbr-maps",
    "href": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#interactive-gbr-maps",
    "title": "Plotting eReefs data",
    "section": "Interactive GBR maps",
    "text": "Interactive GBR maps\nLets have a closer look at the site for which we have extracted the eReefs data by plotting it on an interactive leaflet map along with the GBR reef features. The GBR reef features layer is located in the eAtlas Web Mapping Service (AIMS). A list of other layers available in this server can be found on the eAtlas GeoServer.\nYou can find out more about creating leaflet maps in R in this RStudio leaflet tutorial.\n\n# Get unique coordinates from data (in this case only one set of lat/lon)\nsite_coords &lt;- data.frame(\n  lat = unique(ereefs_data$latitude),\n  lon = unique(ereefs_data$longitude)\n)\n# Plot coordinates on a leaflet map\nsite_map &lt;- site_coords |&gt;\n    leaflet( # create a blank leaflet map\n      options = leafletOptions(attributionControl=FALSE) # remove the 'leaflet' watermark\n    ) |&gt;\n    addTiles() |&gt;  # adds a basemap (OpenStreetMap by default)\n    addMarkers() |&gt;  # add a marker at the given coordinates\n    addScaleBar()\n\n# Add the GBR reef features (WMS layer) to the map\nsite_map &lt;- site_map |&gt;\n    addWMSTiles(\n      baseUrl = \"https://maps.eatlas.org.au/maps/wms?\", # Link to WMS server\n      layers = c(\"ea:GBR_GBRMPA_GBR-features\"), # Names of layers (located in the WMS server) to display\n      options = WMSTileOptions(format = \"image/png\", transparent = TRUE)\n    )\n\n# Display the map centred at our site\nsite_map |&gt;\n    setView(lng = site_coords$lon[1], lat = site_coords$lat[1], zoom = 9)\n\n\n\n\n\n\n\nFigure 2: Interactive map showing site for which eReefs data was extracted."
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#basic-time-series-plots",
    "href": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#basic-time-series-plots",
    "title": "Plotting eReefs data",
    "section": "Basic time series plots",
    "text": "Basic time series plots\nSince we are interested in gaining some insight into the possibility of northern coral larvae migrating southward through our site, we should have a look at the north-south current velocity data.\n\nereefs_data |&gt;\n  dplyr::filter(variable == \"v\") |&gt;  # select the current variable\n  ggplot(aes(x = aggregated_date_time, y = mean)) + # and plot the daily mean\n  geom_line(alpha=0.7) + # specify a line graph of the mean\n  geom_abline(slope = 0, intercept = 0, color = \"red\", linewidth = 1) + # add a line a y=0\n  scale_x_datetime(date_breaks = \"1 year\", date_labels = \"%Y\") + # show only years on x-axis\n  theme_bw(base_size=13) +\n  labs(x = \"Year\", y = \"Northward current velocity (m/s)\")\n\n\n\n\n\n\n\nFigure 3: Time series plot of daily mean northward current velocity.\n\n\n\n\n\nHere we see that there does appear to be a cyclical pattern to the northward current at our site, with southward currents (i.e. negative northward current) primarily in the wet season. This is great news, as this is when coral spawning occurs!\nHowever, in order to get a better idea of what is happening, we should have a closer look at the data for the coral spawning period of roughly October - January."
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#advanced-time-series-plots",
    "href": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#advanced-time-series-plots",
    "title": "Plotting eReefs data",
    "section": "Advanced time series plots",
    "text": "Advanced time series plots\nWe wish to produce the plot below, looking at all our data in an approximate coral spawning season of October through January, for each of the years in our data.\n\n\n\n\n\n\n\n\nFigure 4: Times series plot of daily mean northward current velocity during October through January for each year 2010-11 to 2021-22.\n\n\n\n\n\nThe first thing we can note from this plot is it’s very cluttered, making it difficult to detect any clear trend in our data. However, before we investigate this further, lets look at how the plot was made.\nUnfortunately, there does not seem to be any out-of-the-box or intuitive ways to plot a given season (i.e. period less than 12 months) for multiple years in ggplot. However, we can hack a solution by writing a function called ggTS_byYear which creates a fake date variable where all the data is converted to be in the same year and then plots this fake date along the x-axis and groups the data based on the real date. This function is defined below.\nYou can copy and paste this function into your script if you would like start using it straight away. However, it is also heavily commented should you wish to customise issues, or just simply see how it works.\n\n\n\n\n\n\nWhen defining large functions such as this it is useful to save the function in a separate script and then import the script into R, e.g. source('ggTS_byYear.R').\n\n\n\n\n########################################################################\n## PLOT TIME SERIES BY YEAR OVER A GIVEN PERIOD &lt;= 12 MONTHS          ##\n## ------------------------------------------------------------------ ##\n## RETURNS: ggplot object (without geoms)                             ##\n## REQUIRES: ggplot, dplyr, magritter, lubridate                      ##\n## ------------------------------------------------------------------ ##\n## Example:                                                           ##\n##    salinity_time_series_plot &lt;-                                    ##\n##      ggTS_byYear(                                                  ##\n##        data = eReefs_data,                                         ##\n##        date_col_name = date_time,                                  ##\n##        response_col_name = daily_mean_salinity,                    ##\n##        start_month = 6,                                            ##\n##        end_month = 5                                               ##\n##      ) +                                                           ##\n##      geom_line() +                                                 ##\n##      labs(y = \"Daily mean salinity\", x = \"Date\", colour = \"Year\")  ##\n##                                                                    ##\n## Warning: not designed for plot periods &gt; 12 months                 ##\n##                                                                    ##\n## Function concept: fake year(s) is used to put data for all years   ##\n##                   on same x-axis, whereas plot period denotes      ##\n##                   real year(s)pertaining to the data dates         ##\n##                                                                    ##\n## Note: x-axis major breaks are months, if a different period is     ##\n##       required, use the ggplot2::scale_x_datetime() function       ##\n########################################################################\nggTS_byYear &lt;- function(\n    data, # dataframe with POSIX dates and continuous response\n    date_col_name, # the name of the dataframe column with the date variable to plot\n    response_col_name, # the name of the dataframe column with the response variable to plot\n    start_month = 1, # lower time series limit (default January)\n    end_month = 12, # upper time series limit (default December)\n    minor_breaks_period = \"1 day\" # the period for the graph's x-axis minor breaks (e.g. 1 week, 1 day)\n) {\n  # SETUP\n  require(ggplot2)  # for plotting\n  require(magrittr) # source of the pipe  |&gt; ) function\n  require(dplyr)    # data manipulation\n  require(lubridate) # date handling\n  fake_year &lt;- 0001 # fake year used to have all dates over same period (grouped by real year)\n\n  # APPEND VARIABLES TO DATA FOR USE IN PLOTTING\n  data = data |&gt;\n    mutate(\n      datetime = as_datetime({{date_col_name}}), # Ensure dates in POSIX format\n      year = year(datetime), # Create columns for real year and\n      month = month(datetime) # real month\n    )\n\n  # THE CASE WHEN THE PLOT PERIOD IS WITHIN A SINGLE CALENDER YEAR (e.g. June 2016 - Nov 2016)\n  if (start_month &lt;= end_month) {\n    # Get x-axis breaks and labels:\n    plot_months = c(start_month:(end_month+1)) # vector of months to plot (including end_month)\n    plot_breaks = make_datetime(fake_year, plot_months) # x-axis major breaks at each month\n    # Assign data to plot periods and fake years and filter out data not needed:\n    data &lt;- data |&gt;\n      mutate(\n        # Plot period is within the real year (e.g. June 2016 - October 2016)\n        plot_period_label = paste(year), # data for all months pertain to respective year\n        dummy_date = update(datetime, year = fake_year) # all data plotted over fake year (e.g. 0001)\n      ) |&gt;\n      filter(month &gt;= start_month & month &lt;= end_month)\n  }\n\n  # THE CASE WHEN THE PLOT PERIOD IS SPREAD ACROSS TWO CALENDER YEARS (e.g. Nov 2016 - June 2017)\n  if (start_month &gt; end_month) {\n    # Get x-axis breaks and labels\n    plot_months_y1 = c(start_month:12) # a vector of months to plot in the former year\n    plot_months_y2 = c(1:(end_month+1)) # a vector of months to plot in the latter year\n    plot_months = c(plot_months_y1, plot_months_y2)\n    plot_breaks &lt;- c(\n      make_datetime(fake_year, plot_months_y1),\n      make_datetime(fake_year+1, plot_months_y2)\n    )\n    # Assign data to plot periods (i.e. based on real dates), create the fake date\n    # (using fake_year and fake_year +1), and filter out data not needed:\n    data &lt;- data |&gt;\n      mutate(\n        # Plot period crosses two calender years, therefore\n        # data for months prior to start_month pertain to preceding plot period\n        plot_period_start = ifelse(month &gt;= start_month, year, year-1),\n        plot_period_end = plot_period_start+1,\n        plot_period_label = paste(plot_period_start, substr(plot_period_end, 3, 4), sep = '-'),\n        # Dummy dates: months after start_month plotted in fake year (e.g. 0001), months prior plotted in 0002\n        dummy_year = ifelse(month &gt;= start_month, fake_year, fake_year + 1),\n        dummy_date = update(datetime, year = dummy_year)\n      ) |&gt;\n      filter(month &gt;= start_month | month &lt;= end_month)\n  }\n\n  # CREATE X-AXIS (DATES) BREAK LABELS\n  # If end_month is 12 (December), plot_months ends at 13 (January of next year)\n  plot_months &lt;- replace(plot_months, plot_months==13, 1) # Change 13 to 1\n  break_labels &lt;- month.abb[plot_months]\n\n  # CREATE PLOT\n  ts_plot &lt;- data |&gt;\n    ggplot(aes(x = dummy_date, y = {{response_col_name}}, group = plot_period_label, colour = plot_period_label)) +\n    labs(x = \"Date\", y = \"Response\",  colour = \"Year\") +\n    theme_bw() +\n    scale_x_datetime(breaks = plot_breaks, labels = break_labels, date_minor_breaks = minor_breaks_period)\n\n  return(ts_plot)\n}\n\nNow we have our function definition, lets see it in action. Recall that our previous plot was quite cluttered, which made it difficult to discern any trends in the data. So this time, let’s plot for just the month of October.\n\nereefs_data |&gt;\n  filter(variable == \"v\") |&gt;\n  ggTS_byYear(aggregated_date_time, mean, start_month = 10, end_month = 10) +\n  geom_line(linewidth = 0.7) +\n  geom_abline(slope = 0, intercept = 0, linewidth = 0.8) +\n  labs(y = \"Northward current velocity (m/s)\")\n\n\n\n\n\n\n\nFigure 5: Times series plot of daily mean northward current velocity during October for each year 2010 to 2021.\n\n\n\n\n\nThis is significantly easier to digest. However, there is still a lot going on, and sometimes we may wish to see more than a single month at a time."
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#interactive-time-series-plots",
    "href": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#interactive-time-series-plots",
    "title": "Plotting eReefs data",
    "section": "Interactive time series plots",
    "text": "Interactive time series plots\nWe can create interactive plots with the plotly package, enabling us to view individual years, compare subsets of the years, and zoom in on our data and pan across. As this interactivity is a virtue of the html file format, plotly plots have limited application (e.g. cannot be used for reports). However, they do allow us to explore our data very easily and intuitively.\nLet’s plot our data across an entire year and use the zoom and pan features to explore in more detail. We will plot the graph from September through August, as this aligns with the dates for the data we have extracted.\nWe will initially show only the years 2015-16 and 2021-22, as this provides a nice instance of high variability in the northward current in different years.\n\n# Create a ggplot and turn it into a plotly plot\nplotSepSep &lt;- ereefs_data |&gt;\n  filter(variable == \"v\") |&gt;\n  ggTS_byYear(aggregated_date_time, mean, start_month=9, end_month=8) +\n  geom_line(linewidth = 0.7) +\n  geom_abline(slope = 0, intercept = 0, linewidth = 0.5) +\n  labs(y = \"Northward current velocity (m/s)\") +\n  theme(legend.position=\"top\")\n\nplotSepSep |&gt;\n  ggplotly(tooltip = \"none\") |&gt;  # don't show any information on hover (alternative: \"mean\" would show mean current)\n  style(visible = \"legendonly\", traces = c(1:5, 7:11))  # don't show years 2010-2014 & 2016-2020 initially\n\n\n\n\n\n\n\nFigure 6: Interactive plot of daily mean northward current velocity for September through August 2010-11 to 2021-22 (double click years to toggle display of all years).\n\n\n\n\nThis plot allows us to see what is happening both in individual years and across all the years.\nLooking at all years we the trend of current predominantly to the north (positive value) and less variable in (roughly) April through July with southward currents becoming more frequent and strong in August and peaking in (roughly) September through March.\nZooming in to our approximate coral spawning season of October through January, we can see that there are often periods of predominant southward currents, however when these periods occur is highly variable between years."
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#exploring-wind-current",
    "href": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#exploring-wind-current",
    "title": "Plotting eReefs data",
    "section": "Exploring wind & current",
    "text": "Exploring wind & current\nSince we are interested in the surface current (i.e. we have data for northward current at 0.5 m depth), it seems reasonable to suspect that the wind may be a key driving force behind the current direction and velocity. Let’s examine this suspicion with by looking at the relationship between the mean northward wind and current speeds.\n\n# Create a wide format dataset (separate columns for mean current and wind)\nmeanWindCurr &lt;- ereefs_data |&gt;\n  select(aggregated_date_time, variable, mean) |&gt;  # remove unneeded variables\n  pivot_wider(names_from = variable, values_from = mean) |&gt;\n  mutate(year = aggregated_date_time |&gt;  year())\n\n# Compute Pearson's correlation coefficient for the northward daily mean current and windspeed\nr = cor.test(meanWindCurr$wspeed_v, meanWindCurr$v, method = \"pearson\")\n\n# Dual timeseries plot of windspeed and current\nlabs &lt;- c(\"Northward current velocity (m/s)\", \"Northward wind speed (m/s)\")\nnames(labs) &lt;- c(\"v\", \"wspeed_v\")\nereefs_data |&gt;\n  ggplot(aes(x = aggregated_date_time, y = mean)) + # and plot the daily mean\n  geom_line(alpha=0.7) + # specify a line graph of the mean\n  geom_hline(yintercept = 0, color = \"red\", linewidth = 0.5) + # add a line a y=0\n  geom_vline(xintercept = 2011:2022, color = \"red\") +\n  scale_x_datetime(date_breaks = \"1 year\", date_labels = \"%Y\")+ # show only years on x-axis\n  theme_bw(base_size = 14) +\n  theme(panel.grid.major.x = element_line(color = \"black\", linewidth = 0.5, linetype = \"dotted\"),\n        panel.grid.minor.x = element_line(color = \"grey\", linewidth = 0.5, linetype = \"dotted\"),\n        panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank()) +\n  labs(x = \"Year\", y = \"Daily mean\") +\n  facet_wrap(~variable, nrow = 2, scales= \"free_y\", labeller = labeller(variable = labs))\n\n\n\n\n\n\n\nFigure 7: Time series of daily mean northward current and wind velocities for September 2010 to September 2022.\n\n\n\n\n\nIn Figure 7 we can see that there appears to be a reasonably close relationship between the daily mean northward wind speed and current velocity. This is supported by a Pearson’s correlation coefficient of \\(r=\\) 0.79. We can better visualise this relationship on the scatterplot in Figure 8.\nFrom this data alone we cannot conclude that wind is a driver of current, as it may just be a confounding variable. However, we have at least found a reasonably strong positive relationship between northward windspeed and current.\n\n# Scatter plot of daily mean wind and current\nmeanWindCurr |&gt;\n  ggplot(aes(x = wspeed_v, y = v)) +\n  geom_point(alpha = 0.2, size = 2) +\n  geom_hline(yintercept = 0, colour = \"grey\") +\n  geom_vline(xintercept = 0, colour = \"grey\") +\n  theme_bw(base_size=14) +\n  labs(x = \"Northward wind speed (m/s)\", y = \"Northward current velocity (m/s)\")\n\n\n\n\n\n\n\nFigure 8: Scatter plot of daily mean northward wind and current velocities for September 2010 to September 2022."
  },
  {
    "objectID": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#conclusions-limitations",
    "href": "tutorials/r/plot-ereefs-data/time-series-plot/time_series_plot_r.html#conclusions-limitations",
    "title": "Plotting eReefs data",
    "section": "Conclusions & limitations",
    "text": "Conclusions & limitations\nWhile it would be nice to draw some conclusions from our analyses about the possibility of northern coral larvae migrating into the southern GBR, the reality is that we can’t. Coral larvae travel on complex paths and predicting these paths is an active and highly sophisticated area of research (find out more in the AIMS connectivity webpage; see a real approach to answering this question in this reef resilience article). Unfortunately, this question is just too complicated for any real insights to be gained from data for only a single site. But, at the very least, we did make some nice graphs!\nWe also saw that there are indeed periods of southward currents at our site, and these southward currents are most prevalent between the months of (roughly) October through February, and are particularly uncommon between April through June. We have shown that there is a positive relationship between the northward windspeed and northward current for our site."
  },
  {
    "objectID": "tutorials/r/follow-along/r_and_rstudio_setup.html",
    "href": "tutorials/r/follow-along/r_and_rstudio_setup.html",
    "title": "Setup R and RStudio",
    "section": "",
    "text": "Learn how to setup R and RStudio and follow along with the eReefs tutorials."
  },
  {
    "objectID": "tutorials/r/follow-along/r_and_rstudio_setup.html#r-and-rstudio-setup",
    "href": "tutorials/r/follow-along/r_and_rstudio_setup.html#r-and-rstudio-setup",
    "title": "Setup R and RStudio",
    "section": "R and RStudio setup",
    "text": "R and RStudio setup\nDownload and install R (the programming language) and RStudio (the integrated development environment) following these RStudio installation instructions."
  },
  {
    "objectID": "tutorials/r/follow-along/r_and_rstudio_setup.html#linux-dependencies",
    "href": "tutorials/r/follow-along/r_and_rstudio_setup.html#linux-dependencies",
    "title": "Setup R and RStudio",
    "section": "Linux dependencies",
    "text": "Linux dependencies\nRStudio will automatically detect and install the required R packages, but it can’t install any system-level (Linux) dependencies. These need to be installed separately.\nOn Ubuntu, you can install those dependencies using the following command:\n\nsudo apt-get install libnetcdf-dev libgdal-dev libproj-dev libgeos-dev libudunits2-dev libsqlite3-dev libcurl4-openssl-dev libcurl4-openssl-dev libssl-dev libxml2-dev libfreetype-dev libfontconfig1-dev libharfbuzz-dev libfribidi-dev libpng-dev libtiff5-dev libjpeg-dev"
  },
  {
    "objectID": "tutorials/r/follow-along/r_and_rstudio_setup.html#download-tutorial-files",
    "href": "tutorials/r/follow-along/r_and_rstudio_setup.html#download-tutorial-files",
    "title": "Setup R and RStudio",
    "section": "Download tutorial files",
    "text": "Download tutorial files\nThe eReefs tutorials for R are located in the ereefs-tutorials GitHub repository. Download the repository’s code by clicking Code &gt; Download ZIP. After unzipping the repository’s Zip archive, you will find each R tutorial placed in its own folder, under ereefs-tutorials-main/tutorials/r."
  },
  {
    "objectID": "tutorials/r/follow-along/r_and_rstudio_setup.html#open-tutorial-in-rstudio",
    "href": "tutorials/r/follow-along/r_and_rstudio_setup.html#open-tutorial-in-rstudio",
    "title": "Setup R and RStudio",
    "section": "Open tutorial in RStudio",
    "text": "Open tutorial in RStudio\nOpen RStudio and create a new project: File &gt; New Project &gt; Existing Directory &gt; Browse &gt; select the tutorial folder. Then open the main tutorial qmd file from the Files tab, in the bottom-right panel of RStudio.\nThe project will open in the main panel of RStudio. To ensure everything runs smoothly, make sure you’re in Source editing mode, in the top-left corner of the editor. The Visual mode can cause issues with running R code reliably.\nYou can now run the code chunks within the tutorial by clicking the  button in the top-right corner of each chunk, in order.\n\n\n\nRStudio controls\n\n\n\nKeyboard shortcuts\nChunks can also be run be pressing Ctrl+Shift+Enter, and sections of code (e.g. half a line, a single line, multiple lines) can be run by selecting the code and pressing Ctrl+Enter (replace Ctrl with Cmd on Mac)."
  },
  {
    "objectID": "tutorials/r/follow-along/r_and_rstudio_setup.html#installing-r-packages",
    "href": "tutorials/r/follow-along/r_and_rstudio_setup.html#installing-r-packages",
    "title": "Setup R and RStudio",
    "section": "Installing R packages",
    "text": "Installing R packages\nYou may encounter the situation where an R package used in the tutorials have not yet been installed on your machine. Installing packages in R is easy, just run the following command from an open R console:\n\ninstall.packages(\"&lt;package name&gt;\")  # replace &lt;package name&gt; with the name of the R package you wish to install"
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html",
    "href": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "",
    "text": "Learn the basics of extracting eReefs data from the AIMS server with OPeNDAP in .\nIn this tutorial we will look at how to access eReefs data directly from the AIMS THREDDS server in R.\nThis server hosts aggregated eReefs model data in NetCDF file format and offers access to the data files via OPeNDAP, HTTP Server, and Web Map Service (WMS). While we could download the data files manually via the HTTPServer link, this approach is cumbersome when downloading multiple files, given their large size. Thankfully, OPeNDAP provides a way to access the data files over the internet and extract only the data we want.\nFor example, say we want the daily mean surface temperature at a single location for the last 30 days. If we were to download the 30 individual daily aggregated NetCDF files, with each file ~ 350 Mb, this would require us to download over 10 Gb of data just to get 300 numbers. The vast majority of this data would be irrelevant to our needs as the NetCDF files contain data for a range of variables, at a range of depths, for many, many locations. However, with OPeNDAP, we can extract the daily mean values directly from the server without downloading any unneeded data."
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#motivating-problem",
    "href": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#motivating-problem",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Motivating problem",
    "text": "Motivating problem\nWe will extract the daily mean water temperature for the 10th of December 2022 at 1.5 m depth across the entire scope of the eReefs model. We will then save and plot this data. This example will introduce the basics of how to connect to files on the server and extract the data we want."
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#navigating-the-ereefs-server",
    "href": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#navigating-the-ereefs-server",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Navigating the eReefs server",
    "text": "Navigating the eReefs server\nADD: Info about the folder and file naming on the AIMS THREDDS Server."
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#r-packages",
    "href": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#r-packages",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "R packages",
    "text": "R packages\n\nlibrary(RNetCDF) # working with netcdf files (incl. via OPeNDAP)\nlibrary(raster) # creating and manipuling rasters\n\n\n\n\n\n\n\nWhile the ncdf4 package is commonly used to work with NetCDF files in R, it does not offer compatibility with OPeNDAP for Windows (only Mac and Linux). For this reason we will use the RNetCDF package which offers similar functionality and Windows compatibility with OPeNDAP. Note that if you are using Mac or Linux and wish to use ncdf4, the functions used herein have obvious analogues; for example ncdf4::nc_open() vs. RNetCDF::open.nc()."
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#connect-to-a-file-on-the-server",
    "href": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#connect-to-a-file-on-the-server",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Connect to a file on the server",
    "text": "Connect to a file on the server\nFirst we need to find the right NetCDF file on the server. The available eReefs data NetCDF files are listed in the AIMS THREDDS Server catalogue. We will navigate to the eReefs 4 km Hydrodynamic Model daily aggregated data for the month of December 2022 and copy the OPeNDAP data URL.\n\n\ninput_file &lt;- \"https://thredds.ereefs.aims.gov.au/thredds/dodsC/ereefs/gbr4_v2/daily-monthly/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-2022-12.nc\"\n\nWe can then open a connection to this file using the RNetCDF::open.nc function.\n\ndailyAggDec22.nc &lt;- open.nc(input_file)\n\n\n\n\n\n\n\nIf you wish to download NetCDF files from the server you can click the HTTPServer link instead of OPeNDAP. The file can then be loaded into R by specifying the path: open.nc(\"&lt;path to downloaded file&gt;\")."
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#print-a-file-summary",
    "href": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#print-a-file-summary",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Print a file summary",
    "text": "Print a file summary\nIf we wish to investigate the structure of the file we have connected to, including what variables and dimensions are available, we can print a summary.\n\nsummary &lt;- print.nc(dailyAggDec22.nc)\n\n\n\nsummary\n\n\n\nnetcdf classic {\ndimensions:\n    time = UNLIMITED ; // (31 currently)\n    k = 17 ;\n    latitude = 723 ;\n    longitude = 491 ;\nvariables:\n    NC_FLOAT mean_cur(longitude, latitude, k, time) ;\n        NC_CHAR mean_cur:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/LCEWMP01/\" ;\n        NC_CHAR mean_cur:coordinates = \"time zc latitude longitude\" ;\n        NC_CHAR mean_cur:units = \"ms-1\" ;\n        NC_CHAR mean_cur:short_name = \"mean_cur\" ;\n        NC_CHAR mean_cur:aggregation = \"mean_speed\" ;\n        NC_CHAR mean_cur:standard_name = \"mean_current_speed\" ;\n        NC_CHAR mean_cur:long_name = \"mean_current_speed\" ;\n        NC_INT mean_cur:_ChunkSizes = 1, 1, 133, 491 ;\n    NC_FLOAT salt(longitude, latitude, k, time) ;\n        NC_CHAR salt:qudt__unit = \"http://qudt.org/vocab/unit/PSU\" ;\n        NC_CHAR salt:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/PSLTMP01/\" ;\n        NC_CHAR salt:coordinates = \"time zc latitude longitude\" ;\n        NC_CHAR salt:short_name = \"salt\" ;\n        NC_CHAR salt:aggregation = \"Daily\" ;\n        NC_CHAR salt:units = \"PSU\" ;\n        NC_CHAR salt:long_name = \"Salinity\" ;\n        NC_INT salt:_ChunkSizes = 1, 1, 133, 491 ;\n    NC_FLOAT temp(longitude, latitude, k, time) ;\n        NC_CHAR temp:puv__parameter = \"https://vocab.nerc.ac.uk/collection/P01/current/TEMPMP01/\" ;\n        NC_CHAR temp:coordinates = \"time zc latitude longitude\" ;\n        NC_CHAR temp:short_name = \"temp\" ;\n        NC_CHAR temp:aggregation = \"Daily\" ;\n        NC_CHAR temp:units = \"degrees C\" ;\n        NC_CHAR temp:long_name = \"Temperature\" ;\n        NC_INT temp:_ChunkSizes = 1, 1, 133, 491 ;\n    NC_FLOAT u(longitude, latitude, k, time) ;\n        NC_CHAR u:vector_components = \"u v\" ;\n        NC_CHAR u:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/LCEWMP01/\" ;\n        NC_CHAR u:coordinates = \"time zc latitude longitude\" ;\n        NC_CHAR u:short_name = \"u\" ;\n        NC_CHAR u:standard_name = \"eastward_sea_water_velocity\" ;\n        NC_CHAR u:vector_name = \"Currents\" ;\n        NC_CHAR u:aggregation = \"Daily\" ;\n        NC_CHAR u:units = \"ms-1\" ;\n        NC_CHAR u:long_name = \"Eastward current\" ;\n        NC_INT u:_ChunkSizes = 1, 1, 133, 491 ;\n    NC_FLOAT v(longitude, latitude, k, time) ;\n        NC_CHAR v:vector_components = \"u v\" ;\n        NC_CHAR v:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/LCNSMP01/\" ;\n        NC_CHAR v:coordinates = \"time zc latitude longitude\" ;\n        NC_CHAR v:short_name = \"v\" ;\n        NC_CHAR v:standard_name = \"northward_sea_water_velocity\" ;\n        NC_CHAR v:vector_name = \"Currents\" ;\n        NC_CHAR v:aggregation = \"Daily\" ;\n        NC_CHAR v:units = \"ms-1\" ;\n        NC_CHAR v:long_name = \"Northward current\" ;\n        NC_INT v:_ChunkSizes = 1, 1, 133, 491 ;\n    NC_DOUBLE zc(k) ;\n        NC_CHAR zc:units = \"m\" ;\n        NC_CHAR zc:positive = \"up\" ;\n        NC_CHAR zc:long_name = \"Z coordinate\" ;\n        NC_CHAR zc:axis = \"Z\" ;\n        NC_CHAR zc:coordinate_type = \"Z\" ;\n        NC_CHAR zc:_CoordinateAxisType = \"Height\" ;\n        NC_CHAR zc:_CoordinateZisPositive = \"up\" ;\n    NC_DOUBLE time(time) ;\n        NC_CHAR time:units = \"days since 1990-01-01 00:00:00 +10\" ;\n        NC_CHAR time:long_name = \"Time\" ;\n        NC_CHAR time:standard_name = \"time\" ;\n        NC_CHAR time:coordinate_type = \"time\" ;\n        NC_CHAR time:puv__uom = \"http://vocab.nerc.ac.uk/collection/P06/current/UTAA/\" ;\n        NC_CHAR time:calendar = \"gregorian\" ;\n        NC_CHAR time:_CoordinateAxisType = \"Time\" ;\n        NC_INT time:_ChunkSizes = 1024 ;\n    NC_DOUBLE latitude(latitude) ;\n        NC_CHAR latitude:units = \"degrees_north\" ;\n        NC_CHAR latitude:long_name = \"Latitude\" ;\n        NC_CHAR latitude:standard_name = \"latitude\" ;\n        NC_CHAR latitude:coordinate_type = \"latitude\" ;\n        NC_CHAR latitude:projection = \"geographic\" ;\n        NC_CHAR latitude:puv__ofProperty = \"http://vocab.nerc.ac.uk/collection/S06/current/S0600045/\" ;\n        NC_CHAR latitude:puv__uom = \"http://vocab.nerc.ac.uk/collection/P06/current/DEGN/\" ;\n        NC_CHAR latitude:_CoordinateAxisType = \"Lat\" ;\n    NC_DOUBLE longitude(longitude) ;\n        NC_CHAR longitude:puv__uom = \"http://vocab.nerc.ac.uk/collection/P06/current/DEGE/\" ;\n        NC_CHAR longitude:units = \"degrees_east\" ;\n        NC_CHAR longitude:long_name = \"Longitude\" ;\n        NC_CHAR longitude:standard_name = \"longitude\" ;\n        NC_CHAR longitude:coordinate_type = \"longitude\" ;\n        NC_CHAR longitude:projection = \"geographic\" ;\n        NC_CHAR longitude:_CoordinateAxisType = \"Lon\" ;\n    NC_FLOAT mean_wspeed(longitude, latitude, time) ;\n        NC_CHAR mean_wspeed:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/ESEWMPXX/\" ;\n        NC_CHAR mean_wspeed:coordinates = \"time latitude longitude\" ;\n        NC_CHAR mean_wspeed:units = \"ms-1\" ;\n        NC_CHAR mean_wspeed:short_name = \"mean_wspeed\" ;\n        NC_CHAR mean_wspeed:aggregation = \"mean_speed\" ;\n        NC_CHAR mean_wspeed:standard_name = \"mean_wind_speed\" ;\n        NC_CHAR mean_wspeed:long_name = \"mean_wind_speed\" ;\n        NC_INT mean_wspeed:_ChunkSizes = 1, 133, 491 ;\n    NC_FLOAT eta(longitude, latitude, time) ;\n        NC_CHAR eta:puv__parameter = \"https://vocab.nerc.ac.uk/collection/P01/current/ASLVMP01/\" ;\n        NC_CHAR eta:coordinates = \"time latitude longitude\" ;\n        NC_CHAR eta:short_name = \"eta\" ;\n        NC_CHAR eta:standard_name = \"sea_surface_height_above_geoid\" ;\n        NC_CHAR eta:aggregation = \"Daily\" ;\n        NC_CHAR eta:units = \"metre\" ;\n        NC_CHAR eta:positive = \"up\" ;\n        NC_CHAR eta:long_name = \"Surface elevation\" ;\n        NC_INT eta:_ChunkSizes = 1, 133, 491 ;\n    NC_FLOAT wspeed_u(longitude, latitude, time) ;\n        NC_CHAR wspeed_u:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/ESEWMPXX/\" ;\n        NC_CHAR wspeed_u:coordinates = \"time latitude longitude\" ;\n        NC_CHAR wspeed_u:short_name = \"wspeed_u\" ;\n        NC_CHAR wspeed_u:aggregation = \"Daily\" ;\n        NC_CHAR wspeed_u:units = \"ms-1\" ;\n        NC_CHAR wspeed_u:long_name = \"eastward_wind\" ;\n        NC_INT wspeed_u:_ChunkSizes = 1, 133, 491 ;\n    NC_FLOAT wspeed_v(longitude, latitude, time) ;\n        NC_CHAR wspeed_v:puv__parameter = \"http://vocab.nerc.ac.uk/collection/P01/current/ESNSMPXX/\" ;\n        NC_CHAR wspeed_v:coordinates = \"time latitude longitude\" ;\n        NC_CHAR wspeed_v:short_name = \"wspeed_v\" ;\n        NC_CHAR wspeed_v:aggregation = \"Daily\" ;\n        NC_CHAR wspeed_v:units = \"ms-1\" ;\n        NC_CHAR wspeed_v:long_name = \"northward_wind\" ;\n        NC_INT wspeed_v:_ChunkSizes = 1, 133, 491 ;\n\n// global attributes:\n        NC_CHAR :Conventions = \"CF-1.0\" ;\n        NC_CHAR :Parameter_File_Revision = \"$Revision: 1753 $\" ;\n        NC_CHAR :Run_ID = \"2.1\" ;\n        NC_CHAR :Run_code = \"GBR4 Hydro|G0.00|H2.10|S0.00|B0.00\" ;\n        NC_CHAR :_CoordSysBuilder = \"ucar.nc2.dataset.conv.CF1Convention\" ;\n        NC_CHAR :aims_ncaggregate_buildDate = \"2023-01-25T03:48:20+10:00\" ;\n        NC_CHAR :aims_ncaggregate_datasetId = \"products__ncaggregate__ereefs__gbr4_v2__daily-monthly/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_daily-monthly-2022-12\" ;\n        NC_CHAR :aims_ncaggregate_firstDate = \"2022-12-01T00:00:00+10:00\" ;\n        NC_CHAR :aims_ncaggregate_inputs = \"[products__ncaggregate__ereefs__gbr4_v2__raw/EREEFS_AIMS-CSIRO_gbr4_v2_hydro_raw_2022-12::MD5:bd27b033d40e598c20348044720deb73]\" ;\n        NC_CHAR :aims_ncaggregate_lastDate = \"2022-12-31T00:00:00+10:00\" ;\n        NC_CHAR :bald__isPrefixedBy = \"prefix_list\" ;\n        NC_CHAR :date_created = \"Sun Dec 11 21:24:39 2022\" ;\n        NC_CHAR :description = \"Aggregation of raw hourly input data (from eReefs AIMS-CSIRO GBR4 Hydrodynamic v2 subset) to daily means. Also calculates mean magnitude of wind and ocean current speeds. Data is regridded from curvilinear (per input data) to rectilinear via inverse weighted distance from up to 4 closest cells.\" ;\n        NC_CHAR :ems_version = \"v1.4.0 rev(6949)\" ;\n        NC_CHAR :history = \"2023-01-24T10:49:00+10:00: vendor: AIMS; processing: None summaries\n2023-01-25T03:48:20+10:00: vendor: AIMS; processing: Daily summaries\" ;\n        NC_CHAR :metadata_link = \"https://eatlas.org.au/data/uuid/350aed53-ae0f-436e-9866-d34db7f04d2e\" ;\n        NC_CHAR :paramfile = \"./prm/gbr4_hydro_nrt.prm\" ;\n        NC_CHAR :paramhead = \"GBR 4km resolution grid\" ;\n        NC_CHAR :prefix_list_puv__ = \"https://w3id.org/env/puv#\" ;\n        NC_CHAR :prefix_list_qudt__ = \"http://qudt.org/vocab/unit/\" ;\n        NC_CHAR :technical_guide_link = \"https://eatlas.org.au/pydio/public/aims-ereefs-platform-technical-guide-to-derived-products-from-csiro-ereefs-models-pdf\" ;\n        NC_CHAR :technical_guide_publish_date = \"2020-08-18\" ;\n        NC_CHAR :title = \"eReefs AIMS-CSIRO GBR4 Hydrodynamic v2 daily aggregation\" ;\n        NC_CHAR :DODS_EXTRA.Unlimited_Dimension = \"time\" ;\n}"
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#extract-data",
    "href": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#extract-data",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Extract data",
    "text": "Extract data\nNow that we have an open connection to a file on the server we need to extract the daily mean temperature at 1.5m depth for the 10th of December.\nFrom the summary output above we can see that the variable corresponding to temperature is: \\(\\texttt{ temp(longitude, latitude, k, time)}\\).\nThe dimensions for temperature are in brackets. This means that there is a temperature value for every combination of longitude, latitude, depth (k) and time. We can now see why these NetCDF files are so large.\nTo extract data from the file we will use the function\nRNetCDF::var.get.nc(ncfile, variable, start=NA, count=NA, ...)\nWe need to give the function:\n\nncfile: a NetCDF file connection; in our case dailyAggDec22.nc.\nvariable: the name or id of the data variable we wish to extract; in our case \"temp\".\nstart: a vector of indices of where to start getting data, one for each dimension of the variable. Since we have \\(\\texttt{temp(longitude, latitude, k, time)}\\) we need to tell the function where to start getting data along each of the four dimensions.\ncount: similar to start, but specifying the number of temperature values to extract along each dimension.\n\nLet’s look at how to construct our start and count vectors.\n\n\n\n\n\n\nThe default values of start and count are NA, in which case all data for the given variable will be extracted.\n\n\n\nDepth: Starting with depth is easy because we have a constant value of interest (1.5 m). The index k corresponds to different depths as shown in the table below, where we see that for the 4km models k=16 maps to a depth of 1.5 m.\n\n\nTable of eReefs depths corresponding to index k\n\n\n\n\n\n\nIndex k (R)\nIndex k (Python)\nHydrodynamic 1km model\nHydrodynamic & BioGeoChemical 4km models\n\n\n\n\n1\n0\n-140.00\n-145.00\n\n\n2\n1\n-120.00\n-120.00\n\n\n3\n2\n-103.00\n-103.00\n\n\n4\n3\n-88.00\n-88.00\n\n\n5\n4\n-73.00\n-73.00\n\n\n6\n5\n-60.00\n-60.00\n\n\n7\n6\n-49.00\n-49.00\n\n\n8\n7\n-39.50\n-39.50\n\n\n9\n8\n-31.00\n-31.00\n\n\n10\n9\n-24.00\n-23.75\n\n\n11\n10\n-18.00\n-17.75\n\n\n12\n11\n-13.00\n-12.75\n\n\n13\n12\n-9.00\n-8.80\n\n\n14\n13\n-5.25\n-5.55\n\n\n15\n14\n-2.35\n-3.00\n\n\n16\n15\n-0.50\n-1.50\n\n\n17\n16\nNA\n-0.50\n\n\n\n\n\n\n\n\nTime: Since we have the daily aggregated data for December 2022, and are interested only in a single day (the 10th), time is also a constant value. From the summary output we can see we have 31 time indexes, these correspond to the day of the month, therefore we want time=10.\nLongitude and latitude: We want temperatures for every available longitude and latitude so we can plot the data across the entire spatial range of the eReefs model. Therefore we want to start at index 1 and count for the entire length of latitude and longitude. To get the lengths we could note the values from the summary output, where we see \\(\\texttt{longitude = 491}\\) and \\(\\texttt{latitude = 723}\\). However we could also get the lengths programmatically.\n\nlon &lt;- var.get.nc(dailyAggDec22.nc, \"longitude\")\nlat &lt;- var.get.nc(dailyAggDec22.nc, \"latitude\")\ndata.frame(length(lon), length(lat))\n\n  length.lon. length.lat.\n1         491         723\n\n\n\n\n\n\n\n\nWithin the eReefs NetCDF files, the dimensions \\(\\texttt{longitude, latitude, k, time}\\) have corresponding variables longitude, latitude, zc, time (see summary output). Note that we would extract the dimension \\(\\texttt{k}\\) variable with var.get.nc(..., variable = \"zc\").\n\n\n\nNow we are ready to construct our start and count vectors and extract the data.\n\n# SETUP START AND COUNT VECTORS\n# Recall the order of the dimensions: (lon, lat, k , time)\n# We start at lon=1, lat=1, k=16, time=10 and get temps for\n# every lon and lat while holding depth and time constant\nlon_st &lt;- 1\nlat_st &lt;- 1\ndepth_st &lt;- 16  # index k = 16 --&gt; depth = 1.5 m\ntime_st &lt;- 10   # index time = 10 --&gt; 10th day of month\n\nlon_ct &lt;- length(lon) # get temps for all lons and lats\nlat_ct &lt;- length(lat)\ntime_ct &lt;- 1  # Hold time and depth constant\ndepth_ct &lt;- 1\n\nstart_vector &lt;- c(lon_st, lat_st, depth_st, time_st)\ncount_vector &lt;- c(lon_ct, lat_ct, time_ct, depth_ct)\n\n# EXTRACT DATA\ntemps_10Dec22_1p5m &lt;- var.get.nc(\n  ncfile = dailyAggDec22.nc,\n  variable = \"temp\",\n  start = start_vector,\n  count = count_vector\n)\n\n# Get the size of our extracted data\ndims &lt;- dim(temps_10Dec22_1p5m)\ndata.frame(nrows = dims[1], ncols = dims[2])\n\n  nrows ncols\n1   491   723"
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#close-file-connection",
    "href": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#close-file-connection",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Close file connection",
    "text": "Close file connection\nNow that our extracted data is stored in memory, we should close the connection to the NetCDF file on the server.\n\nclose.nc(dailyAggDec22.nc)"
  },
  {
    "objectID": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#save-the-data",
    "href": "tutorials/r/access-ereefs-data-server/basic-server-access/basic_server_access_r.html#save-the-data",
    "title": "Accessing eReefs data from the AIMS server",
    "section": "Save the data",
    "text": "Save the data\nNow that we have our extracted data we may wish to save it for future use. To do this we first convert the data from its current form as a matrix array into a raster, and then save the raster as a NetCDF file (GeoTIFF and other formats are also possible).\nWhen converting extracted eReefs data to rasters we need to apply the transpose t() and flip() functions in order to get the correct orientation.\n\ntemps_raster &lt;- temps_10Dec22_1p5m |&gt;\n  t() |&gt;   # transpose temps matrix\n  raster(  # create raster\n    xmn = min(lon), xmx = max(lon),\n    ymn = min(lat), ymx = max(lat),\n    crs = CRS(\"+init=epsg:4326\")\n  ) |&gt;\n  flip(direction = 'y') # flip the raster\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\ntemps_raster\n\nclass      : RasterLayer \ndimensions : 723, 491, 354993  (nrow, ncol, ncell)\nresolution : 0.0299389, 0.02995851  (x, y)\nextent     : 142.1688, 156.8688, -28.69602, -7.036022  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : layer \nvalues     : 22.08588, 32.48817  (min, max)\n\n\n\n\n\n\n\n\nIn the code chunk above we used pipes |&gt;. Pipes are very useful when passing a dataset through a sequence of functions. In the code above we take our extracted temps, transpose them, turn them into a raster, and then flip the raster. The final result is saved to temps_raster.\n\n\n\nNow we have our raster (in the correct orientation), saving it is easy.\n\n# Save the raster file in the /data subdirectory\n# Note the '.nc' file extension since we are going to save as NetCDF\nsave_file &lt;- \"data/ereefsDailyMeanWaterTemperature_10Dec2022_Depth1p5m.nc\"\n\nwriteRaster(\n  x = temps_raster, # what to save\n  filename = save_file, # where to save it\n  format = \"CDF\", # what format to save it as\n  overwrite = TRUE # whether to replace any existing file with the same name\n)\n\n\n\n\n\n\n\nThe raster package uses the ncdf4 package to deal with NetCDF files. You can install this package by running install.packages(\"ncdf4\") from an R console.\n\n\n\nTo prove to ourselves that this worked, lets read the file back in and plot it.\n\n# Read back in the saved NetCDF file as a raster\nsaved_temps &lt;- raster(save_file)\n\n# Plot the data\nplot(saved_temps)\n\n\n\n\n\n\n\nFigure 1: Extracted eReefs daily aggregated mean temperature at 1.5m depth for 10 December 2022.\n\n\n\n\n\nHooray! We can now see our saved data plotted in Figure 1."
  },
  {
    "objectID": "TO_DO.html",
    "href": "TO_DO.html",
    "title": "List of ideas to possible implement",
    "section": "",
    "text": "List of ideas to possible implement\n\nResources section. Linked from the homepage. A collection of things like:\n\ntable mapping index k to depths for the models (recall python k = R k - 1 since python starts counting at 0 🤦)\neReefs shapefiles (boundary, grid)\neAtlas reef boundaries shapefile\netc.\n\nEfficient server access (R & Python). The third tutorial in the server access series. Considers ways to maximize efficiency of accessing data from the server (e.g. for 50,000 points). Provide a time comparison of R vs python. (Minimize the number of files opened, Check if points are in the eReefs model scope, Check if multiple points in single model grid cell).\nMap plot tutorial (R & Python) in plotting series. Looks at plotting points and rasters on maps (static and interactive with leaflet).\nIntro tut. — work to do:\n\nSource images (incl. icons in fig 1)\nAdd in more about model validation?\nAdd in more about model accuracy! Incl. a mini-review of studies assessing this.\nCurvilinear vs linear grids\nContinue working on model variable lists\n\nGeneral to do:\n\nChange tutorial series banner\nBetter description of BGC scenarios\nLink back to the aims ereefs website from navbar"
  }
]