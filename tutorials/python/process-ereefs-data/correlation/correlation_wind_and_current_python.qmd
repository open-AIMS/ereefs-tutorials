---
title: "Processing eReefs data {{< fa brands python >}}"
subtitle: "The effect of wind on surface current"
---

:::{.tutorial-description}
Learn how to use eReefs data to answer the question '**How strong does the wind need to be to set the
direction of the surface ocean currents?**' in {{< fa brands python >}} python.
:::

## Motivating problem

The East Australian Current (EAC) usually is a strong southward current near Myrmidon and Davies Reefs.
During winter months the wind moves in a north eastern direction in the near opposite direction to the EAC.
When the wind is low the surface currents are dominated by the EAC. As the wind picks up, at some speed the
wind overpowers the EAC and the surface current moves in the direction of the wind.

We will use the [AIMS eReefs extraction tool](https://extraction.ereefs.aims.gov.au/) to extract data for two
locations of interest:

* Myrmidon Reef which is on the far outer edge of the GBR and is almost right in the middle of the southern
Eastern Australian Current; and

* Davies Reef which is further in the reef matrix, but in a similar sector of the GBR.

<!-- The locations of these two reefs are shown in @fig-site-locations. -->

We then process the data and investigate the relationship between the strength of the wind and the direction
of the surface currents for the two locations.

```{python mapSites, include=FALSE}
#| fig-cap: Myrmidon Reef and Davies Reef locations (see [TUTORIAL NAME](TUTORIAL LINK HERE) to learn how to create maps like this).
#| label: fig-site-locations
#| code-fold: true
#| code-summary: show code to create map
```

## Analysis method

To determine the relation between the wind and the surface currents we will use the AIMS eReefs extraction
tool to pull out hourly time series wind and current data for our two locations of interest. We will then
look at the correlation between the wind and current vectors, where a correlation of 1 indicates they are
pointing in the same direction, and -1 indicated they are in opposite directions.


## Setting up to get the data

To extract the time series data using the extraction tool we need to create a CSV file containing the sites
of interest. This file needs to contain the coordinates and names of the sites. To create this I first added
my points manually in Google Earth Pro. This was done to simply get the location of Myrmidon and Davies
Reefs. Using Google Earth to create your CSV file for the extraction tool is only useful if you don't already
know the coordinates of your sites.


[Screenshot of Google Earth Pro with Myromidon and Davies reef sites](images/google-earth-reef-locatons.jpg)

The points can be added using the `Add placemark` tool (looks like a pin). The locations can be seen by
displaying the placemark properties. The resulting KML file can be found here:
[extraction-tool-locations.kml](resources/extraction-tool-locations.kml).

The location of the two sites were copied to create the
[CSV file for the data extraction tool](resources/site_coordinates.csv).


## Extracting the data

The CSV file was uploaded to the [AIMS extraction tool](https://extraction.ereefs.aims.gov.au/) and the
extraction was performed with the following settings:

* *Data collection*: GBR1 Hydro (Version 2)

* *Variables*:
  - Eastward wind speed (`wspeed_u`)
  - Northward wind speed (`wspeed_v`)
  - Northward current (`v`)
  - Eastward current (`u`)

* *Date range*: 1 January 2019 - 31 December 2019

* *Time step*: hourly

* *Depths*: -2.35 m

Once the extraction request was submitted the dataset was created after an one hour of processing the data
was available for download from [Extraction request: Example dataset: Wind-vs-Current at Davies and Myrmidon
Reefs (2019)](https://extraction.ereefs.aims.gov.au/data/2009.c451dc3).

## Downloading the data

In this notebook we will download the data using scripting. There is no need to re-run the extraction request
as each extraction performed by the extraction tool has a permanent public page created for it that can be
used to facilitate sharing of the data.

Let's first create a temporary folder to contain the downloaded data. Note: The temp folder is excluded using
the .gitignore so it is not saved to the code repository, which is why we must reproduce it.

```{python}
import os
if not os.path.exists('temp'):
  os.makedirs('temp')
```

Now let's download the data. The file to download is 12.9 MB and so this download might take a little while.
To allow us to re-run this script without having to wait for the download each time we first check that the
download has not already been done.

```{python}
import urllib.request
extractionfile = os.path.join('temp','2009.c451dc3-collected.csv')  # Use os.path.join so the script will work cross-platform

if not os.path.exists(extractionfile):
  print("Downloading extraction data ...")
  url = 'https://api.ereefs.aims.gov.au/data-extraction/request/2009.c451dc3/files/2009.c451dc3-collected.csv'
  req = urllib.request.urlretrieve(url, extractionfile)
  print(req)
else:
  print("Skipping redownloading extraction data")
```


## Reading and cleaning the data

Read the resulting CSV file into a
[Pandas data frame](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html).

```{python}
import pandas as pd
from janitor import clean_names
df = pd.read_csv(extractionfile).clean_names()
```

```{python}
#| eval: false
print(repr(df.head()))
```

```{python}
#| echo: false
df.head()
```

The first thing we might notice about the data is that, somewhat confusingly, we have a bunch of aggregation
statistics (mean, median, p5, p95, lowest, highest) which all take the same value. This is because we have
extracted hourly "aggregated" data, but the time step for the eReefs model is also hourly. Therefore each row
represents an aggregation over a single data point. Don't worry if you are confused by this, it's not
important. Just think of this as a quirk of the eReefs data extraction tool. We'll clean this up now by
replacing the aggregation statistics with a single column called `value` and rename `aggregated_date_time` to
`date_time`, to avoid any further confusion.

```{python}
# Create 'value' column, remove aggregation statistic columns, rename aggregated_date_time
df2 = df.\
  assign(value = df['mean']).\
  drop(columns=['mean', 'median', 'p5', 'p95', 'lowest','highest']).\
  rename(columns={"aggregated_date_time": "date_time"})
```

```{python}
#| eval: false
print(repr(df2.head()))
```

```{python}
#| echo: false
df2.head()
```

Much better! Now it is clear to see that the data is in long format. That is, a single row for each value and
a separate column describing the meaning of the value --- in this case the column `variable` describes what
the column `value` means. Converting the data into [tidy](https://r4ds.had.co.nz/tidy-data.html) format will
help with subsequent analyses (and is probably also the format you are most comfortable working with). When
data is in tidy format, each variable has its own column, each observation has its own row, and each value
has its own cell.

If we think of the wind and current velocities as the things being "measured", then the observations in the
dataset are the values of the measurements for each time point at each site. Therefore we would like a single
row for each unique combination of `date_time` and `site_name` (or `latitude` and `longitude` pair). Let's
see if this is what we get we try to "widen" the data into tidy format.

```{python}
# Try pivoting the data into tidy format
df2_pivot = df2.pivot(
  index = ["site_name", "latitude", "longitude", "date_time", "depth"],
  columns="variable",
  values="value"
)
```

```{python}
#| eval: false
print(repr(df2_pivot.head()))
```

```{python}
#| echo: false
df2_pivot.head()
```

With all the `NA`s in the wind and current speed columns, this doesn't look tidy at all! And it's because we
forgot to account for depth. However, we can notice that depth is actually a redundant variable in this
dataset. That's because wind speed implies a `NA` depth value (coded as 10000m in the extracted data) and
current implies a depth of -2.35m (as this is the only depth we chose to extract). Therefore we can just
remove depth entirely from our dataset without losing any information, that is, as long as we remember that
the current relates to a depth of -2.35m. For the forgetful among us, we could rename the current variables
to `v_2.35m` and `u_2.35m`. In fact moving the depth into the variable names would be a good solution to
create a tidy dataset if we had selected multiple depths.

```{python}
# Pivot into tidy format (excluding depth)
df_tidy = df2.pivot(
  index = ["site_name", "latitude", "longitude", "date_time"],
  columns="variable",
  values="value"
)
```

```{python}
#| eval: false
print(repr(df_tidy.head()))
```

```{python}
#| echo: false
df_tidy.head()
```

Much better! Now each row gives the wind and current speeds for different sites at different points in time.



## Correlation

Our aim is to create an index that estimates the correlation of the current and the wind vectors.

The correlation of the current and wind vectors can be estimated based using the
[dot product](https://www.mathsisfun.com/algebra/vectors-dot-product.html). An overview of the relationship
between correlation and using the dot product is described in
[Geometric Interpretation of the Correlation between Two Variables](https://medium.com/@ns2586/geometric-interpretation-of-the-correlation-between-two-variables-4011fb3ea18e).
The correlation between the two vectors is given by:

$$
r = \cos(\theta) = \frac{a \cdot b}{||a||\cdot||b||}
$$


where $a \cdot b$ is the dot product between the two vectors and $||a||$ and $||b||$ are the magnitudes of
the vectors. The dot product can be calculated as

$$
a \cdot b = a_x \times b_x + a_y \times b_y
$$

and the magnitude of the vectors as

$$
||a|| = \sqrt{a^2_x + a^2_y} \;\;, \; \;\;\;\;
||b|| = \sqrt{b^2_x + b^2_y}
$$

```{python}
import numpy as np
df_corr = df_tidy
df_corr['currentmag'] = np.sqrt(df_corr['u']**2 + df_corr['v']**2)
df_corr['windmag'] = np.sqrt(df_corr['wspeed_u']**2 + df_corr['wspeed_v']**2)
df_corr['windcurrentcorr'] = (df_corr['u'] * df_corr['wspeed_u']  +  df_corr['v'] * df_corr['wspeed_v']) / (df_corr['currentmag'] * df_corr['windmag'])
```

```{python}
#| eval: false
print(repr(df_corr.head()))
```

```{python}
#| echo: false
df_corr.head()
```


Let's look at the relationship between the wind and current as a function of the wind speed. Here we are
considering each hourly sample as an independent estimate of the relationship. In reality this is not the
case as the longer the wind blows the more effect it will have on the current. As this is just a coding
example and not an in-depth analysis we don't need to worry about this limitation of the analysis.

Let's pull out the data for Davies and Myrmidon Reefs separately so they are easy to plot.

```{python}
davies = df_corr.query('site_name == "Davies Reef"')
myrmidon = df_corr.query('site_name == "Myrmidon Reef"')
```

```{python}
#| eval: false
print(repr(myrmidon.head()))
```

```{python}
#| echo: false
myrmidon.head()
```

Let's create a scatter plot to see if there is a relationship between the wind and currents.

```{python}
import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

ax.scatter(myrmidon["windmag"], myrmidon["windcurrentcorr"], color='r', s=1)
ax.scatter(davies["windmag"], davies["windcurrentcorr"], color='b', s=1)
ax.set_xlabel('Wind speed (m/s)')
ax.set_ylabel('Wind-current correlation')
ax.set_title('Correlation between wind and surface current (hourly data, 2019)')

plt.tight_layout()
plt.show()
```

This scatter plot shows that the relationship between wind and current is weak. This is not surprising given
that we are considering just the hourly samples, with no consideration for how long the wind has been
blowing. At low wind conditions the current has an even chance of being aligned with the wind
(correlation $r= 1$) as in the opposite direction (correlation $r= -1$), however in high wind we can see that
there is much more chance that the currents are aligned with the wind.

To understand this relationship better we want to understand how much the wind nudges the current in its
direction. If we bin the wind speeds then collect all the correlation samples in each bin then we can see if
they average to zero (indicating that there is no relationship between the wind and current) or there is
average alignment.

```{python}
from scipy import stats
wind = davies["windmag"]
current = davies["windcurrentcorr"]
bin_means, bin_edges, binnumber = stats.binned_statistic(wind, current, 'mean', bins=20)
plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=5,
           label='Davies Reef')

wind = myrmidon["windmag"]
current = myrmidon["windcurrentcorr"]
bin_means, bin_edges, binnumber = stats.binned_statistic(wind, current, 'mean', bins=20)
plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='r', lw=5,
           label='Myrmidon Reef')

plt.xlabel('Wind speed (m/s)')
plt.ylabel('Wind-current correlation')
plt.title('Mean correlation between wind and surface current (hourly data, 2019)')
plt.legend()
plt.show()
```

From this we can see that for wind speeds below about 8 m/s the surface current direction is unrelated to the
wind. Above this wind speed the surface current is increasingly determined by the direction of the wind. By
the time the wind is 16 m/s the direction of the surface current is dominated by the wind direction.

:::{.callout-caution appearance="simple"}
It should be remembered that this analysis is based on the eReefs Hydrodynamic model and as such is not based
on real data. The eReefs model has however been tuned to accurately capture the flow dynamics of the GBR and
so we would expect the estimates from this analysis to be approximately correct.
:::
